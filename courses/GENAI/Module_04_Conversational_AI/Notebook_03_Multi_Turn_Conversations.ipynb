{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Multi-Turn Conversations**\n",
    "# **Introduction**\n",
    "\n",
    "This notebook explores handling multi-turn conversations with Groq LLM,. We'll learn how to manage conversation flow, maintain context across multiple exchanges, and maintaining coherent dialogues.\n",
    "\n",
    "## **LLM Setup for Multi-Turn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from swarmauri.llms.concrete.GroqModel import GroqModel\n",
    "from swarmauri.messages.concrete import SystemMessage, HumanMessage, AgentMessage\n",
    "from swarmauri.conversations.concrete.SessionCacheConversation import SessionCacheConversation\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inittialize the groq model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "llm = GroqModel(api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a conversation with session caching**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create conversation with system context and max_size\n",
    "conversation = SessionCacheConversation(\n",
    "    system_context=SystemMessage(content=\"I am a cooking assistant that provides clear, practical advice.\"),\n",
    "    max_size=6  # This will allow for 3 pairs of human-agent messages\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Simulate a multi-turn conversation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create initial messages ensuring proper alternation of HumanMessage and AgentMessage\n",
    "messages_to_add = [\n",
    "    HumanMessage(content=\"What are basic kitchen tools I need?\"),\n",
    "    AgentMessage(content=\"Essential kitchen tools include: chef's knife, cutting board, measuring cups, mixing bowls, and pans.\"),\n",
    "    HumanMessage(content=\"How do I maintain my knives?\"),\n",
    "    AgentMessage(content=\"Keep knives sharp, wash by hand, and store them safely in a knife block or magnetic strip.\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add messages in a single call :**\n",
    "\n",
    "This will maintain the alternating pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add messages in a single call - this will maintain the alternating pattern\n",
    "conversation.add_messages(messages_to_add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add a new question and get LLM response**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "User: What's the difference between chopping and dicing?\n",
      "\n",
      "Assistant:\n",
      " **Chopping:**\n",
      "- Involves cutting ingredients into larger pieces of varying sizes.\n",
      "- Usually used for tougher ingredients like vegetables or meats.\n",
      "- Results in unevenly sized pieces.\n",
      "\n",
      "\n",
      "**Dicing:**\n",
      "- Involves cutting ingredients into small, uniform cubes.\n",
      "- Typically used for softer ingredients like fruits, vegetables, or herbs.\n",
      "- Results in evenly sized cubes.\n"
     ]
    }
   ],
   "source": [
    "# Add a new question and get LLM response\n",
    "new_question = \"What's the difference between chopping and dicing?\"\n",
    "conversation.add_message(HumanMessage(content=new_question))\n",
    "response = llm.predict(conversation)\n",
    "\n",
    "\n",
    "print(f\"\\nUser: {new_question}\")\n",
    "print(f\"\\nAssistant:\\n {response.get_last().content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Managing Conversation Flow**\n",
    "**Example showing how to handle conversation flow with context:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a conversation with larger context window**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = SessionCacheConversation(\n",
    "    system_context=SystemMessage(content=\"I am a cooking assistant\"),\n",
    "    max_size=8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Simulate a recipe discussion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate cooking guidance with LLM\n",
    "cooking_questions = [\n",
    "    \"How do I make pasta carbonara?\",\n",
    "    \"What ingredients do I need?\",\n",
    "    \"How long should I cook the pasta?\",\n",
    "    \"How do I know when it's done?\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "User: How do I make pasta carbonara?\n",
      "Assistant: **Ingredients:**\n",
      "\n",
      "- 1 pound spaghetti\n",
      "- 1/2 cup heavy cream\n",
      "- 1/4 cup Parmesan cheese, grated\n",
      "- 1 egg yolk\n",
      "- 2 tablespoons butter\n",
      "- 1/4 cup chopped pancetta or bacon\n",
      "- Salt and pepper to taste\n",
      "\n",
      "**Instructions:**\n",
      "\n",
      "1. **Cook the pasta:**\n",
      "   - Bring a large pot of salted water to a boil.\n",
      "   - Add the spaghetti and cook for about 1 minute less than the package directions.\n",
      "   - Drain well when ready.\n",
      "\n",
      "\n",
      "2. **Make the sauce:**\n",
      "   - In a large skillet, melt the butter over medium heat.\n",
      "   - Add the pancetta or bacon and cook until crispy, about 5 minutes.\n",
      "   - Add the cream and bring to a simmer.\n",
      "   - Reduce heat and simmer for 2 minutes.\n",
      "\n",
      "\n",
      "3. **Add the egg yolk and cheese:**\n",
      "   - In a small bowl, whisk together the egg yolk and Parmesan cheese.\n",
      "\n",
      "\n",
      "4. **Combine the sauce and pasta:**\n",
      "   - Remove the skillet from the heat and immediately add the cooked pasta.\n",
      "   - Toss the pasta with the sauce until it is evenly coated.\n",
      "\n",
      "\n",
      "5. **Finish with cheese and\n",
      "\n",
      "User: What ingredients do I need?\n",
      "Assistant: - 1 pound spaghetti\n",
      "- 1/2 cup heavy cream\n",
      "- 1/4 cup Parmesan cheese, grated\n",
      "- 1 egg yolk\n",
      "- 2 tablespoons butter\n",
      "- 1/4 cup chopped pancetta or bacon\n",
      "- Salt and pepper to taste\n",
      "\n",
      "User: How long should I cook the pasta?\n",
      "Assistant: The pasta should be cooked for about 1 minute less than the package directions.\n",
      "\n",
      "User: How do I know when it's done?\n",
      "Assistant: The pasta is done when it is al dente, which means it is cooked through but still has a slight bite to it. It should be firm to the bite, but not hard.\n"
     ]
    }
   ],
   "source": [
    "# Process questions sequentially\n",
    "for question in cooking_questions:\n",
    "    conversation.add_message(HumanMessage(content=question))\n",
    "    response = llm.predict(conversation)\n",
    "    conversation.add_message(AgentMessage(content=response.get_last().content))\n",
    "    \n",
    "    print(f\"\\nUser: {question}\")\n",
    "    print(f\"Assistant: {response.get_last().content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Conclusion**\n",
    "**In this notebook, we've learned:**\n",
    "\n",
    "\n",
    "Managing multi-turn conversations with Groq LLM\n",
    "\n",
    "Maintaining context across conversation turns\n",
    "\n",
    "Creating interactive guidance systems\n",
    "\n",
    "Handling step-by-step dialogues with LLM responses\n",
    "\n",
    "**Next, we'll explore how to implement effective conversation memory management.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **NOTEBOOK METADATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Dominion John \n",
      "GitHub Username: DOMINION-JOHN1\n",
      "Last Modified: 2024-11-04 12:40:13.121040\n",
      "Platform: Windows 11\n",
      "Python Version: 3.12.7 (tags/v3.12.7:0b05ead, Oct  1 2024, 03:06:41) [MSC v.1941 64 bit (AMD64)]\n",
      "Swarmauri Version: Swarmauri Version: 0.5.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import platform\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# Display author information\n",
    "author_name = \"Dominion John \" \n",
    "github_username = \"DOMINION-JOHN1\"  \n",
    "\n",
    "print(f\"Author: {author_name}\")\n",
    "print(f\"GitHub Username: {github_username}\")\n",
    "\n",
    "# Last modified datetime (file's metadata)\n",
    "notebook_file = \"Notebook_03_Multi_Turn_Conversations.ipynb\"\n",
    "try:\n",
    "    last_modified_time = os.path.getmtime(notebook_file)\n",
    "    last_modified_datetime = datetime.fromtimestamp(last_modified_time)\n",
    "    print(f\"Last Modified: {last_modified_datetime}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not retrieve last modified datetime: {e}\")\n",
    "\n",
    "# Display platform, Python version, and Swarmauri version\n",
    "print(f\"Platform: {platform.system()} {platform.release()}\")\n",
    "print(f\"Python Version: {sys.version}\")\n",
    "\n",
    "import swarmauri\n",
    "\n",
    "try:\n",
    "    version = swarmauri.__version__\n",
    "except AttributeError:\n",
    "    version = f\"Swarmauri Version: 0.5.1\"\n",
    "\n",
    "print(f\"Swarmauri Version: {version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swarmauri-0.5.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
