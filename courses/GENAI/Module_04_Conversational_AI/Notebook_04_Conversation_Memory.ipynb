{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Conversation Memory Management**\n",
    "# **Introduction**\n",
    "This notebook focuses on managing conversation memory effectively, including handling memory constraints, implementing memory policies, and managing conversation history.\n",
    "\n",
    "## **Basic Memory Management**\n",
    "**Let's start with basic conversation memory handling:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from swarmauri.llms.concrete.GroqModel import GroqModel\n",
    "from swarmauri.messages.concrete import SystemMessage, HumanMessage, AgentMessage\n",
    "from swarmauri.conversations.concrete.SessionCacheConversation import SessionCacheConversation\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialize the Groq model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "llm = GroqModel(api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **LLM basic memory Setup:**\n",
    " Create a conversation with limited memory\n",
    " \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create conversation with memory limit\n",
    "conversation = SessionCacheConversation(\n",
    "    system_context=SystemMessage(content=\"You are a knowledgeable research assistant\"),\n",
    "    max_size=2,session_max_size=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Add  conversations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test question series\n",
    "questions = [\n",
    "    \"in  a sentence Tell me about renewable energy\",\n",
    "    \"in  a sentence What about solar power specifically?\",\n",
    "    \"in  a sentence How do solar panels work?\",\n",
    "    \"in  a sentence What's their efficiency?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Responses:\n",
      "\n",
      "Q: in  a sentence Tell me about renewable energy\n",
      "A: Renewable energy sources like solar, wind, hydro, geothermal, and tidal energy harness naturally replenished resources to generate electricity, heat homes, and power transportation, offering sustainable and environmentally friendly alternatives to fossil fuels.\n",
      "\n",
      "Q: in  a sentence What about solar power specifically?\n",
      "A: Solar power harnesses the abundant energy of the sun through photovoltaic cells or thermal collectors, generating electricity or heat for various applications.\n",
      "\n",
      "Q: in  a sentence How do solar panels work?\n",
      "A: Solar panels harness the energy of the sun through the photovoltaic effect. Sunlight is absorbed by silicon atoms in the solar cells, causing electrons to be excited and creating a flow of electricity.\n",
      "\n",
      "Q: in  a sentence What's their efficiency?\n",
      "A: Their efficiency varies depending on the specific task, with some models achieving human-parity in certain areas like language translation and question-answering, while others struggle with tasks that require common sense or physical interaction.\n"
     ]
    }
   ],
   "source": [
    "print(\"Responses:\")\n",
    "for question in questions:\n",
    "    conversation.add_message(HumanMessage(content=question))\n",
    "    response = llm.predict(conversation)\n",
    "    conversation.add_message(AgentMessage(content=response.get_last().content))\n",
    "    print(f\"\\nQ: {question}\")\n",
    "    print(f\"A: {response.get_last().content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a knowledgeable research assistant\n"
     ]
    }
   ],
   "source": [
    "for msg in conversation.history:\n",
    "       print(msg.content)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solar panels harness the energy of the sun through the photovoltaic effect. Sunlight is absorbed by silicon atoms in the solar cells, causing electrons to be excited and creating a flow of electricity.\n",
      "Solar panels harness the energy of the sun through the photovoltaic effect. Sunlight is absorbed by silicon atoms in the solar cells, causing electrons to be excited and creating a flow of electricity.\n",
      "in  a sentence What's their efficiency?\n",
      "Their efficiency varies depending on the specific task, with some models achieving human-parity in certain areas like language translation and question-answering, while others struggle with tasks that require common sense or physical interaction.\n",
      "Their efficiency varies depending on the specific task, with some models achieving human-parity in certain areas like language translation and question-answering, while others struggle with tasks that require common sense or physical interaction.\n"
     ]
    }
   ],
   "source": [
    "for msg in conversation.session:\n",
    "       print(msg.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Differentiating max_size and session_cache_max_size**\n",
    "\n",
    "\n",
    "\n",
    "This example demonstrates how max_size and session_cache_max_size work differently. \n",
    "The max_size parameter determines how many recent messages the model is aware of, while session_cache_max_size controls how many messages the user can see in the conversation history.\n",
    "\n",
    "**Create conversation with system context**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "conversation = SessionCacheConversation(\n",
    "    system_context=SystemMessage(content=\"You are a helpful cooking assistant\"),\n",
    "    max_size=3,             # Keeps last 3 exchanges in memory\n",
    "    session_max_size=10   # Keeps last 6 exchanges in session cache\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Simulate a chat with cooking questions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cooking_exchanges = [\n",
    "    (\"How do I make pasta carbonara?\", \"To make pasta carbonara, you'll need...\"),\n",
    "    (\"What ingredients do I need?\", \"For the ingredients, you'll want eggs, pancetta...\"),\n",
    "    (\"How long should I cook the pasta?\", \"Cook the pasta for about 8-10 minutes.\"),\n",
    "    (\"How do I know when it's done?\", \"The pasta is done when it's tender.\"),\n",
    "    (\"Should I add salt?\", \"Yes, add a pinch of salt to the water.\"),\n",
    "    (\"Can I add cream?\", \"Traditionally, carbonara doesn’t use cream.\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add each message in the conversation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for question, response in cooking_exchanges:\n",
    "    conversation.add_messages([\n",
    "        HumanMessage(content=question),\n",
    "        AgentMessage(content=response)\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Show conversation history (affected by max_size)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CONVERSATION HISTORY (max_size=3):\n",
      "\n",
      "You are a helpful cooking assistant\n",
      "\n",
      "Can I add cream?\n",
      "\n",
      "Traditionally, carbonara doesn’t use cream.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\nCONVERSATION HISTORY (max_size=3):\")\n",
    "for message in conversation.history:\n",
    "    print(f\"\\n{message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Show session cache (affected by session_cache_size)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SESSION CACHE (session_cache_size=6):\n",
      "\n",
      "What ingredients do I need?\n",
      "\n",
      "For the ingredients, you'll want eggs, pancetta...\n",
      "\n",
      "How long should I cook the pasta?\n",
      "\n",
      "Cook the pasta for about 8-10 minutes.\n",
      "\n",
      "How do I know when it's done?\n",
      "\n",
      "The pasta is done when it's tender.\n",
      "\n",
      "Should I add salt?\n",
      "\n",
      "Yes, add a pinch of salt to the water.\n",
      "\n",
      "Can I add cream?\n",
      "\n",
      "Traditionally, carbonara doesn’t use cream.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\nSESSION CACHE (session_cache_size=6):\")\n",
    "for message in conversation.session:\n",
    "    print(f\"\\n{message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Differentiating max_size and session_cache_max_size with LLM**\n",
    "\n",
    "In the example, Conversation 1 has a max_size of 3 and a session_cache_max_size of 10, while Conversation 2 has a max_size of 6 and a session_cache_max_size of 20. \n",
    "This means that the model in Conversation 1 is only aware of the last 3 messages, but the user can see up to 10 messages in the history. Similarly, the model in Conversation 2 is aware of the last 6 messages, but the user can see up to 20 messages.\n",
    "\n",
    "\n",
    "**This is helpful as it keeps token cost low but still allow the you to be able to recall previous parts of the conversation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create conversations with different memory limits:**\n",
    "\n",
    "Conversation1 has a short model memory with short user chat history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_size sets the model's awareness of recent messages\n",
    "# session_cache_max_size sets the number of messages the user can see\n",
    "conversation1 = SessionCacheConversation(\n",
    "    system_context=SystemMessage(content=\"You are a research assistant\"),\n",
    "    max_size=2,session_max_size=4\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversation2  has a long model memory with short user chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation2 = SessionCacheConversation(\n",
    "    system_context=SystemMessage(content=\"You are a technical support agent\"), \n",
    "    max_size=6,\n",
    "    session_max_size=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add messages to both conversations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add messages to both conversations\n",
    "research_questions = [\n",
    "    \"in  a sentence What is machine learning?\",\n",
    "    \"in  a sentence How does it work?\", \n",
    "    \"in  a sentence What are the main applications?\",\n",
    "    \"in  a sentence What are the challenges?\",\n",
    "    \"in  a sentence What is a list in Python?\",\n",
    "    \"in  a sentence How do I create a dictionary?\",\n",
    "    \"in  a sentence What are list comprehensions?\"\n",
    "]\n",
    "\n",
    "for question in research_questions:\n",
    "    conversation1.add_message(HumanMessage(content=question))\n",
    "    conversation1.add_message(AgentMessage(content=llm.predict(conversation1).get_last().content))\n",
    "\n",
    "for question in research_questions:\n",
    "    conversation2.add_message(HumanMessage(content=question))\n",
    "    conversation2.add_message(AgentMessage(content=llm.predict(conversation2).get_last().content))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print conversation1 model memory and session cache(number of messages users can see)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model memory and awareness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation 1 (max_size=2, session_cache_max_size=4):\n",
      "You are a research assistant\n",
      "in  a sentence What are list comprehensions?\n",
      "List comprehensions are a syntax in some programming languages that allows creating new lists by iterating over existing lists and applying a transformation or condition to each element.\n"
     ]
    }
   ],
   "source": [
    "# Print the list of information retained in the model memory\n",
    "print(\"Conversation 1 (max_size=2, session_cache_max_size=4):\")\n",
    "for msg in conversation1.history:\n",
    "    print (msg.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Session cache (number of messages users can see)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation 1 (max_size=2, session_cache_max_size=4):\n",
      "There are numerous challenges associated with the research process, including limited access to data, methodological limitations, and biases in data collection and analysis.\n",
      "in  a sentence What is a list in Python?\n",
      "A list in Python is a mutable, ordered collection of elements that can hold various data types like integers, strings, floats, or even other lists. It is denoted by square brackets `[]` and can contain multiple values separated by commas.\n",
      "A list in Python is a mutable, ordered collection of elements that can hold various data types like integers, strings, floats, or even other lists. It is denoted by square brackets `[]` and can contain multiple values separated by commas.\n",
      "in  a sentence How do I create a dictionary?\n",
      "Creating a dictionary involves meticulous planning, data collection, and meticulous organization. You'll need to define the scope, determine the language or domain, gather and analyze data, establish indexing and cross-referencing systems, and design the physical or digital format of the dictionary.\n",
      "Creating a dictionary involves meticulous planning, data collection, and meticulous organization. You'll need to define the scope, determine the language or domain, gather and analyze data, establish indexing and cross-referencing systems, and design the physical or digital format of the dictionary.\n",
      "in  a sentence What are list comprehensions?\n",
      "List comprehensions are a syntax in some programming languages that allows creating new lists by iterating over existing lists and applying a transformation or condition to each element.\n",
      "List comprehensions are a syntax in some programming languages that allows creating new lists by iterating over existing lists and applying a transformation or condition to each element.\n"
     ]
    }
   ],
   "source": [
    "# Print the number of messages users can see\n",
    "print(\"Conversation 1 (max_size=2, session_cache_max_size=4):\")\n",
    "for msg in conversation1.session:\n",
    "    print (msg.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print Conversation2 model memory and session cache(number of messages users can see)**\n",
    "\n",
    "Model memory and awareness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CONVERSATION 2 (max_size=6, session_cache_max_size=10):\n",
      "You are a technical support agent\n",
      "in  a sentence How do I create a dictionary?\n",
      "You can create a dictionary in Python using curly braces { } and pairing keys with values. Keys must be unique and of a immutable type, while values can be of any type. For example, `my_dict = {\"name\": \"John\", \"age\": 30}` creates a dictionary with the key-value pairs \"name\" -> \"John\" and \"age\" -> 30.\n",
      "in  a sentence What are list comprehensions?\n",
      "List comprehensions are a concise and efficient way to create new lists by iterating over existing lists and performing transformations or conditional logic on each element.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\nCONVERSATION 2 (max_size=6, session_cache_max_size=10):\")\n",
    "for msg in conversation2.history:\n",
    "    print(msg.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Session cache (number of messages users can see)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CONVERSATION 2 (max_size=6, session_cache_max_size=10):\n",
      "Machine learning systems can suffer from biases in the training data, leading to fairness and privacy concerns, and their performance can degrade in real-world environments due to unforeseen circumstances.\n",
      "in  a sentence What is a list in Python?\n",
      "A list in Python is an ordered collection of elements that can be of any type, allowing for flexible data manipulation and retrieval.\n",
      "A list in Python is an ordered collection of elements that can be of any type, allowing for flexible data manipulation and retrieval.\n",
      "in  a sentence How do I create a dictionary?\n",
      "You can create a dictionary in Python using curly braces { } and pairing keys with values. Keys must be unique and of a immutable type, while values can be of any type. For example, `my_dict = {\"name\": \"John\", \"age\": 30}` creates a dictionary with the key-value pairs \"name\" -> \"John\" and \"age\" -> 30.\n",
      "You can create a dictionary in Python using curly braces { } and pairing keys with values. Keys must be unique and of a immutable type, while values can be of any type. For example, `my_dict = {\"name\": \"John\", \"age\": 30}` creates a dictionary with the key-value pairs \"name\" -> \"John\" and \"age\" -> 30.\n",
      "in  a sentence What are list comprehensions?\n",
      "List comprehensions are a concise and efficient way to create new lists by iterating over existing lists and performing transformations or conditional logic on each element.\n",
      "List comprehensions are a concise and efficient way to create new lists by iterating over existing lists and performing transformations or conditional logic on each element.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\nCONVERSATION 2 (max_size=6, session_cache_max_size=10):\")\n",
    "for msg in conversation2.session:\n",
    "    print(msg.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Conversation 1 model's memory size: 3\n",
      "Conversation 2 model's memory size: 5\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nConversation 1 model's memory size:\", len(conversation1.history))\n",
    "print(\"Conversation 2 model's memory size:\", len(conversation2.history))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Conclusion**\n",
    "**In this notebook, we've explored:**\n",
    "\n",
    "\n",
    "\n",
    "Integration of Groq LLM with memory management\n",
    "\n",
    "Handling memory constraints in LLM conversations\n",
    "\n",
    "Comparing different memory policies with LLM responses\n",
    "\n",
    "Managing complex conversations with memory tracking\n",
    "\n",
    "Understanding how memory affects LLM context and responses\n",
    "\n",
    "**These techniques help create more effective and memory-efficient conversational AI applications.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **NOTEBOOK METADATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Dominion John \n",
      "GitHub Username: DOMINION-JOHN1\n",
      "Last Modified: 2024-11-04 13:33:22.996990\n",
      "Platform: Windows 11\n",
      "Python Version: 3.12.7 (tags/v3.12.7:0b05ead, Oct  1 2024, 03:06:41) [MSC v.1941 64 bit (AMD64)]\n",
      "Swarmauri Version: Swarmauri Version: 0.5.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import platform\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# Display author information\n",
    "author_name = \"Dominion John \" \n",
    "github_username = \"DOMINION-JOHN1\"  \n",
    "\n",
    "print(f\"Author: {author_name}\")\n",
    "print(f\"GitHub Username: {github_username}\")\n",
    "\n",
    "# Last modified datetime (file's metadata)\n",
    "notebook_file = \"Notebook_04_Conversation_Memory.ipynb\"\n",
    "try:\n",
    "    last_modified_time = os.path.getmtime(notebook_file)\n",
    "    last_modified_datetime = datetime.fromtimestamp(last_modified_time)\n",
    "    print(f\"Last Modified: {last_modified_datetime}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not retrieve last modified datetime: {e}\")\n",
    "\n",
    "# Display platform, Python version, and Swarmauri version\n",
    "print(f\"Platform: {platform.system()} {platform.release()}\")\n",
    "print(f\"Python Version: {sys.version}\")\n",
    "\n",
    "import swarmauri\n",
    "\n",
    "try:\n",
    "    version = swarmauri.__version__\n",
    "except AttributeError:\n",
    "    version = f\"Swarmauri Version: 0.5.1\"\n",
    "\n",
    "print(f\"Swarmauri Version: {version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swarmauri-0.5.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
