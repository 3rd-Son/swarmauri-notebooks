{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Notebook_03_Text_to_Speech**\n",
    "# **Introduction**\n",
    "Text-to-Speech (TTS) technology represents a key aspect of audio generation, converting written text into audible speech. This functionality supports applications ranging from virtual assistants and accessibility aids to automated content narration. Swarmauri’s SDK provides a seamless interface for TTS, encapsulated in the OpenAIAudioTTS class, allowing users to synthesize natural-sounding audio from textual inputs.\n",
    "\n",
    "**In this notebook, we’ll dive into the configuration and usage of TTS with Swarmauri SDK:**\n",
    "\n",
    "We’ll begin by setting up the TTS feature in OpenAIAudioTTS to generate audio directly from text.\n",
    "Next, we’ll experiment with language, tone, and speech rate customization, which allows for more dynamic and context-specific applications.\n",
    "\n",
    "Finally, we’ll explore how to integrate Swarmauri TTS in multilingual setups, using specific examples of generating speech in different languages for a global user base.\n",
    "\n",
    "This practical exploration will demonstrate how to leverage Swarmauri’s high-quality audio models, making it easier to integrate TTS features for diverse audiences while maintaining clear and responsive audio output. This functionality is particularly beneficial for applications in accessibility, customer support, and automated information delivery."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import of dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from swarmauri.llms.concrete.OpenAIAudioTTS import OpenAIAudioTTS\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize OpenAI Audio TTS\n",
    "API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "llm = OpenAIAudioTTS(api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set up paths**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = Path.cwd()\n",
    "output_dir = os.path.join(root_dir, \"output\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Configuration:\n",
      "Default Model: tts-1\n",
      "Default Voice: alloy\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Model Configuration:\")\n",
    "print(f\"Default Model: {llm.name}\")\n",
    "print(f\"Default Voice: {llm.voice}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Available models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Available Models:\n",
      "- tts-1\n",
      "- tts-1-hd\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nAvailable Models:\")\n",
    "available_models = llm.allowed_models\n",
    "for model in available_models:\n",
    "    print(f\"- {model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Basic Text-to-Speech Generation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Voices to demonstrate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "voices = [\"alloy\", \"echo\", \"fable\", \"onyx\", \"nova\", \"shimmer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this section, we'll generate audio by modifying parameters like model name and voice. We'll save the audio to a file and observe how changing these parameters impacts the output.**\n",
    "\n",
    "Assign the selected model and voice to the llm object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llm.name = \"tts-1\"\n",
    "llm.voice = \"alloy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output file path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = os.path.join(output_dir,\"tts-1_alloy_output.mp3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate audio using the predict method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"Hello, this is a test of streaming text-to-speech output.\"\n",
    "print(\"\\nText-to-Speech Demonstration:\")\n",
    "audio_file = llm.predict(\n",
    "    text=sample_text, \n",
    "    audio_path=output_path\n",
    ")\n",
    "print(\"\\nDemonstration complete! Check the output directory for generated audio files.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **2. Generating Audio with Different Voices and Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nText-to-Speech Demonstrations:\")\n",
    "sample_text = \"Hello, this is a test of streaming text-to-speech output.\"\n",
    "\n",
    "for model_name in available_models:\n",
    "    llm.name = model_name\n",
    "    \n",
    "    for voice in voices:\n",
    "        llm.voice = voice\n",
    "        \n",
    "        # Output file path\n",
    "        output_path = os.path.join(output_dir, f\"{model_name}_{voice}_output.mp3\")\n",
    "        \n",
    "        print(f\"\\nGenerating audio - Model: {model_name}, Voice: {voice}\")\n",
    "        \n",
    "        # Predict method\n",
    "        audio_file = llm.predict(\n",
    "            text=sample_text, \n",
    "            audio_path=output_path\n",
    "        )\n",
    "       print(\"\\nDemonstration complete! Check the output directory for generated audio files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. Model Serialization Validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Serialization:\n",
      "Original Model ID: c5047e78-cc32-4503-9915-2d78749c804b\n",
      "Validated Model ID: c5047e78-cc32-4503-9915-2d78749c804b\n",
      "Validation Successful: True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\nModel Serialization:\")\n",
    "model_json = llm.model_dump_json()\n",
    "validated_model = OpenAIAudioTTS.model_validate_json(model_json)\n",
    "print(f\"Original Model ID: {llm.id}\")\n",
    "print(f\"Validated Model ID: {validated_model.id}\")\n",
    "print(f\"Validation Successful: {llm.id == validated_model.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Conclusion**\n",
    "\n",
    "The text-to-speech capabilities offered by Swarmauri’s OpenAIAudioTTS module prove invaluable in creating accessible and engaging user experiences. Through this notebook, we’ve illustrated the ease of using Swarmauri SDK to convert text into speech and customize parameters for tailored audio outputs. \n",
    "\n",
    "The flexibility provided by Swarmauri’s TTS enables integration into applications that require fast, reliable, and contextually appropriate speech synthesis, paving the way for enhanced user interactions. This foundational knowledge prepares users to explore deeper customization, enabling fully personalized audio generation workflows in advanced scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from swarmauri.utils.print_notebook_metadata  import print_notebook_metadata\n",
    "\n",
    "print_notebook_metadata(author_name = \"Dominion John \" , github_username = \"DOMINION-JOHN1\" )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swarmauri-0.5.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
