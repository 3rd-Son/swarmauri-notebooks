{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b8a5c12-0252-4de7-b734-c0a566217076",
   "metadata": {},
   "source": [
    "# Notebook 01: Testing and Validation\n",
    "\n",
    "**Purpose:** This notebook will cover testing and validation of the different AI components (text, image, and audio) in the multi-modal chatbot system.\n",
    "\n",
    "# Project Setup\n",
    "\n",
    "Before we begin, we need to load the necessary libraries and initialize the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "53754d97-3121-4ced-accd-0260c4ba679a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "import base64\n",
    "import requests\n",
    "from swarmauri.utils.base64_to_img_url import base64_to_img_url\n",
    "\n",
    "\n",
    "# Import the required classes from swarmauri library\n",
    "from swarmauri.llms.concrete.FalAIVisionModel import FalAIVisionModel\n",
    "from swarmauri.utils.base64_to_img_url import base64_to_img_url\n",
    "from swarmauri.llms.concrete.OpenAIAudioTTS import OpenAIAudioTTS\n",
    "from swarmauri.llms.concrete.OpenAIModel import OpenAIModel as LLM\n",
    "from swarmauri.conversations.concrete.Conversation import Conversation\n",
    "from swarmauri.messages.concrete.HumanMessage import HumanMessage\n",
    "from swarmauri.llms.concrete.DeepInfraImgGenModel import DeepInfraImgGenModel\n",
    "\n",
    "\n",
    "# Load environment variables and API key\n",
    "load_dotenv()\n",
    "\n",
    "# Fetch the API keys from environment variables\n",
    "DEEPINFRA_API_KEY = os.getenv(\"DEEPINFRA_API_KEY\")\n",
    "\n",
    "IMGBB_API_KEY = os.getenv(\"IMGBB_API_KEY\")\n",
    "\n",
    "API_KEY = os.getenv(\"FAL_KEY\")\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "\n",
    "# Initialize the OpenAI Model\n",
    "llm = LLM(api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Initialize the DeepInfra Image Generation Model\n",
    "llm_img_gen = DeepInfraImgGenModel(api_key=DEEPINFRA_API_KEY)\n",
    "\n",
    "# Initialize FALAI Vison Model\n",
    "falai_vision_model = FalAIVisionModel(api_key=API_KEY) if API_KEY else None\n",
    "\n",
    "# Initialize Text-to-Speech Model\n",
    "tts = OpenAIAudioTTS(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1009756-93b8-4e38-bd12-d4f1f25fc3ac",
   "metadata": {},
   "source": [
    "# Text Generation\n",
    "\n",
    "Our chatbot will start by responding to text prompts, generating text based on user input. The chatbot can answer questions, create stories, or even perform more complex tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "06ed0cf8-b36b-4982-b08c-932b2833a053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate responses and display prompt engineering effects\n",
    "def generate_text_response(prompt: str) -> str:\n",
    "    # Create a new conversation instance for each prompt\n",
    "    conversation = Conversation()\n",
    "    conversation.add_message(HumanMessage(content=prompt))\n",
    "\n",
    "    # Generate model response\n",
    "    llm.predict(conversation=conversation)\n",
    "\n",
    "    # Display prompt, model response, and usage data\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(\"Response:\", conversation.get_last().content)\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Usage Data:\", conversation.get_last().usage)\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "68533ea4-a389-4643-8fd8-3314bd50cdb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Tell me a short story about a scientist discovering new planets.\n",
      "Response: Dr. Elena Vega had spent years studying the stars and searching for new planets. She had always dreamed of making a groundbreaking discovery that would change the course of history. And finally, that day had come.\n",
      "\n",
      "One night, while studying the data from her telescope, Dr. Vega noticed something peculiar. There seemed to be a series of unexplained gravitational anomalies in a distant part of the galaxy. Intrigued, she decided to focus her attention on this area and investigate further.\n",
      "\n",
      "After weeks of meticulous research and analysis, Dr. Vega finally made a breakthrough. She had discovered not one, but three new planets orbiting a distant star. These planets were unlike anything she had ever seen before - they were teeming with life and had unique geological formations that defied all known scientific principles.\n",
      "\n",
      "Excited by her discovery, Dr. Vega quickly published her findings and soon the entire scientific community was buzzing with excitement. The discovery of these new planets opened up a whole new realm of possibilities for space exploration and colonization.\n",
      "\n",
      "Dr. Vega's name became synonymous with discovery and exploration, and she was hailed as a pioneer in the field of astrophysics. But for her, the greatest reward was knowing that her discovery had opened up a whole new world of possibilities for future generations to explore and uncover\n",
      "==================================================\n",
      "Usage Data: prompt_tokens=19 completion_tokens=256 total_tokens=275 prompt_time=3.4054744243621826 completion_time=0.0010066032409667969 total_time=3.4064810276031494 completion_tokens_details={'audio_tokens': 0, 'reasoning_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0} prompt_tokens_details={'audio_tokens': 0, 'cached_tokens': 0}\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Example text prompt\n",
    "example_prompt = \"Tell me a short story about a scientist discovering new planets.\"\n",
    "\n",
    "text_response = generate_text_response(example_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e5c784-e9d5-44a6-8cff-b62b1b500506",
   "metadata": {},
   "source": [
    "# Image Generation\n",
    "\n",
    "In addition to text-based responses, our chatbot will also be capable of generating images based on text descriptions. The user can provide textual prompts, and the chatbot will generate corresponding visual content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b400e183-6ecc-4bca-bfc7-22d9175e52c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate an image from text\n",
    "def generate_image_from_text(prompt: str):\n",
    "    # Create a conversation instance\n",
    "    conversation = Conversation()\n",
    "\n",
    "    # Add the user prompt to the conversation\n",
    "    human_message = HumanMessage(content=prompt)\n",
    "    conversation.add_message(human_message)\n",
    "\n",
    "    # Generate the detailed description of the scene based on the prompt\n",
    "    llm = DeepInfraModel(api_key=DEEPINFRA_API_KEY)\n",
    "    prediction = llm.predict(conversation=conversation, **{\"temperature\": 0})\n",
    "\n",
    "    # Get the detailed description from the conversation\n",
    "    detailed_description = conversation.get_last().content\n",
    "\n",
    "    # Generate the image from the detailed description\n",
    "    image_base64 = llm_img_gen.generate_image_base64(detailed_description)\n",
    "\n",
    "    # Upload the generated image to IMGBB\n",
    "    try:\n",
    "        image_url = base64_to_img_url(image_base64, IMGBB_API_KEY)\n",
    "        return image_url\n",
    "    except Exception as e:\n",
    "        print(\"Error uploading the image:\", e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "392e9bca-5ac7-4947-aa1d-037da9345830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Image URL: https://i.ibb.co/7vdtyyB/761db0c955f8.jpg\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "image_prompt = \"A futuristic city skyline at night with neon lights and flying cars\"\n",
    "image_url = generate_image_from_text(image_prompt)\n",
    "\n",
    "if image_url:\n",
    "    print(\"Generated Image URL:\", image_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58769fbd-1cab-4c75-b185-b75df8efce76",
   "metadata": {},
   "source": [
    "# Audio Generation\n",
    "\n",
    "For audio generation, our chatbot can convert text responses into speech using a Text-to-Speech (TTS) model. It can also create sound effects based on user descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e75225b3-3415-41be-a649-f8fe98a2c4f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating speech for text: Welcome to the text-to-speech demonstration using OpenAI's TTS service.\n",
      "Generated audio saved to: output\\sample_output.mp3\n"
     ]
    }
   ],
   "source": [
    "# Setup output directory\n",
    "output_dir = Path(\"output\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "def generate_audio_from_text(text: str, voice: str = \"alloy\", model: str = \"tts-1\", output_filename: str = \"output.mp3\") -> str:\n",
    "    \"\"\"\n",
    "    Generates speech from input text and saves it as an audio file.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The text to be converted into speech.\n",
    "        voice (str): The voice to be used for TTS. Default is \"alloy\".\n",
    "        model (str): The TTS model to be used. Default is \"tts-1\".\n",
    "        output_filename (str): The name of the output audio file. Default is \"output.mp3\".\n",
    "\n",
    "    Returns:\n",
    "        str: The file path of the generated audio.\n",
    "    \"\"\"\n",
    "    # Set model and voice\n",
    "    tts.name = model\n",
    "    tts.voice = voice\n",
    "    \n",
    "    # Define output path\n",
    "    output_path = output_dir / output_filename\n",
    "    \n",
    "    # Generate speech and save to file\n",
    "    print(f\"Generating speech for text: {text}\")\n",
    "    audio_file = tts.predict(\n",
    "        text=text,\n",
    "        audio_path=str(output_path)\n",
    "    )\n",
    "    \n",
    "    # Return path to the generated audio file\n",
    "    return str(output_path)\n",
    "\n",
    "# Example usage\n",
    "sample_text = \"Welcome to the text-to-speech demonstration using OpenAI's TTS service.\"\n",
    "audio_file_path = generate_audio_from_text(sample_text, voice=\"shimmer\", model=\"tts-1\", output_filename=\"sample_output.mp3\")\n",
    "print(f\"Generated audio saved to: {audio_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982049ca-1b41-41fa-9663-3e069d81a5b5",
   "metadata": {},
   "source": [
    "# Computer Vision with Image Inputs\n",
    "\n",
    "The chatbot can also handle images as input. It will process the images using computer vision models and respond with descriptive captions, tags, or detailed analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5e886d80-7297-41b1-a175-85b3b3e3bad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_image(image_url: str) -> str:\n",
    "    \"\"\"Analyze an image using FalAIVisionModel to detect tags and describe content.\"\"\"\n",
    "    try:\n",
    "        result = falai_vision_model.process_image(image_url=image_url, prompt=\"Describe the content of this image.\")\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return f\"Error processing image: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2cf13738-5d78-4f64-a7af-0cdbbade59f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image URL:  https://llava-vl.github.io/static/images/monalisa.jpg\n",
      "Image Analysis Result: The image you've provided is a representation of the famous painting \"Mona Lisa\" by Leonardo da Vinci. The painting is renowned for its depiction of a woman with a subtle smile, set against a distant landscape. The artwork is characterized by its use of sfumato, a technique that creates a\n"
     ]
    }
   ],
   "source": [
    "# Example image URL (can be replaced with user-uploaded images)\n",
    "new_image_url = \"https://llava-vl.github.io/static/images/monalisa.jpg\"\n",
    "image_analysis_result = analyze_image(new_image_url)\n",
    "print(\"Image URL: \", new_image_url)\n",
    "print(f\"Image Analysis Result: {image_analysis_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3ede72-c591-49d6-a85f-8f3a174ca891",
   "metadata": {},
   "source": [
    "# Summary:\n",
    "\n",
    "- **Text Generation:** The chatbot generates text responses for queries or prompts.\n",
    "- **Image Generation:** Based on text descriptions, the chatbot can generate corresponding images.\n",
    "- **Audio Generation:** The chatbot can convert text into speech for audio responses.\n",
    "- **Computer Vision:** The chatbot can analyze images and provide descriptions or tags based on visual input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a89169e-5b02-41fe-8aef-baef1240944d",
   "metadata": {},
   "source": [
    "# Notebook Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "202dfe20-c7b9-48bf-9751-6bd49c783f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Huzaifa Irshad\n",
      "GitHub Username: irshadhuzaifa\n",
      "Last Modified: 2024-11-06 18:04:16.976657\n",
      "Platform: Windows 11\n",
      "Python Version: 3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:17:27) [MSC v.1929 64 bit (AMD64)]\n",
      "Swarmauri Version: Swarmauri Version: 0.5.1\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# Display author information\n",
    "author_name = \"Huzaifa Irshad\" \n",
    "github_username = \"irshadhuzaifa\"  \n",
    "\n",
    "print(f\"Author: {author_name}\")\n",
    "print(f\"GitHub Username: {github_username}\")\n",
    "\n",
    "# Last modified datetime (file's metadata)\n",
    "notebook_file = \"Notebook_01_Testing_and_Validation.ipynb\"\n",
    "try:\n",
    "    last_modified_time = os.path.getmtime(notebook_file)\n",
    "    last_modified_datetime = datetime.fromtimestamp(last_modified_time)\n",
    "    print(f\"Last Modified: {last_modified_datetime}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not retrieve last modified datetime: {e}\")\n",
    "\n",
    "# Display platform, Python version, and Swarmauri version\n",
    "print(f\"Platform: {platform.system()} {platform.release()}\")\n",
    "print(f\"Python Version: {sys.version}\")\n",
    "\n",
    "import swarmauri\n",
    "\n",
    "try:\n",
    "    version = swarmauri.__version__\n",
    "except AttributeError:\n",
    "    version = f\"Swarmauri Version: 0.5.1\"\n",
    "\n",
    "print(f\"Swarmauri Version: {version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5960c203-026e-46a2-874b-dea02ea89530",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swarmauri(0.5.1)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
