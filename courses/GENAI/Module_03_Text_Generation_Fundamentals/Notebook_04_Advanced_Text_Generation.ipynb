{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3998fb1b-f74c-4938-bd14-7bf58a3b964e",
   "metadata": {},
   "source": [
    "# Notebook_04_Advanced_Text_Generation\n",
    "## Objective\n",
    "\n",
    "This notebook explores advanced capabilities of text generation models, including real-time streaming, response customization, and batch processing. By leveraging these features, we can create interactive, tailored, and efficient AI applications that adapt to user needs.\n",
    "\n",
    "## Advanced Capabilities in Text Generation\n",
    "\n",
    "This notebook demonstrates advanced functionalities, which can enhance both the performance and adaptability of AI applications. By using streaming, customization, and batch processing, you can create interactive and responsive applications that cater to different user needs and use cases.\n",
    "\n",
    "- **Streaming** is ideal for real-time applications, providing immediate feedback as tokens are generated.\n",
    "- **Customization** allows responses to be tailored according to specific needs, making interactions more engaging and user-focused.\n",
    "- **Batch Processing** enables efficient handling of multiple queries, which is crucial for applications like chatbots, customer service, and large-scale Q&A systems.\n",
    "\n",
    "This notebook provides a comprehensive look at advanced text generation features, which can be invaluable for developing robust, user-centric AI solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7ef5a202-c320-42b0-a5c3-a1e7b5686428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a9790960-8c75-4229-b50e-36baf2d4191d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "31adf7c4-17a9-4ea8-be1f-434fd34d7682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required classes\n",
    "from swarmauri.llms.concrete.OpenAIModel import OpenAIModel as LLM\n",
    "from swarmauri.conversations.concrete.Conversation import Conversation\n",
    "from swarmauri.messages.concrete.HumanMessage import HumanMessage\n",
    "from swarmauri.messages.concrete.SystemMessage import SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b9370283-b37d-4d6f-be36-dcad39eb7a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "llm = LLM(api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee07c3b-5e4f-4a89-aa49-6c46d9411dbd",
   "metadata": {},
   "source": [
    "# Section 1: Streaming Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a8da60dc-21d9-471a-a3a6-57c42ced7285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streaming Output\n",
    "conversation = Conversation()\n",
    "input_text = \"Write a story about a brave knight.\"\n",
    "conversation.add_message(HumanMessage(content=input_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a346ad-7e65-4047-9248-ed2332cc9a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "collected_tokens = []\n",
    "for token in llm.stream(conversation=conversation):\n",
    "    print(token, end=\"\")  # Real-time token streaming\n",
    "    collected_tokens.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d6fa9e-e379-4c66-a42d-3f67cd6f7b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\nFull Response:\", \"\".join(collected_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f2535c-6723-41c1-a878-fbc401109dad",
   "metadata": {},
   "source": [
    "## Streaming Example\n",
    "\n",
    "- This example shows how tokens are streamed in real-time for a prompt.\n",
    "- Streaming is demonstrated by a prompt asking for a story about a brave knight. As the model generates each token, it is printed immediately, simulating a real-time response.\n",
    "- This is particularly useful in cases where immediate feedback is valuable, such as in user-facing applications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1dd90d-de08-4082-88ab-001bb8294ef6",
   "metadata": {},
   "source": [
    "# Section 2: Customized Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bc9e412e-090b-4e6f-9d28-65d218611918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customized Text Generation\n",
    "conversation = Conversation()\n",
    "custom_context = \"Always respond with empathy.\"\n",
    "conversation.add_message(SystemMessage(content=custom_context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "55f50fb2-d4da-4d53-83d9-3082c99e66bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = [\n",
    "    \"I just lost my job. What should I do?\",\n",
    "    \"I'm feeling very stressed about my exams.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3682cf-ae4d-450e-8162-702312318adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for input_text in input_texts:\n",
    "    conversation.add_message(HumanMessage(content=input_text))\n",
    "    llm.predict(conversation=conversation)\n",
    "\n",
    "    # Display each user prompt, model response, and usage statistics\n",
    "    print(\"User Prompt:\", input_text)\n",
    "    print(\"Response:\", conversation.get_last().content)\n",
    "    print(\"Usage Data:\", conversation.get_last().usage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cb26bf-3f0d-44f7-b42d-a10be7d487d9",
   "metadata": {},
   "source": [
    "## Customized Text Generation:\n",
    "\n",
    "- We add a `SystemMessage` to set a specific behavior for the model. In this case, we instruct it to “respond with empathy.”\n",
    "- This message affects all following interactions within this conversation.\n",
    "- We test two prompts that could benefit from empathetic responses. Observe how the model’s answers are influenced by the system message."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb07a59d-907f-4d10-a33c-d14ac2ed70c2",
   "metadata": {},
   "source": [
    "# Section 3: Batch Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "668d17fd-1d35-4df5-89bd-6bfaa7356337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Generation\n",
    "batch_prompts = [\"What is AI?\", \"Explain machine learning.\", \"How does deep learning differ from ML?\"]\n",
    "conversations = [Conversation() for _ in batch_prompts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f7be61b9-2be9-4b6d-b543-a4c9af95ed3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add messages to each conversation\n",
    "for conv, prompt in zip(conversations, batch_prompts):\n",
    "    conv.add_message(HumanMessage(content=prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b236374-b15a-43d0-a04f-c1e83b2d2333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run batch prediction\n",
    "results = llm.batch(conversations=conversations)\n",
    "for conv, result in zip(conversations, results):\n",
    "    print(\"Prompt:\", conv.get_first().content)\n",
    "    print(\"Response:\", result.get_last().content)\n",
    "    print(\"Usage Data:\", result.get_last().usage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246bade3-6162-4768-9fc5-7ab75307d845",
   "metadata": {},
   "source": [
    "## Batch Example:\n",
    "\n",
    "- In this section, multiple prompts are processed simultaneously in a batch, simulating a high-volume use case.\n",
    "- Each prompt is handled independently, and responses are generated in parallel, improving efficiency for multi-query applications such as chatbots or FAQs.\n",
    "- We display each prompt, its response, and the associated usage data, allowing us to see how batch processing impacts resource usage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a90916-a71e-47fa-a522-863815e4474f",
   "metadata": {},
   "source": [
    "# Notebook Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b464dd32-4735-4bda-9e63-2e755b67f272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Huzaifa Irshad\n",
      "GitHub Username: irshadhuzaifa\n",
      "Last Modified: 2024-11-05 18:32:04.232149\n",
      "Platform: Windows 11\n",
      "Python Version: 3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:17:27) [MSC v.1929 64 bit (AMD64)]\n",
      "Swarmauri Version: Swarmauri Version: 0.5.1\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# Display author information\n",
    "author_name = \"Huzaifa Irshad\" \n",
    "github_username = \"irshadhuzaifa\"  \n",
    "\n",
    "print(f\"Author: {author_name}\")\n",
    "print(f\"GitHub Username: {github_username}\")\n",
    "\n",
    "# Last modified datetime (file's metadata)\n",
    "notebook_file = \"Notebook_04_Advanced_Text_Generation.ipynb\"\n",
    "try:\n",
    "    last_modified_time = os.path.getmtime(notebook_file)\n",
    "    last_modified_datetime = datetime.fromtimestamp(last_modified_time)\n",
    "    print(f\"Last Modified: {last_modified_datetime}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not retrieve last modified datetime: {e}\")\n",
    "\n",
    "# Display platform, Python version, and Swarmauri version\n",
    "print(f\"Platform: {platform.system()} {platform.release()}\")\n",
    "print(f\"Python Version: {sys.version}\")\n",
    "\n",
    "import swarmauri\n",
    "\n",
    "try:\n",
    "    version = swarmauri.__version__\n",
    "except AttributeError:\n",
    "    version = f\"Swarmauri Version: 0.5.1\"\n",
    "\n",
    "print(f\"Swarmauri Version: {version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8a15e3-2c03-4f50-8513-a119c314da5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swarmauri(0.5.1)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
