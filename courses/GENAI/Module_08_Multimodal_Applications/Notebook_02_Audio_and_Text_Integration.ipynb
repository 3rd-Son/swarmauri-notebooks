{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Audio and Text Integration using Swarmauri**\n",
    "## **Introduction**\n",
    "This notebook explores audio processing capabilities using swarmauri, including text-to-speech with OpenAIAudioTTS and audio transcription with GroqAIAudio.\n",
    "\n",
    "As language models and speech recognition technologies continue to advance, the ability to bridge the gap between spoken and written communication has become increasingly important.\n",
    "\n",
    "We'll start by exploring text-to-speech (TTS) generation, where we'll learn how to convert text into  audio output. This capability opens the door to a wide range of applications, from automated narration and audiobook creation to assistive technologies and voice interfaces.\n",
    "\n",
    "Next, we'll learn  about speech-to-text (STT) transcription, enabling us to convert audio recordings into text. This functionality is crucial for a variety of use cases, such as meeting transcripts, video subtitling, and language translation.\n",
    "Throughout the notebook, we'll showcase the swarmauri library's integration with powerful AI models, such as OpenAIAudioTTS and GroqAIAudio, to provide robust and reliable audio processing capabilities.\n",
    "## **Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from swarmauri.llms.concrete.OpenAIAudioTTS import OpenAIAudioTTS\n",
    "from swarmauri.llms.concrete.GroqAIAudio import GroqAIAudio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load environment variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialize models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tts_model = OpenAIAudioTTS(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "transcribe_model = GroqAIAudio(api_key=os.getenv(\"GROQ_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Text-to-Speech Generation**\n",
    "**Basic TTS generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated audio saved to: output.mp3\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, this is a test of text-to-speech output.\"\n",
    "audio_path = \"output.mp3\"\n",
    "result = tts_model.predict(text=text, audio_path=audio_path)\n",
    "print(f\"Generated audio saved to: {Path(result).name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stream audio chunks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = []\n",
    "for chunk in tts_model.stream(text=text):\n",
    "    chunks.append(chunk)\n",
    "full_audio = b\"\".join(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Speech-to-Text Transcription**\n",
    "**Basic transcription**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription:  Thank you.\n"
     ]
    }
   ],
   "source": [
    "# Basic transcription\n",
    "audio_file = \"Recording (2).m4a\"\n",
    "transcription = transcribe_model.predict(audio_path=audio_file)\n",
    "print(f\"Transcription: {transcription}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Translation task**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation:  Hello. Hello. May I? Yes, of course. Oh, my God. Sorry. It's all right. I'm just a little bit tired. I'm going to go and get some rest. I'm going to go and get some rest. I'm going to go and get some rest. I'm going to go and get some rest. I'm going to go and get some rest. I'm going to go and get some rest. I'm going to go and get some rest. I'm going to go and get some rest. I'm going to go and get some rest. I'm going to go and get some rest. I'm going to go and get some rest. I'm going to go and get some rest. I'm going to go and get some rest. I'm going to go and get some rest. Sorry. It's all right. What's your name? Cécile. And you? My name is François. I'm delighted to meet you. You come from here? Yes, I live here. And you? I come from La Rochelle. What do you do in life? I'm a gallerist. I have a small gallery in the 10th. I'm a gallerist. I have a small gallery in the 10th. I'm a gallerist. I have a small gallery in the 10th. I'm a gallerist. I'm a gallerist. I'm a gallerist. I'm a gallerist. I'm a gallerist. I'm a gallerist. I'm a gallerist. I'm a gallerist. I'm a gallerist. I have a small gallery in the 10th. And you? I'm an actor. I play in a small theatre. Ah, interesting. And how old are you? I'm 32. And you? Guess 30 maybe No I 31 years old You know I looking for a small apartment Do you have any advice Can you help me Yes, I can help you. But it's already late, I still have a meeting with a client. Do you have time tomorrow around 3pm to go for a coffee? Yes, of course. Can you give me your phone number? So, my mobile number is 06 23 04 87 58. Ok. And yours? Mine is 06 89 54 42 61. I'm sorry. I'll call you tomorrow for our meeting. All right. Goodbye. Goodbye. See you tomorrow. See you tomorrow.\n"
     ]
    }
   ],
   "source": [
    "# Translation task\n",
    "french_audio = \"French.mp3\"\n",
    "translation = transcribe_model.predict(\n",
    "    audio_path=french_audio,\n",
    "    task=\"translation\"\n",
    ")\n",
    "print(f\"Translation: {translation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Batch Processing**\n",
    "\n",
    "**Batch TTS processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch TTS processing\n",
    "text_paths = {\n",
    "    \"Hello\": \"hello.mp3\",\n",
    "    \"Good morning\": \"morning.mp3\",\n",
    "    \"Welcome\": \"welcome.mp3\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: hello.mp3\n",
      "Generated: morning.mp3\n",
      "Generated: welcome.mp3\n"
     ]
    }
   ],
   "source": [
    "results = tts_model.batch(text_path_dict=text_paths)\n",
    "for path in results:\n",
    "    file_name = Path(path).name\n",
    "    print(f\"Generated: {file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Async Processing**\n",
    "**Async TTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "async_test.mp3\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "# Async TTS\n",
    "text = \"This is an async test\"\n",
    "result = await tts_model.apredict(text=text, audio_path=\"async_test.mp3\")\n",
    "print (Path(result).name)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Async batch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: first.mp3\n",
      "Generated: second.mp3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    # Async batch\n",
    "    text_paths = {\n",
    "        \"First message\": \"first.mp3\",\n",
    "        \"Second message\": \"second.mp3\"\n",
    "    }\n",
    "    batch_results = await tts_model.abatch(text_path_dict=text_paths)\n",
    "    for path in batch_results:\n",
    "        file_name = Path(path).name\n",
    "        print(f\"Generated: {file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Conclusion**\n",
    "**By the end of this notebook, you will have a comprehensive understanding of how to use audio and text integration using swarmauri. You'll be able to:**\n",
    "\n",
    "Text-to-speech generation\n",
    "\n",
    "Audio transcription and translation\n",
    "\n",
    "Implement batch processing for both text-to-speech and speech-to-text tasks, improving efficiency and scalability.\n",
    "\n",
    "Explore asynchronous audio processing, enabling you to handle long-running tasks and improve the responsiveness of your applications.\n",
    "\n",
    "This notebook lays the foundation for incorporating multimodal capabilities into your projects. \n",
    "We encourage you to build upon the concepts and techniques presented here, and to continuously explore  audio and text integration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swarmauri-0.5.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
