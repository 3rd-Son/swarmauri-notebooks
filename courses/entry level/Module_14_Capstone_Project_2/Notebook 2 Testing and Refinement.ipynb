{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Notebook 2: Testing and Refinement - (Peer Review and Feedback)**\n",
    "## **Introduction:**\n",
    "\n",
    "In this notebook, weâ€™ll focus on testing and refining the RAG agent developed in the previous step. By simulating different queries and examining the responses, you can identify areas where the retrieval or generation could be improved. Testing is crucial for ensuring robust performance, especially in complex systems involving both retrieval and generation. We encourage you to share your results with peers and gather feedback to improve your project.\n",
    "\n",
    "After running the queries, analyze the results. Are they accurate? Do they match your expectations? Share your findings with your peers to gain new insights and potential improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RagAgent Resource Type: Agent\n"
     ]
    }
   ],
   "source": [
    "# Import dependencies\n",
    "import os\n",
    "from swarmauri.chunkers.concrete.SentenceChunker import SentenceChunker\n",
    "from swarmauri_community.parsers.concrete.FitzPdfParser import PDFtoTextParser as Parser\n",
    "from swarmauri.llms.concrete.GroqModel import GroqModel\n",
    "from swarmauri.vector_stores.concrete.TfidfVectorStore import TfidfVectorStore\n",
    "from swarmauri.agents.concrete.RagAgent import RagAgent\n",
    "from swarmauri.documents.concrete.Document import Document\n",
    "from swarmauri.messages.concrete.SystemMessage import SystemMessage\n",
    "from swarmauri.conversations.concrete.MaxSystemContextConversation import MaxSystemContextConversation\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables \n",
    "load_dotenv()\n",
    "\n",
    "# Initialize PDF parser\n",
    "parser = Parser()\n",
    "file_path = \"main.pdf\"\n",
    "documents = parser.parse(file_path)\n",
    "\n",
    "# Chunk the parsed text\n",
    "chunker = SentenceChunker()\n",
    "chunked_documents = [Document(content=chunk) for chunk in chunker.chunk_text(documents[0].content)]\n",
    "\n",
    "# Initialize Vector Store and add chunked documents\n",
    "vector_store = TfidfVectorStore()\n",
    "vector_store.add_documents(chunked_documents)\n",
    "\n",
    "# Set up the LLM and system context for RagAgent\n",
    "API_KEY = os.getenv(\"API_KEY\")\n",
    "llm = GroqModel(api_key=API_KEY)\n",
    "system_context = SystemMessage(content=\"\"\" Your name is PDF reading assistant. You are to help readers with accurate informations using this article below as context :  {documents}\"\"\")\n",
    "conversation = MaxSystemContextConversation(system_context=system_context, max_size=20)\n",
    "\n",
    "# Initialize RagAgent\n",
    "agent = RagAgent(llm=llm, conversation=conversation, system_context=system_context, vector_store=vector_store)\n",
    "\n",
    "# Check agent resource type\n",
    "print(\"RagAgent Resource Type:\", agent.resource)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use different queries to test the RAGAgent**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query the RagAgent with a prompts based on the pdf content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what are the differences between transformers and convultionary networks\"\n",
    "\n",
    "response = agent.exec(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: what are the differences between transformers and convultionary networks\n",
      "Response: **Transformers and convolutional networks differ in their underlying architectures and how they process information:**\n",
      "\n",
      "**1. Convolutional Networks:**\n",
      "\n",
      "- Uses convolution filters to extract spatial features.\n",
      "- Filters slide over the image, extracting local features from a small neighborhood.\n",
      "- Suitable for capturing spatial dependencies in local regions.\n",
      "\n",
      "\n",
      "**2. Transformers:**\n",
      "\n",
      "- Relies on attention mechanisms to capture spatial relationships between pixels.\n",
      "- Uses self-attention to focus on relevant parts of the image, regardless of their physical distance.\n",
      "- Can capture global context and long-range dependencies more effectively than CNNs.\n",
      "\n",
      "\n",
      "**Key Differences:**\n",
      "\n",
      "- **Parallel Processing:** Transformers can process information in parallel, while CNNs process information sequentially.\n",
      "- **Spatial Attention:** Transformers capture spatial relationships through attention, while CNNs rely on convolution filters.\n",
      "- **Computational Complexity:** Transformers are generally more computationally expensive than CNNs.\n",
      "\n",
      "\n",
      "**Advantages of Transformers:**\n",
      "\n",
      "- Improved accuracy for some object detection tasks.\n",
      "- Can capture long-range dependencies and spatial context more efficiently.\n",
      "- More robust to occlusions and perspective variations.\n",
      "\n",
      "\n",
      "**Advantages of Convolutional Networks:**\n",
      "\n",
      "- More computationally efficient than transformers.\n",
      "- Easier to implement and train.\n",
      "- Suitable for tasks where spatial locality is important\n"
     ]
    }
   ],
   "source": [
    "# Display the response\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Response: {response}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another test query based on the pdf content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_2 = \"According to the article explain DETR\"\n",
    "response_2 = agent.exec(query_2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: According to the article explain DETR\n",
      "Response: **DETR (Detection Transformer)** is a novel object detection model based on transformer networks. It revolutionizes object detection by:\n",
      "\n",
      "**1. Transformer Architecture:**\n",
      "- Uses self-attention to capture spatial relationships between pixels.\n",
      "- Relies on the encoder-decoder architecture to predict object locations and class probabilities.\n",
      "\n",
      "\n",
      "**2. Key Components:**\n",
      "\n",
      "- **Query-Key-Value (QKV) Architecture:** Queries are used to retrieve relevant key-value pairs from the encoded image features, allowing for efficient object detection.\n",
      "- **Object Queries:** Generate object-specific queries to represent the objects in the image.\n",
      "- **Set Prediction:** Predicts the bounding boxes and class probabilities of the detected objects.\n",
      "\n",
      "\n",
      "**3. Advantages:**\n",
      "\n",
      "- **Accuracy:** Outperforms previous object detection models on various benchmarks.\n",
      "- **Efficiency:** Can handle large and complex images efficiently due to parallel processing.\n",
      "- **Generalized Detectability:** More robust to occlusions, clutter, and perspective variations.\n",
      "\n",
      "\n",
      "**4. Applications:**\n",
      "\n",
      "- Autonomous vehicles: Object detection in real-time environments.\n",
      "- Robotics: Object manipulation and navigation.\n",
      "- Medical imaging: Automated disease detection.\n",
      "- Security and surveillance: Object tracking and anomaly detection.\n",
      "\n",
      "\n",
      "**Strengths of DET\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"Query: {query_2}\")\n",
    "print(f\"Response: {response_2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: According to the article explain DINO\n",
      "Response: \n",
      "\n",
      "\n",
      "**DINO (DETR in the wild with objects not in the training set)**:\n",
      "\n",
      "- A variant of DETR that addresses the issue of detecting objects not present in the training set.\n",
      "- Uses a different query generation process that incorporates visual information from the image.\n",
      "- Improves generalization and robustness to unseen objects.\n",
      "\n",
      "\n",
      "**Key Features of DINO:**\n",
      "\n",
      "- **Self-supervised learning:** Uses self-supervised training techniques to learn meaningful representations from unlabeled data.\n",
      "- **Object discovery:** Automatically discovers new objects in the wild without prior knowledge.\n",
      "- **Generalizability:** Detects a wide range of objects in diverse environments.\n",
      "\n",
      "\n",
      "**Applications of DINO:**\n",
      "\n",
      "- Autonomous vehicles: Detecting unseen objects on roads.\n",
      "- Robotics: Object recognition in real-world scenarios.\n",
      "- Search and rescue: Locating people or objects in disaster zones.\n"
     ]
    }
   ],
   "source": [
    "query_3 = \"According to the article explain DINO\"\n",
    "response_3 = agent.exec(query_3)\n",
    "print(f\"Query: {query_3}\")\n",
    "print(f\"Response: {response_3}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Conclusion**\n",
    "\n",
    "By testing the RagAgent and seeking peer feedback, you can enhance the performance and reliability of your solution. Refinement based on constructive input ensures a robust final product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **NOTEBOOK METADATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Dominion John \n",
      "GitHub Username: DOMINION-JOHN1\n",
      "Last Modified: 2024-10-23 12:07:31.426870\n",
      "Platform: Windows 11\n",
      "Python Version: 3.12.7 (tags/v3.12.7:0b05ead, Oct  1 2024, 03:06:41) [MSC v.1941 64 bit (AMD64)]\n",
      "Swarmauri Version: 0.5.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import platform\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# Display author information\n",
    "author_name = \"Dominion John \" \n",
    "github_username = \"DOMINION-JOHN1\"  \n",
    "\n",
    "print(f\"Author: {author_name}\")\n",
    "print(f\"GitHub Username: {github_username}\")\n",
    "\n",
    "# Last modified datetime (file's metadata)\n",
    "notebook_file = \"Notebook 2 Testing and Refinement.ipynb\"\n",
    "try:\n",
    "    last_modified_time = os.path.getmtime(notebook_file)\n",
    "    last_modified_datetime = datetime.fromtimestamp(last_modified_time)\n",
    "    print(f\"Last Modified: {last_modified_datetime}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not retrieve last modified datetime: {e}\")\n",
    "\n",
    "# Display platform, Python version, and Swarmauri version\n",
    "print(f\"Platform: {platform.system()} {platform.release()}\")\n",
    "print(f\"Python Version: {sys.version}\")\n",
    "\n",
    "# Checking Swarmauri version\n",
    "try:\n",
    "    import swarmauri\n",
    "    print(f\"Swarmauri Version: {swarmauri.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"Swarmauri is not installed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swarmauri(0.5.0)",
   "language": "python",
   "name": "swarmauri-0.5.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
