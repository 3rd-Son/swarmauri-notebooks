{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Notebook 3: Document Parsers - Extracting Text from PDFs**\n",
    "## **Introduction:**\n",
    "Document parsers focus on extracting content from various types of documents, such as PDFs, Word files, or images. In this notebook, we’ll explore the FitzPdfParser, which extracts text from PDF files. This is useful when analyzing or processing content from scanned documents or reports."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from swarmauri_community.parsers.concrete.FitzPdfParser import PDFtoTextParser as Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instantiate the parser**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = Parser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Specify the path to the PDF file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"main.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parse the PDF to extract text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = parser.parse(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Display the extracted text from the PDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Content: Engineering Applications of Artificial Intelligence 126 (2023) 107021\n",
      "Available online 4 September 2023\n",
      "0952-1976/© 2023 Elsevier Ltd. All rights reserved.\n",
      "Contents lists available at ScienceDirect\n",
      "Engineering Applications of Artificial Intelligence\n",
      "journal homepage: www.elsevier.com/locate/engappai\n",
      "Survey paper\n",
      "Transformer for object detection: Review and benchmark✩\n",
      "Yong Li a,∗, Naipeng Miao a, Liangdi Ma b, Feng Shuang a, Xingwen Huang a\n",
      "a Guangxi Key Laboratory of Intelligent Control and Maintenance of Power Equipment, School of Electrical Engineering, Guangxi University, No. 100, Daxuedong\n",
      "Road, Xixiangtang District, Nanning, 530004, Guangxi, China\n",
      "b School of Software, Tsinghua University, No. 30 Shuangqing Road, Haidian District, Beijing, 100084, China\n",
      "A R T I C L E\n",
      "I N F O\n",
      "Keywords:\n",
      "Review\n",
      "Object detection\n",
      "Transformer-based models\n",
      "COCO2017 dataset\n",
      "Benchmark\n",
      "A B S T R A C T\n",
      "Object detection is a crucial task in computer vision (CV). With the rapid advancement of Transformer-\n",
      "based models in natural language processing (NLP) and various visual tasks, Transformer structures are\n",
      "becoming increasingly prevalent in CV tasks. In recent years, numerous Transformer-based object detectors\n",
      "have been proposed, achieving performance comparable to mainstream convolutional neural network-based\n",
      "(CNN-based) approaches. To provide researchers with a comprehensive understanding of the development,\n",
      "advantages, disadvantages, and future potential of Transformer-based object detectors in Artificial Intelligence\n",
      "(AI), this paper systematically reviews the mainstream methods and analyzes the limitations and challenges\n",
      "encountered in their current applications, while also offering insights into future research directions. We have\n",
      "reviewed a large number of papers, selected the most prominent Transformer detection methods, and divided\n",
      "them into Transformer Neck and Transformer Backbone categories for introduction and comparative analysis.\n",
      "Furthermore, we have constructed a benchmark using the COCO2017 dataset to evaluate different object\n",
      "detection algorithms. Finally, we summarize the challenges and prospects in this field.\n",
      "1. Introduction\n",
      "Object detection is a fundamental task in computer vision that\n",
      "requires simultaneous classification and localization of potential objects\n",
      "within a single image (Zhao et al., 2019). As such, it plays a cru-\n",
      "cial role in various applications, including autonomous driving (Chen\n",
      "et al., 2015, 2017), face recognition (Sung and Poggio, 1998), pedes-\n",
      "trian detection (Dollar et al., 2012), and medical detection (Kobatake\n",
      "and Yoshinaga, 1996). The performance of object detection directly\n",
      "influences object tracking, environment perception, and scene under-\n",
      "standing (Felzenszwalb et al., 2010). Recently, deep learning-based\n",
      "object detection methods have gained considerable attention due to\n",
      "the rapid development of deep learning. However, numerous challenges\n",
      "remain, such as balancing accuracy and efficiency, handling multi-scale\n",
      "objects, and creating lightweight models.\n",
      "Traditional mainstream object detection methods have predomi-\n",
      "nantly utilized convolutional neural networks (CNNs), including Faster\n",
      "R-CNN (Ren et al., 2016), SSD (Liu et al., 2016), and YOLO with\n",
      "its variants (Redmon et al., 2016; Redmon and Farhadi, 2018, 2017;\n",
      "Bochkovskiy et al., 2020; Ge et al., 2021). Owing to the remark-\n",
      "able success of Transformers in natural language processing (NLP),\n",
      "✩This work was supported by the Guangxi Science and Technology base and Talent Project (Grant No. Guike AD22080043), the Key Laboratory of\n",
      "Advanced Manufacturing Technology, Ministry of Education (Grant No. GZUAMT2021KF04), and the National Natural Science Foundation of China (Grant No.\n",
      "61720106009).\n",
      "∗Corresponding author.\n",
      "E-mail address: yongli@gxu.edu.cn (Y. Li).\n",
      "researchers have endeavored to adapt Transformer architectures for\n",
      "computer vision tasks. As a result, numerous Transformer-based vision\n",
      "models have emerged in recent years, achieving performance levels that\n",
      "are comparable or even superior to their CNN counterparts.\n",
      "Transformer (Vaswani et al., 2017) was initially proposed as an\n",
      "architecture based on the self-attention mechanism for machine trans-\n",
      "lation and sequence modeling tasks (Sutskever et al., 2014). In recent\n",
      "years, Transformer has experienced significant advancements in NLP\n",
      "and has become a mainstream deep learning model, such as BERT (De-\n",
      "vlin et al., 2018) and its variants (Lan et al., 2019; Liu et al., 2019), GPT\n",
      "series (Radford et al., 2018, 2019; Brown et al., 2020), and others. Due\n",
      "to its scalability, Transformer can be pre-trained on large datasets and\n",
      "subsequently fine-tuned for downstream tasks.\n",
      "Transformers in object detection have garnered increasing attention,\n",
      "particularly over the last three years. Several high-performance models\n",
      "have been proposed, such as DETR (Carion et al., 2020), Deformable\n",
      "DETR (Dai et al., 2017), Swin Transformer (Liu et al., 2021b,a),\n",
      "DINO (Zhang et al., 2022a), and more. Currently, Transformer-based\n",
      "models have emerged as a new paradigm in object detection, making a\n",
      "systematic analysis and evaluation of numerous existing Transformer-\n",
      "based detectors essential for future research.\n",
      "https://doi.org/10.1016/j.engappai.2023.107021\n",
      "Received 22 October 2022; Received in revised form 25 May 2023; Accepted 19 August 2023\n",
      "Engineering Applications of Artificial Intelligence 126 (2023) 107021\n",
      "2\n",
      "Y. Li et al.\n",
      "Fig. 1. Chronological overview of most Transformer-based object detection methods.\n",
      "Some reviews (Khan et al., 2021; Liu et al., 2021c; Arkin et al.,\n",
      "2021; Han et al., 2022; Arkin et al., 2022) have provided detailed\n",
      "introductions and analyses of Transformer-based detectors. In contrast\n",
      "to these surveys, our study not only presents a thorough comparison\n",
      "of the strengths and weaknesses of object detectors based on both\n",
      "Transformer and CNN architectures, but also classifies the prevalent\n",
      "Transformer-based detectors into Transformer Backbone and Trans-\n",
      "former Neck categories. Moreover, we systematically analyze their\n",
      "performance, potential, and limitations. We investigate the advance-\n",
      "ments and constraints of various state-of-the-art Transformer-based\n",
      "detectors (Table 6) and establish benchmarks for these methods using\n",
      "the COCO2017 dataset (Tables. 4, 5). We hope this review delivers\n",
      "a comprehensive understanding of Transformer-based object detectors\n",
      "for researchers.\n",
      "We have categorized existing methods into two groups based on\n",
      "the role of Transformer in the overall model architecture: Transformer\n",
      "Neck and Transformer Backbone, as illustrated in Fig. 1. We present\n",
      "a detailed analysis of representative methods, compare these methods\n",
      "horizontally on the COCO2017 dataset (Lin et al., 2014), and summa-\n",
      "rize the novelty of each method, such as Transformer Backbone with\n",
      "hierarchical representation structure, spatial prior acceleration based\n",
      "on sparse attention, and pure sequence processing for object detection,\n",
      "among others. The main contributions of this paper are as follows:\n",
      "1. We provide a comprehensive summary of state-of-the-art Trans-\n",
      "former-based object detectors from the past three years, high-\n",
      "lighting recent breakthroughs in Transformer architecture for\n",
      "object detection. For each representative model, we offer an in-\n",
      "depth analysis while examining its relationship and connections\n",
      "with other models, both incrementally and comparatively. More-\n",
      "over, we compare the strengths and weaknesses of Transformer\n",
      "and CNN architectures, and further discuss the performance, key\n",
      "features, and limitations of both Transformer Neck (DETR-like\n",
      "models) and Transformer Backbone (ViT-like models).\n",
      "2. We comprehensively compare mainstream models on the same\n",
      "dataset, establish a benchmark based on the COCO2017 dataset,\n",
      "and offer insightful discussions.\n",
      "3. We present an in-depth analysis of the transition as Transformer\n",
      "architecture extends from sequence to visual tasks. Furthermore,\n",
      "we discuss the future development of Transformer and CNN\n",
      "approaches in object detection.\n",
      "The rest structure of this paper is organized as follows. Section 2\n",
      "introduces the main object detection datasets and evaluation metrics,\n",
      "as well as the Attention mechanism and Transformer basic architecture.\n",
      "Section 3 outlines the current mainstream Transformer-based object de-\n",
      "tectors. Section 4 discusses the methods of these models in a multi-level\n",
      "comparison. Section 5 concludes the paper with an outlook.\n",
      "2. Transformer architecture\n",
      "Transformer is an architecture based on the attention mechanism\n",
      "proposed by Vaswani et al. (2017) in 2017, which was initially used\n",
      "for machine translation tasks and subsequently achieved great success\n",
      "in NLP (Devlin et al., 2018). The success of Transformer is attributed\n",
      "Fig. 2. Transformer structure. The Input Embedding module of Transformer encoder\n",
      "(left column) can map the input sequence to Embedding space and pass it to encoder\n",
      "module for processing. The Transformer decoder (right column) receives the previous\n",
      "output sequence and the output sequence from the intermediate encoder. The previous\n",
      "output sequence will be shifted one bit to the right, and the start token will be appended\n",
      "before the sequence to get the input from the decoder. The feed-forward network and\n",
      "the multi-head attention module are repeated 𝑁times to form the encoder and decoder.\n",
      "to its unique architecture, whose core design is the Encoder–Decoder\n",
      "structure based on self-attention. As shown in Fig. 2, Transformer con-\n",
      "sists of three main blocks: multi-headed attention, positional encoding,\n",
      "and feed-forward network. Multi-head attention (MHA) block and Feed-\n",
      "forward network block are the main modules of Encoder and Decoder.\n",
      "Position encoding is a vital module to all Transformer variants and is\n",
      "responsible for attaching position information to the input sequence. In\n",
      "this section, these fundamental techniques are described in detail.\n",
      "2.1. Basic architecture\n",
      "The structure of Transformer is based on encoder–decoder. The\n",
      "encoder consists of 𝑁basic encoder modules, as shown in Fig. 2. Every\n",
      "encoder module consists of a multi-head attention module (MHA) and a\n",
      "feed-forward network (FFN). And then, they are cascaded with residual\n",
      "connection and layer normalization one by one. Finally, the output of\n",
      "the encoder module is shown in Eq. (1):\n",
      "𝑂𝑢𝑡𝑝𝑢𝑡= 𝐿𝑎𝑦𝑒𝑟𝑁𝑜𝑟𝑚(𝑥+ 𝑆𝑢𝑏𝐿𝑎𝑦𝑒𝑟(𝑥)),\n",
      "(1)\n",
      "where 𝑥is the input sequence, and 𝑆𝑢𝑏𝐿𝑎𝑦𝑒𝑟represents the attention\n",
      "module or feedforward network.\n",
      "2.2. Self-attention\n",
      "2.2.1. Scaled dot-product attention\n",
      "The self-attention mechanism module, as the core component of\n",
      "Transformer, consists of two main parts: (1) Linear projection layer:\n",
      "the input sequence is mapped into 3 different vectors (query 𝑄, key\n",
      "𝐾, value 𝑉). The input sequences are 𝑋∈R𝑛𝑥×𝑑𝑥and 𝑌∈R𝑛𝑦×𝑑𝑦,\n",
      "Engineering Applications of Artificial Intelligence 126 (2023) 107021\n",
      "3\n",
      "Y. Li et al.\n",
      "Fig. 3. (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of\n",
      "several attention layers running in parallel.\n",
      "where 𝑛and 𝑑denote the length and dimension of the input sequence,\n",
      "respectively. Then 𝑄, 𝐾, and 𝑉are generated as follows:\n",
      "𝑄= 𝑋𝑊𝑄,\n",
      "𝐾= 𝑌𝑊𝐾,\n",
      "𝑉= 𝑌𝑊𝑉,\n",
      "(2)\n",
      "where 𝑊𝑄∈R𝑑𝑥×𝑑𝑞, 𝑊𝐾∈R𝑑𝑦×𝑑𝑘and 𝑊𝑉∈R𝑑𝑦×𝑑𝑣are the learnable\n",
      "weight matrices. The 𝑑𝑞and 𝑑𝑘denotes the dimensions of 𝑊𝑄and 𝑊𝐾,\n",
      "respectively. The dimension of 𝑊𝑉is 𝑑𝑣. When 𝑋= 𝑌, Eq. (2) is the\n",
      "self-attention computation, and when 𝑋≠𝑌, it is the cross-attention\n",
      "computation in the Decoder module.\n",
      "(2) Attention layer: Transformer adopts a special attention method\n",
      "called Scaled Dot-Product Attention, as shown in Fig. 3 (left). The\n",
      "input consists of 𝑄in 𝑑𝑞dimensions, 𝐾in 𝑑𝑘dimensions and 𝑉in\n",
      "𝑑𝑣dimensions, and the scaled attention matrix is calculated as shown\n",
      "in Eq. (3).\n",
      "Attention (𝑄, 𝐾, 𝑉) = softmax\n",
      "(\n",
      "𝑄𝐾𝑇\n",
      "√\n",
      "𝑑𝑘\n",
      ")\n",
      "𝑉,\n",
      "(3)\n",
      "where\n",
      "1\n",
      "√\n",
      "𝑑𝑘is the scaling factor. The attention weights are obtained by\n",
      "computing the dot product of Q for all K. The attention weights are then\n",
      "normalized by the scaling factor\n",
      "1\n",
      "√\n",
      "𝑑𝑘and the softmax layer. The output\n",
      "weights are assigned to the corresponding elements of V to obtain the\n",
      "final attention matrix.\n",
      "2.2.2. Multi-head attention\n",
      "However, the modeling ability of single-head attention is weak.\n",
      "To address this problem, Vaswani et al. (2017) proposed multi-head\n",
      "attention (MHA). The structure is shown in Fig. 3 (right). MHA can\n",
      "enhance the modeling ability of each attention layer without changing\n",
      "the number of parameters.\n",
      "Compared to single-head attention, MHA maps Q, K, and V linearly\n",
      "to different dimensional subspaces (𝑑𝑞, 𝑑𝑘, 𝑑𝑣) to compute similarity and\n",
      "compute the attention function in parallel. As shown in Eq. (4), the\n",
      "resulting vectors are concatenated and mapped again to obtain the final\n",
      "output.\n",
      "MultiHead(𝑄, 𝐾, 𝑉) = Concat ( head 1, … , head h\n",
      ") 𝑊𝑂,\n",
      "where head i = Attention\n",
      "(\n",
      "𝑄𝑊𝑄\n",
      "𝑖, 𝐾𝑊𝐾\n",
      "𝑖, 𝑉𝑊𝑉\n",
      "𝑖\n",
      ")\n",
      ",\n",
      "(4)\n",
      "where 𝑊𝑄\n",
      "𝑖\n",
      "∈R𝑑model ×𝑑𝑞, 𝑊𝐾\n",
      "𝑖\n",
      "∈R𝑑model ×𝑑𝑘, 𝑊𝑉\n",
      "𝑖\n",
      "∈R𝑑model × 𝑑𝑣, 𝑊𝑂\n",
      "𝑖\n",
      "∈\n",
      "R𝑑model ×ℎ𝑑𝑣is the projection parameter matrix. Multi-head attention\n",
      "reduces the dimensionality of each vector when calculating the atten-\n",
      "tion of each head, which reduces overfitting to a certain extent. Since\n",
      "attention has different distributions in different subspaces, this module\n",
      "fuses the feature relationships between different sequence dimensions\n",
      "in vector concatenation.\n",
      "2.3. Position-wise feed-forward networks\n",
      "The output of the MHA layer is fed into the feed-forward network\n",
      "(FFN). FFN is mainly composed of two linear transformations with a\n",
      "RuLU activation in between. The output of FFN can be expressed as\n",
      "shown in Eq. (5):\n",
      "FFN(𝑥) = max (0, 𝑥𝑊1 + 𝑏1\n",
      ") 𝑊2 + 𝑏2,\n",
      "(5)\n",
      "where 𝑊1 and 𝑊2 denote weight matrices of the two fully connected\n",
      "layers.\n",
      "2.4. Positional encoding\n",
      "Unlike CNN and RNN, self-attention computation brings the ad-\n",
      "vantage of parallel computing while losing word order information.\n",
      "Therefore, positional encoding is used to provide positional information\n",
      "to the model. In detail, a position-dependent signal is added to each\n",
      "word embedding for each input sequence to help the model incorporate\n",
      "the order of words. The output of positional encoding has the same\n",
      "dimension as the embedding layer. So it can be superimposed directly\n",
      "on Embedding. The positional information of each token (a sequence\n",
      "of primitives obtained after the text has been divided into words) and\n",
      "its semantic information (Embedding) are fully integrated and passed\n",
      "to the subsequent layer.\n",
      "There are many variants of positional encoding. The original Trans-\n",
      "former uses sine and cosine functions for positional encoding, as shown\n",
      "in Eq. (6).\n",
      "𝑃𝐸(𝑝𝑜𝑠,2𝑖) = sin (𝑝𝑜𝑠∕100002𝑖∕𝑑model ) ,\n",
      "𝑃𝐸(𝑝𝑜𝑠,2𝑖+1) = cos (𝑝𝑜𝑠∕100002𝑖∕𝑑model ) ,\n",
      "(6)\n",
      "where 𝑝𝑜𝑠is the position and 𝑖is the dimension. That is, each dimension\n",
      "of the position encoding corresponds to a sine wave. The wavelengths\n",
      "form a geometric progression from 2𝜋to 10000×2𝜋. For any fixed offset\n",
      "𝑘, 𝑃𝐸𝑝𝑜𝑠+𝑘can be represented as a linear function of 𝑃𝐸𝑝𝑜𝑠.\n",
      "2.5. Remarks\n",
      "The self-attention mechanism allows Transformer to break through\n",
      "the limitation that RNN models cannot be computed in parallel and\n",
      "improve the computational efficiency. Compared with CNN, the self-\n",
      "attentive mechanism has a global perceptual field. The number of\n",
      "operations required to compute the association between two locations\n",
      "does not grow with distance, so it has a stronger ability to learn long-\n",
      "range dependencies. In addition, Transformer has a general modeling\n",
      "capability. Transformer can be regarded as a fully connected graph\n",
      "modeling method that can model heterogeneous nodes by projecting\n",
      "them into a comparable space to compute similarity. Therefore, there is\n",
      "a sufficient theoretical basis for using Transformer for various computer\n",
      "vision tasks based on its general modeling capability. Considering\n",
      "the dimensional differences between images and text, the images are\n",
      "converted into sequences and can then be input into the model for\n",
      "processing.\n",
      "Moreover, we compare the characteristics of CNN and Transformer.\n",
      "As shown in Table 1, Transformer tends to model shapes more but\n",
      "requires massive data for training. In contrast, CNN tends to model local\n",
      "textures more but has to pile many convolutional layers to have a large\n",
      "enough receptive field to get global information (Geirhos et al., 2019).\n",
      "3. Transformer for object detection\n",
      "This section first introduces common datasets and evaluation met-\n",
      "rics for object detection and analyzes classic Transformer-based object\n",
      "detectors. According to their structural difference, We classify the\n",
      "listed detectors as Transformer Neck-based detectors and Transformer\n",
      "Backbone-based detectors. The Transformer Neck-based detector infers\n",
      "the class labels and bounding box coordinates with a set of learnable\n",
      "object queries but does not change the backbone used for feature\n",
      "extraction. Transformer Backbone-based detectors propose a generic\n",
      "visual backbone that flattens the image into a sequence instead of\n",
      "convolution for feature extraction. Multiscale feature fusion is also\n",
      "Engineering Applications of Artificial Intelligence 126 (2023) 107021\n",
      "4\n",
      "Y. Li et al.\n",
      "Table 1\n",
      "Summary of the highlights and limitations of CNN and Transformer.\n",
      "Architecture\n",
      "Highlights\n",
      "Limitations\n",
      "Transformer\n",
      "(1) The attention mechanism\n",
      "amplifies the significance of crucial\n",
      "aspects of an image while reducing\n",
      "the rest, thereby concentrating on\n",
      "more relevant features. This\n",
      "mechanism assists the Transformer in\n",
      "modeling the long-range\n",
      "dependencies of input sequence\n",
      "elements and thus enhances its\n",
      "generalization ability for samples\n",
      "outside the distribution (Bai et al.,\n",
      "2021). (2) Unlike methods such as\n",
      "RNN and LSTM, the Transformer\n",
      "allows for parallel computations. (3)\n",
      "Given its straightforward yet\n",
      "adaptable design, the Transformer\n",
      "can tackle multiple tasks\n",
      "simultaneously, rendering it a\n",
      "potential candidate for a\n",
      "general-purpose model handling\n",
      "various tasks.\n",
      "(1) Transformer-based models are\n",
      "known for their substantial data\n",
      "requirements and computationally\n",
      "expensive nature, particularly when\n",
      "applied to vision tasks (He et al.,\n",
      "2021). (2) They are also\n",
      "characterized by a slower rate of\n",
      "convergence, which can pose\n",
      "challenges in their utilization (Gao\n",
      "et al., 2021). (3) Further, these\n",
      "models often involve high\n",
      "computational overhead, which\n",
      "exacerbates their deployment issues\n",
      "in resource-constrained settings (Li\n",
      "et al., 2022).\n",
      "CNN\n",
      "(1) CNN-based models have strong\n",
      "local feature extraction ability\n",
      "benefited from inductive bias\n",
      "properties such as translation\n",
      "invariance, weight sharing, and\n",
      "sparse connectivity. (2) CNNs can\n",
      "operate in parallel with lower\n",
      "computational complexity than\n",
      "Transformer.\n",
      "(1) CNN rarely encodes relative\n",
      "feature positions, instead favoring\n",
      "receptive field expansion via larger\n",
      "kernels or stacked layers, often\n",
      "reducing local convolution’s\n",
      "computational and statistical\n",
      "efficiency. (2) CNN’s global feature\n",
      "capture is comparatively weaker than\n",
      "Transformer models (Liu et al.,\n",
      "2021b).\n",
      "Table 2\n",
      "Briefing on datasets for object detection.\n",
      "Name\n",
      "Image volume\n",
      "class\n",
      "Source\n",
      "Annotation format\n",
      "VOC2007\n",
      "9963\n",
      "20\n",
      "PASCAL\n",
      "XML\n",
      "VOC2012\n",
      "17112\n",
      "20\n",
      "PASCAL\n",
      "XML\n",
      "COCO2017\n",
      "121408\n",
      "80\n",
      "Microsoft\n",
      "JSON\n",
      "incorporated in many methods to improve detection accuracy and\n",
      "replace the CNN backbone in classical detectors. In reviewing these\n",
      "methods, we summarize the optimization innovations or modules of the\n",
      "different methods. Finally, we compare their performance in Table 4\n",
      "and Table 5 and give analyzation and discussion on improvements of\n",
      "the above methods.\n",
      "3.1. Common datasets and evaluation metrics\n",
      "3.1.1. Common datasets for object detection\n",
      "Datasets are the basis for measuring and comparing algorithm\n",
      "performance. The commonly used object detection datasets are Pas-\n",
      "cal VOC2007(Everingham et al., 2007), Pascal VOC2012(Everingham\n",
      "et al., 2012) and Microsoft COCO2017(Lin et al., 2014), as shown in\n",
      "Table 2. The Pascal VOC dataset has only 20 object categories and is\n",
      "regarded as a benchmark dataset for object detection. Compared with\n",
      "VOC, the COCO dataset has more small objects and more objects in\n",
      "a single image, and most of the objects are non-centrally distributed\n",
      "and more similar to the real environment. Thus COCO dataset is\n",
      "more difficult for object detection and has been the mainstream object\n",
      "detection dataset in recent years.\n",
      "3.1.2. Evaluation metrics\n",
      "Common evaluation metrics for object detection include Precision,\n",
      "Recall, Average Precision (AP), and mean Average Precision (mAP). In\n",
      "addition to classification, the object detection task localizes the object\n",
      "further with a bounding box associated with its corresponding confi-\n",
      "dence score to report how certain the bounding box of the object class\n",
      "is detected. Therefore to determine how many objects were detected\n",
      "correctly and how many false positives were generated, we use the\n",
      "Intersection over Union (IoU) metric.\n",
      "Intersection over Union (IoU). IoU is an evaluation metric that\n",
      "quantifies the similarity between the ground truth bounding box\n",
      "(𝑔𝑡𝑏𝑜𝑥) and the predicted bounding box (𝑝𝑑𝑏𝑜𝑥) to evaluate how good\n",
      "the predicted box is. The IoU score ranges from 0 to 1; the closer the\n",
      "two boxes, the higher the IoU score. It can be calculated as follow:\n",
      "𝐼𝑜𝑈(𝑔𝑡, 𝑝𝑑) = area(𝑔𝑡𝑏𝑜𝑥∩𝑝𝑑𝑏𝑜𝑥)\n",
      "area(𝑔𝑡𝑏𝑜𝑥∪𝑝𝑑𝑏𝑜𝑥) ,\n",
      "(7)\n",
      "For the IoU threshold at 𝛼, True Positive(TP) is a detection for\n",
      "which 𝐼𝑜𝑈(𝑔𝑡, 𝑝𝑑) ≥𝛼and False Positive (FP) is a detection for which\n",
      "𝐼𝑜𝑈(𝑔𝑡, 𝑝𝑑) ≤𝛼. False Negative (FN) is a ground-truth missed together\n",
      "with 𝑔𝑡for which 𝐼𝑜𝑈(𝑔𝑡, 𝑝𝑑) ≤𝛼. The definitions of TP, TN, FP and FN\n",
      "are shown in Table 3.\n",
      "Precision. Precision is the probability of the predicted bounding\n",
      "boxes matching actual ground truth boxes, also referred to as the\n",
      "positive predictive value. Precision scores range from 0 to 1, with a\n",
      "high precision implying that most detected objects match ground truth\n",
      "objects.\n",
      "Recall. Recall is the true positive rate, also referred to as sensitivity,\n",
      "which measures the probability of ground truth objects being correctly\n",
      "detected. Similarly, Recall ranges from 0 to 1, where a high recall score\n",
      "means that most ground truth objects were detected.\n",
      "The Precision and Recall can be calculated as follow:\n",
      "Precision =\n",
      "TP\n",
      "TP + FP ,\n",
      "(8)\n",
      "Recall =\n",
      "TP\n",
      "TP + FN ,\n",
      "(9)\n",
      "Average precision (AP). AP is Area Under the Precision–Recall\n",
      "Curve evaluated at a specific IoU threshold. AP is a single number\n",
      "metric that combines precision and recall and describes the Accuracy–\n",
      "Recall curve by AP among recall values ranging from 0 to 1. It is used\n",
      "to evaluate the performance of object detectors.\n",
      "Engineering Applications of Artificial Intelligence 126 (2023) 107021\n",
      "5\n",
      "Y. Li et al.\n",
      "Table 3\n",
      "Definition of terms.\n",
      "Terms\n",
      "Definitions\n",
      "TP (True Positive)\n",
      "Positive samples are correctly identified as positive samples.\n",
      "TN (True Negative)\n",
      "Negative samples are correctly identified as negative samples.\n",
      "FP (False Positive)\n",
      "False positive samples, that is, negative samples are mistakenly identified as positive samples.\n",
      "FN (False Negative)\n",
      "False negative samples, that is, positive samples are wrongly identified as negative samples.\n",
      "Fig. 4. The pipeline of DETR. The backbone is a convolutional neural network (CNN) that serves as a feature extractor. And Transformer is the core of the DETR architecture,\n",
      "consisting of an encoder and a decoder. The high-dimensional feature map from the backbone is flattened and fed into the encoder. Then encoder processes the spatial information\n",
      "and outputs a sequence of encoded feature vectors. Finally, The output of the decoder is passed through a series of linear layers to predict the final bounding box coordinates and\n",
      "class probabilities for each object query.\n",
      "Source: Image from Carion et al. (2020).\n",
      "Mean average precision (mAP). AP is calculated for each class\n",
      "individually, and mAP is the average of AP values across all classes.\n",
      "The mAP can be calculated as Eq. (10). There are two kinds of mAPs\n",
      "commonly used. (1) PASCAL VOC challenge uses mAP as a metric\n",
      "with an IoU threshold of 0.5. (2) While MS COCO averages mAP over\n",
      "different IoU thresholds 50% to 95% with a step of 0.05, this metric\n",
      "is denoted in papers by mAP@[.5,.95]. Therefore, COCO not only\n",
      "averages AP over all classes but also on the defined IoU thresholds.\n",
      "mAP =\n",
      "∑𝑘\n",
      "𝑖=1 AP𝑖\n",
      "𝑘\n",
      "for 𝑘classes,\n",
      "(10)\n",
      "Frame Per Second (FPS). FPS defines how fast your object detec-\n",
      "tion model processes your video and generates the desired output.\n",
      "3.2. Transformer neck\n",
      "In this section, we review the classic Transformer Neck-based ob-\n",
      "ject detection models in last two years, starting from the original\n",
      "Transformer detector DETR (Carion et al., 2020). The original DETR\n",
      "regards object detection as end-to-end set prediction, thus removing\n",
      "hand-designed components such as anchor boxes and non-maximum\n",
      "suppression (NMS). However, some drawbacks need to be solved in\n",
      "DETR, such as slow convergence and poor detection of small objects.\n",
      "Therefore, many approaches (sparse attention, spatial prior acceler-\n",
      "ation, multi-scale detection) have been proposed to improve it by\n",
      "researchers. We compare the performance of all methods together on\n",
      "the COCO2017 dataset with the benchmark shown in Table 4.\n",
      "3.2.1. DETR\n",
      "DETR proposed by Carion et al. (2020) is the first object detector\n",
      "that successfully uses the Transformer as the main module in object\n",
      "detection. DETR not only has a simpler and more flexible structure\n",
      "but also has comparable performance compared to previous SOTA\n",
      "approaches, such as the highly optimized Faster R-CNN. Unlike classical\n",
      "object detectors, DETR is an end-to-end object detection model. It gets\n",
      "rid of the autoregressive model, performs parallel inference on object\n",
      "relationships and global image context, and then outputs the final\n",
      "predictions. The structure of DETR is shown in Fig. 4.\n",
      "DETR treats the object detection task as an intuitive set prediction\n",
      "problem and discards some traditional hand-craft components such as\n",
      "hand-designed anchor sets and non-maximal suppression (NMS). As\n",
      "shown in Fig. 4, DETR uses CNN Backbone to learn the 2D features\n",
      "of the input image. Then feature maps are unfolded into sequences and\n",
      "fed to the Transformer encoder module (where there is still positional\n",
      "encoding). The output of the Transformer Decoder module is then\n",
      "obtained under the constraint of object queries. Finally, the class and\n",
      "bounding box regression parameters are obtained after a feedforward\n",
      "network.\n",
      "Based on the idea of sequential prediction, DETR regards the predic-\n",
      "tion of the network as a fixed sequence ̃𝑦of length N, ̃𝑦= ̃𝑦𝑖, 𝑖∈(1, 𝑁),\n",
      "(where the value of 𝑁is fixed and much larger than the number of\n",
      "Ground Truth in the image), ̃𝑦𝑖= (̃𝑐𝑖, ̃𝑏𝑖\n",
      "). Meanwhile, the Ground Truth\n",
      "is considered as a sequence 𝑦∶𝑦𝑖= (𝑐𝑖, 𝑏𝑖\n",
      ") (the length must be less\n",
      "than N, so the sequence is filled with 𝜙(for no object), which can be\n",
      "interpreted as the category of background, to make its length equal to\n",
      "N), where 𝑐𝑖denotes the true category to which the object belongs,\n",
      "and 𝑏𝑖denotes a quaternion (containing the center point coordinates\n",
      "and the width and height of the object box, and both are relative to\n",
      "the scale coordinates of the image).\n",
      "So the prediction task can be viewed as a bipartite matching prob-\n",
      "lem between 𝑦and ̃𝑦, with the Hungarian algorithm as the solution\n",
      "method, defining the strategy for minimum matching as follows:\n",
      "̂𝜎= arg min\n",
      "𝜎∈S𝑁\n",
      "𝑁\n",
      "∑\n",
      "𝑖\n",
      "Lmatch\n",
      "(𝑦𝑖, ̂𝑦𝜎(𝑖)\n",
      ") ,\n",
      "(11)\n",
      "where ̃𝜎denotes the matching strategy when finding the minimum\n",
      "loss, for L while considering the similarity prediction between Ground\n",
      "Truth boxes. For 𝜎(𝑖), 𝑐𝑖the predicted category confidence is ̃𝑃𝜎(𝑖)\n",
      "(𝑐𝑖\n",
      ")\n",
      "and the bounding box prediction is ̃𝑏𝜎(𝑖), for non-empty matches, define\n",
      "Lmatch\n",
      "(𝑦𝑖, ̂𝑦𝜎(𝑖)\n",
      ") as: −1{𝑐𝑖≠∅} ̂𝑝𝜎(𝑖)\n",
      "(𝑐𝑖\n",
      ") + 1{𝑐𝑖≠∅}Lbox\n",
      "(𝑏𝑖, ̂𝑏𝜎(𝑖)\n",
      ").\n",
      "In this way, the overall loss is obtained as\n",
      "LHungarian (𝑦, ̂𝑦) =\n",
      "𝑁\n",
      "∑\n",
      "𝑖=1\n",
      "[\n",
      "−log ̂𝑝̂𝜎(𝑖)\n",
      "(𝑐𝑖\n",
      ") + 1{𝑐𝑖≠∅}Lbox\n",
      "(𝑏𝑖, ̂𝑏̂𝜎(𝑖))]\n",
      ",\n",
      "(12)\n",
      "Considering the bounding box scale, the 𝐿1 loss and the IoU loss are\n",
      "linearly combined to obtain the L𝑏𝑜𝑥loss:\n",
      "L𝑏𝑜𝑥= 𝜆iou Liou\n",
      "(𝑏𝑖, ̂𝑏𝜎(𝑖)\n",
      ") + 𝜆L1 ‖‖‖𝑏𝑖−̂𝑏𝜎(𝑖)‖‖‖1 ,\n",
      "(13)\n",
      "Engineering Applications of Artificial Intelligence 126 (2023) 107021\n",
      "6\n",
      "Y. Li et al.\n",
      "Table 4\n",
      "Comparison between Transformer Necks and representative CNNs on COCO2017 Val set. ‘‘Multi-Scale’’ refers to multi-scale inputs. AP denotes IoU threshold = .50:.05:.95. AP50\n",
      "and AP75 denote IoU threshold = .50 and .75. In addition, AP𝑆, AP𝑀, AP𝐿denote different scales of objects. Small means area < 3232. Medium means 3232 < area < 9696. Large\n",
      "means area > 9696.\n",
      "Method\n",
      "Backbone\n",
      "Epochs\n",
      "GFLOPs\n",
      "#Params(M)\n",
      "Multi-scale\n",
      "FPS\n",
      "AP\n",
      "𝐀𝐏50\n",
      "𝐀𝐏75\n",
      "𝐀𝐏𝑆\n",
      "𝐀𝐏𝑀\n",
      "𝐀𝐏𝐿\n",
      "Faster R-CNN + FPN (Ren et al., 2016)\n",
      "ResNet50\n",
      "109\n",
      "180\n",
      "42\n",
      "–\n",
      "26\n",
      "42.0\n",
      "62.1\n",
      "45.5\n",
      "26.6\n",
      "45.4\n",
      "53.4\n",
      "DERR+ (Carion et al., 2020)\n",
      "ResNet50\n",
      "500\n",
      "86\n",
      "41\n",
      "–\n",
      "28\n",
      "42.0\n",
      "62.4\n",
      "44.2\n",
      "20.5\n",
      "45.8\n",
      "61.1\n",
      "DETR-DC5+ (Carion et al., 2020)\n",
      "500\n",
      "187\n",
      "41\n",
      "–\n",
      "12\n",
      "43.4\n",
      "63.1\n",
      "45.9\n",
      "22.5\n",
      "47.3\n",
      "61.1\n",
      "DERR (Carion et al., 2020)\n",
      "50\n",
      "86\n",
      "41\n",
      "–\n",
      "12\n",
      "42.0\n",
      "62.4\n",
      "44.2\n",
      "20.5\n",
      "45.8\n",
      "61.1\n",
      "DETR-DC5 (Carion et al., 2020)\n",
      "50\n",
      "187\n",
      "41\n",
      "–\n",
      "12\n",
      "43.4\n",
      "63.1\n",
      "45.9\n",
      "22.5\n",
      "47.3\n",
      "61.1\n",
      "UP-DETR (Dai et al., 2021a)\n",
      "ResNet50\n",
      "150\n",
      "86\n",
      "41\n",
      "–\n",
      "28\n",
      "40.5\n",
      "60.8\n",
      "42.6\n",
      "19.0\n",
      "44.4\n",
      "60.0\n",
      "UP-DETR+ (Dai et al., 2021a)\n",
      "300\n",
      "86\n",
      "41\n",
      "–\n",
      "28\n",
      "42.8\n",
      "63.0\n",
      "45.3\n",
      "20.8\n",
      "47.1\n",
      "61.7\n",
      "Deformable DETR (Zhu et al., 2021)\n",
      "ResNet50\n",
      "50\n",
      "173\n",
      "40\n",
      "–\n",
      "19\n",
      "43.8\n",
      "62.6\n",
      "47.7\n",
      "26.4\n",
      "47.1\n",
      "58.0\n",
      "Two-stage Deformable DETR (Zhu et al., 2021)\n",
      "50\n",
      "173\n",
      "40\n",
      "–\n",
      "19\n",
      "46.2\n",
      "65.2\n",
      "50.0\n",
      "28.8\n",
      "49.2\n",
      "61.7\n",
      "Conditional DETR (Meng et al., 2021)\n",
      "ResNet50\n",
      "108\n",
      "90\n",
      "44\n",
      "–\n",
      "–\n",
      "43.0\n",
      "64.0\n",
      "45.7\n",
      "22.7\n",
      "46.7\n",
      "61.5\n",
      "Conditional DETR-DC5 (Meng et al., 2021)\n",
      "108\n",
      "195\n",
      "44\n",
      "–\n",
      "–\n",
      "45.1\n",
      "65.4\n",
      "48.5\n",
      "25.3\n",
      "49.0\n",
      "62.2\n",
      "ACT-MTKD(L=16) (Zheng et al., 2021)\n",
      "ResNet50\n",
      "–\n",
      "156\n",
      "–\n",
      "–\n",
      "14\n",
      "40.6\n",
      "–\n",
      "–\n",
      "18.5\n",
      "44.3\n",
      "59.7\n",
      "ACT-MTKD(L=32) (Zheng et al., 2021)\n",
      "–\n",
      "169\n",
      "–\n",
      "–\n",
      "16\n",
      "43.1\n",
      "–\n",
      "–\n",
      "22.2\n",
      "47.1\n",
      "61.4\n",
      "SMCA (Gao et al., 2021)\n",
      "ResNet50\n",
      "50\n",
      "152\n",
      "40\n",
      "–\n",
      "10\n",
      "43.7\n",
      "63.6\n",
      "47.2\n",
      "24.2\n",
      "47.0\n",
      "60.4\n",
      "SMCA+ (Gao et al., 2021)\n",
      "50\n",
      "152\n",
      "108\n",
      "–\n",
      "10\n",
      "45.6\n",
      "65.5\n",
      "49.1\n",
      "25.9\n",
      "49.3\n",
      "62.6\n",
      "Efficient DETR (Yao et al., 2021)\n",
      "ResNet50\n",
      "36\n",
      "159\n",
      "32\n",
      "–\n",
      "–\n",
      "44.2\n",
      "62.2\n",
      "48.0\n",
      "28.4\n",
      "47.5\n",
      "56.6\n",
      "Efficient DETR* (Yao et al., 2021)\n",
      "36\n",
      "210\n",
      "35\n",
      "–\n",
      "–\n",
      "45.1\n",
      "65.4\n",
      "48.5\n",
      "25.3\n",
      "49.0\n",
      "62.2\n",
      "TSP-FCOS (Sun et al., 2021)\n",
      "ResNet50\n",
      "36\n",
      "189\n",
      "51.5\n",
      "–\n",
      "15\n",
      "43.1\n",
      "62.3\n",
      "47.0\n",
      "26.6\n",
      "46.8\n",
      "55.9\n",
      "TSP-RCNN (Sun et al., 2021)\n",
      "36\n",
      "188\n",
      "64\n",
      "–\n",
      "11\n",
      "43.8\n",
      "63.3\n",
      "48.3\n",
      "28.6\n",
      "46.9\n",
      "55.7\n",
      "TSP-RCNN+ (Sun et al., 2021)\n",
      "96\n",
      "188\n",
      "64\n",
      "–\n",
      "11\n",
      "45.0\n",
      "64.5\n",
      "49.6\n",
      "29.7\n",
      "47.7\n",
      "58.0\n",
      "YOLOS-S (Fang et al., 2021)\n",
      "DeiT-S\n",
      "150\n",
      "200\n",
      "30.7\n",
      "–\n",
      "7\n",
      "36.1\n",
      "56.4\n",
      "37.1\n",
      "15.3\n",
      "38.5\n",
      "56.1\n",
      "YOLOS-S (Fang et al., 2021)\n",
      "150\n",
      "179\n",
      "27.9\n",
      "–\n",
      "5\n",
      "37.6\n",
      "57.6\n",
      "39.2\n",
      "15.9\n",
      "40.2\n",
      "57.3\n",
      "YOLOS-B (Fang et al., 2021)\n",
      "DeiT-B\n",
      "150\n",
      "537\n",
      "127\n",
      "–\n",
      "–\n",
      "42.0\n",
      "62.2\n",
      "44.5\n",
      "19.5\n",
      "45.3\n",
      "62.1\n",
      "PnP-DETR-R50-DC5-𝛼-0.33 (Wang et al., 2021b)\n",
      "ResNet50\n",
      "500\n",
      "20.7(omit backbone)\n",
      "–\n",
      "–\n",
      "–\n",
      "42.7\n",
      "62.8\n",
      "45.1\n",
      "22.4\n",
      "46.2\n",
      "60.0\n",
      "PnP-DETR-R50-DC5-𝛼-0.5 (Wang et al., 2021b)\n",
      "500\n",
      "32.9(omit backbone)\n",
      "–\n",
      "–\n",
      "–\n",
      "43.1\n",
      "63.4\n",
      "45.3\n",
      "22.7\n",
      "46.5\n",
      "61.1\n",
      "Dynamic DETR (Dai et al., 2021c)\n",
      "ResNet50\n",
      "40\n",
      "–\n",
      "–\n",
      "–\n",
      "–\n",
      "47.2\n",
      "65.9\n",
      "51.1\n",
      "28.6\n",
      "49.3\n",
      "59.1\n",
      "Anchor DETR-C5 (Wang et al., 2021c)\n",
      "ResNet50\n",
      "50\n",
      "–\n",
      "–\n",
      "–\n",
      "–\n",
      "42.1\n",
      "63.1\n",
      "44.9\n",
      "22.3\n",
      "46.2\n",
      "60.0\n",
      "Anchor DETR-DC5 (Wang et al., 2021c)\n",
      "50\n",
      "–\n",
      "–\n",
      "–\n",
      "–\n",
      "44.2\n",
      "64.7\n",
      "47.5\n",
      "24.7\n",
      "48.2\n",
      "60.6\n",
      "D2ETR (Lin et al., 2022)\n",
      "PVT2\n",
      "50\n",
      "82\n",
      "35\n",
      "–\n",
      "–\n",
      "43.2\n",
      "62.9\n",
      "46.2\n",
      "22.0\n",
      "48.5\n",
      "62.4\n",
      "Deformable D2ETR (Lin et al., 2022)\n",
      "50\n",
      "93\n",
      "40\n",
      "–\n",
      "–\n",
      "50.0\n",
      "67.9\n",
      "54.1\n",
      "31.7\n",
      "53.4\n",
      "66.7\n",
      "Sparse DETR- 𝜌= 10% (Roh et al., 2022)\n",
      "ResNet50\n",
      "50\n",
      "105\n",
      "41\n",
      "–\n",
      "25.3\n",
      "45.3\n",
      "65.8\n",
      "49.3\n",
      "28.4\n",
      "48.3\n",
      "60.1\n",
      "Sparse DETR- 𝜌= 10% (Roh et al., 2022)\n",
      "Swin-T\n",
      "50\n",
      "113\n",
      "41\n",
      "–\n",
      "21.2\n",
      "48.2\n",
      "69.2\n",
      "52.3\n",
      "29.8\n",
      "51.2\n",
      "64.5\n",
      "DAB-DETR (Liu et al., 2022a)\n",
      "ResNet50\n",
      "50\n",
      "202\n",
      "44\n",
      "–\n",
      "–\n",
      "44.5\n",
      "65.1\n",
      "47.7\n",
      "25.3\n",
      "48.2\n",
      "62.3\n",
      "DAB-DETR* (Liu et al., 2022a)\n",
      "50\n",
      "216\n",
      "44\n",
      "–\n",
      "–\n",
      "45.7\n",
      "66.2\n",
      "49.0\n",
      "26.1\n",
      "49.4\n",
      "63.1\n",
      "DN-DETR (Li et al., 2022)\n",
      "ResNet50\n",
      "50\n",
      "94\n",
      "44\n",
      "–\n",
      "–\n",
      "44.1\n",
      "64.4\n",
      "46.7\n",
      "22.9\n",
      "48.0\n",
      "63.4\n",
      "DN-DETR-DC5 (Li et al., 2022)\n",
      "50\n",
      "202\n",
      "44\n",
      "–\n",
      "–\n",
      "46.3\n",
      "66.4\n",
      "49.7\n",
      "26.7\n",
      "50.0\n",
      "64.3\n",
      "DN-Deformable-DETR (Li et al., 2022)\n",
      "50\n",
      "195\n",
      "48\n",
      "–\n",
      "–\n",
      "48.6\n",
      "67.4\n",
      "52.7\n",
      "31.0\n",
      "52.0\n",
      "63.7\n",
      "DINO-4scale (Zhang et al., 2022a)\n",
      "ResNet50\n",
      "12\n",
      "279\n",
      "47\n",
      "–\n",
      "24\n",
      "47.9\n",
      "65.3\n",
      "52.1\n",
      "31.2\n",
      "50.9\n",
      "61.9\n",
      "DINO-5scale (Zhang et al., 2022a)\n",
      "12\n",
      "860\n",
      "47\n",
      "–\n",
      "10\n",
      "48.3\n",
      "65.8\n",
      "52.4\n",
      "32.2\n",
      "51.3\n",
      "62.2\n",
      "DINO-4scale (Zhang et al., 2022a)\n",
      "36\n",
      "–\n",
      "–\n",
      "–\n",
      "–\n",
      "50.5\n",
      "68.3\n",
      "55.1\n",
      "32.7\n",
      "53.9\n",
      "64.9\n",
      "DINO-5scale (Zhang et al., 2022a)\n",
      "36\n",
      "–\n",
      "–\n",
      "–\n",
      "–\n",
      "51.0\n",
      "69.0\n",
      "55.6\n",
      "34.1\n",
      "53.6\n",
      "65.6\n",
      "SAM-DETR (Zhang et al., 2022b)\n",
      "ResNet50\n",
      "50\n",
      "100\n",
      "58\n",
      "–\n",
      "–\n",
      "39.8\n",
      "61.8\n",
      "41.6\n",
      "20.5\n",
      "43.4\n",
      "59.6\n",
      "SAM-DETR-DC5 (Zhang et al., 2022b)\n",
      "50\n",
      "210\n",
      "58\n",
      "–\n",
      "–\n",
      "43.3\n",
      "64.4\n",
      "46.2\n",
      "25.1\n",
      "46.9\n",
      "61.0\n",
      "Pix2Seq (Chen et al., 2021)\n",
      "ResNet50\n",
      "50\n",
      "–\n",
      "37\n",
      "–\n",
      "–\n",
      "43.0\n",
      "61.0\n",
      "45.6\n",
      "25.1\n",
      "46.9\n",
      "59.4\n",
      "Pix2Seq-DC5 (Chen et al., 2021)\n",
      "50\n",
      "–\n",
      "38\n",
      "–\n",
      "–\n",
      "43.2\n",
      "61.0\n",
      "46.1\n",
      "26.6\n",
      "47.0\n",
      "58.6\n",
      "Additionally, we have presented the attention visualization of the\n",
      "encoder and decoder (as shown in Figs. 5 and 6). This visualization\n",
      "aids in understanding how the model focuses on various parts of the\n",
      "input image and utilizes attention mechanisms for object detection.\n",
      "The encoder processes the input image, captures its spatial information,\n",
      "and creates a set of contextualized feature representations. Attention\n",
      "visualization in the encoder demonstrates how the model concentrates\n",
      "on specific regions of the image, emphasizing crucial areas that con-\n",
      "tribute to the comprehension of the objects present. The decoder uses\n",
      "the encoded features to generate final object detections, employing a\n",
      "series of self-attention and cross-attention mechanisms to iteratively\n",
      "refine the predicted object bounding boxes and class labels.\n",
      "In summary, DETR, the first Transformer-based end-to-end object\n",
      "detector, exhibited performance comparable to state-of-the-art (SOTA)\n",
      "methods at the time. However, there are evident drawbacks in its\n",
      "application: slow convergence and low accuracy on small objects.\n",
      "Nonetheless, its end-to-end architecture possesses significant potential\n",
      "and has attracted numerous researchers to explore improvements.\n",
      "3.2.2. UP- DETR\n",
      "Since DERT faces great challenges in training and optimization, it\n",
      "requires a huge amount of training data and an extremely long training\n",
      "schedule, which leads to limitations in application on small datasets.\n",
      "Moreover, the existing pretext task cannot be directly applied to train\n",
      "the Transformer module of DETR, because DETR focuses mainly on\n",
      "spatial localization rather than image instance-based or cluster-based\n",
      "segmentation learning. To address the above issues, Dai et al. (2021a)\n",
      "proposed UP-DETR, a DETR-like model capable of unsupervised pre-\n",
      "training, whose structure is shown in Fig. 7.\n",
      "Multiple query patches are randomly cropped from a given image\n",
      "and the Transformer for detection is pre-trained to predict the bounding\n",
      "boxes of these query patches in the given image. In the pre-training\n",
      "Engineering Applications of Artificial Intelligence 126 (2023) 107021\n",
      "7\n",
      "Y. Li et al.\n",
      "Fig. 5. Encoder self-attention for a set of reference points. It demonstrates the attention distribution after the input image is processed through the Transformer encoder.\n",
      "Fig. 6. Visualization of decoder attention for each predicted object in images from the COCO validation set, using the DETR-DC5 model. Attention scores are represented by\n",
      "distinct colors for different objects. The decoder primarily focuses on object extremities, such as legs and heads, highlighting the model’s ability to capture fine-grained details. It\n",
      "is recommended to view this figure in color for better understanding.\n",
      "Fig. 7. UP-DETR pre-training architecture by random query patch detection: (a) For one single-query patch, which is added to all object queries. (b) For the multi-query patch,\n",
      "which is added each query patch to 𝑁∕𝑀object queries with shuffle and attention mask.\n",
      "Source: Image form Dai et al. (2021a).\n",
      "process, the method addresses the following two key problems. (1) To\n",
      "trade-off the preference of classification and localization in the pre-text\n",
      "task, the backbone network is frozen and a patch feature reconstruction\n",
      "branch is proposed that is jointly optimized with patch detection. (2)\n",
      "For multi-query patch, UP-DETR is introduced in single-query patch\n",
      "and extended to multi-query patches with object query shuffle and\n",
      "attention mask.\n",
      "In summary, UP-DETR proposes a new unsupervised pre-text task-\n",
      "random query patch detection to pre-train the Transformer. The results\n",
      "show that UP-DETR has significantly better performance than DETR in\n",
      "Engineering Applications of Artificial Intelligence 126 (2023) 107021\n",
      "8\n",
      "Y. Li et al.\n",
      "object detection, panorama segmentation, and single detection, even on\n",
      "the PASCAL VOC dataset where the training data is insufficient.\n",
      "3.2.3. YOLOS\n",
      "Inspired by the pre-trained Transformer can fine-tune at the token\n",
      "level tasks (Rajpurkar et al., 2016; Sang and De Meulder, 2003), Fang\n",
      "et al. (2021) proposed YOLOS, a pure sequence-to-sequence trans-\n",
      "former on the basis of DETR (Carion et al., 2020) and ViT (Dosovitskiy\n",
      "et al., 2021). It replaces the class token of the original ViT with the\n",
      "detection token and replaces the image classification loss with the\n",
      "bipartite matching loss of DETR in the training phase, which allows\n",
      "object detection by set prediction. YOLOS demonstrates the generality\n",
      "and transferability of the pre-trained Transformer from image classi-\n",
      "fication to downstream object detection task, which is pre-trained in\n",
      "the classification task and then transfer to the detection task for fine-\n",
      "tuning. Experiments demonstrate that YOLOS-Base, pre-trained on only\n",
      "medium-sized ImageNet datasets can achieve 42.0 box AP.\n",
      "3.2.4. Deformable DETR\n",
      "Inspired by Deformable Convolution ((Dai et al., 2017), Zhu et al.\n",
      "(2021) proposed Deformable DETR. This method combines the ad-\n",
      "vantages of sparse spatial sampling of deformable convolution with\n",
      "the relational modeling capability of Transformer. The Deformable\n",
      "Attention Module (DEM) is introduced to accelerate convergence and\n",
      "fuse multi-scale features to improve accuracy. Moreover, The authors\n",
      "introduce multi-scale feature from FPN (Lin et al., 2016), and then\n",
      "propose Multi-Scale Deformable Attention (MSDA) to replace the Trans-\n",
      "former Attention Module for processing feature maps, as shown in\n",
      "Fig. 8 is shown. Let {𝒙𝑙}𝐿\n",
      "𝑙=1 be the input multi-scale feature map, where\n",
      "𝑥𝑙∈R𝐶×𝐻𝑙×𝑊𝑙. Let ̂𝒑𝑞∈[0, 1]2 be the normalized coordinates of the\n",
      "reference point of each query element 𝑞, and then compute Multi-Scale\n",
      "Deformable Attention as\n",
      "DeformAttn (𝒛𝑞, 𝒑𝑞, 𝒙) =\n",
      "𝑀\n",
      "∑\n",
      "𝑚=1\n",
      "𝑾𝑚\n",
      "[ 𝐾\n",
      "∑\n",
      "𝑘=1\n",
      "𝐴𝑚𝑞𝑘⋅𝑾′\n",
      "𝑚𝒙(𝒑𝑞+ 𝛥𝒑𝑚𝑞𝑘\n",
      ")\n",
      "]\n",
      ",\n",
      "(14)\n",
      "where 𝑚is the index of attention head, 𝑙is the index of input feature\n",
      "level, and k is the index of sampling points. 𝛥𝒑𝑚𝑙𝑞𝑘and 𝐴𝑚𝑙𝑞𝑘denote\n",
      "respectively sampling offset and attention weight of the 𝑘th sampling\n",
      "point in the 𝑙th feature layer and the 𝑚th attention head. The scalar\n",
      "attention weights 𝐴𝑚𝑙𝑞𝑘are normalized to ∑𝐿\n",
      "𝑙=1\n",
      "∑𝐾\n",
      "𝑘=1 𝐴𝑚𝑙𝑞𝑘= 1. The\n",
      "normalized coordinates (0, 0) and (1, 1) of ̂𝒑𝑞∈[0, 1]2 denote the upper\n",
      "left and lower right corners of the image, respectively. The function\n",
      "𝜙𝑙\n",
      "(̂𝒑𝑞\n",
      ") in Eq. (14) rescales the normalized coordinates 𝑷𝑞to the input\n",
      "feature map of the 𝑙th layer. The computational complexity of MSDA\n",
      "is 𝑂(2𝑁𝑞𝐶2 + min (𝐻𝑊𝐶2, 𝑁𝑞𝐾𝐶2)) compared to the original DETR,\n",
      "Deformable DETR requires less than one-tenth of training epochs to\n",
      "achieve better performance (especially on small object).\n",
      "3.2.5. Conditional DETR\n",
      "Meng et al. (2021) proposed Conditional DETR. They visualized ex-\n",
      "periments on the operation of DETR and concluded that cross-attention\n",
      "in DETR is highly dependent on content embedding to locate the\n",
      "four vertices and predict the bounding box. Thus, it increases the\n",
      "training difficulty. So they improved the cross-attention of DETR by\n",
      "concatenating the content query 𝑐𝑞and spatial query 𝑝𝑞, and the key\n",
      "by splicing the content key 𝑐𝑘and spatial key 𝑝𝑘. This inner product of\n",
      "query and key gives the following result:\n",
      "𝐜⊤\n",
      "𝑞𝐜𝑘+ 𝐩⊤\n",
      "𝑞𝐩𝑘,\n",
      "(15)\n",
      "This separates the functions of content query and spatial query\n",
      "so that they focus on the weight of content and space respectively.\n",
      "As shown in Fig. 9, the improved Decoder layer consists of three\n",
      "main modules: 1) The self-attention layer, which is from the previous\n",
      "Decoder layer and is used to remove duplicate predictions as well as\n",
      "predict categories and bounding boxes; (2) The cross-attention layer,\n",
      "which can use embedding output of encoder to complete the embedding\n",
      "of the decoder; (3) The feed-forward networks layer (FFN).\n",
      "The core of the conditional cross-attention mechanism is to learn\n",
      "a conditional spatial query from decoder embedding and reference\n",
      "points, which can explicitly find the boundary regions of the object,\n",
      "thus narrowing down the search object, helping to locate the object,\n",
      "and alleviating the problem of over-reliance on the quality of content\n",
      "embedding in DETR training. The problem of over-reliance on the\n",
      "quality of content embedding in DETR training is alleviated. These\n",
      "refinements improve the convergence speed of DETR by 8× faster and\n",
      "the box mAP on the COCO dataset by 1.8%.\n",
      "3.2.6. Efficient DETR\n",
      "Yao et al. analyzed the mechanisms of DETR and Deformable DETR\n",
      "and found that their common feature is a cascade structure stacked\n",
      "with six Decoders, which is used to iteratively update the object query.\n",
      "The reference point proposed by Deformable DETR visualizes the object\n",
      "query and solves the difficult problem that the object query is difficult\n",
      "to analyze directly. However, different initialization methods of refer-\n",
      "ence points have a great impact on decoder performance. In order to\n",
      "investigate a more efficient way to initialize the object container, Yao\n",
      "et al. proposed Efficient DETR, a two-stage object detector that consists\n",
      "of dense prediction and sparse set prediction, and these two parts share\n",
      "the same detection head.\n",
      "The model generates region proposals using dense detection before\n",
      "initializing the object container, and then uses the highest-scoring 4-\n",
      "dimensional proposal and its 256-dimensional encoder features as the\n",
      "initialization value of the object container, which results in better\n",
      "performance and fast convergence. The experimental results show that\n",
      "Efficient DETR combines the features of dense detection and ensemble\n",
      "detection, and can converge quickly while achieving high performance.\n",
      "The model achieves the SOTA performance at that time on the COCO\n",
      "dataset with only one encoder layer and three decoder layers, while the\n",
      "epoch is reduced by 14× less.\n",
      "3.2.7. SMCA\n",
      "To strengthen the relationship between the visual region of common\n",
      "interest for each object query and the bounding box to be predicted by\n",
      "the query, Gao et al. (2021) introduced spatial prior and multi-scale fea-\n",
      "tures, and proposed Spatially Modulated Co Attention (SMCA), which\n",
      "replaces the cross attention in the original Decoder while keeping the\n",
      "others unchanged.\n",
      "The decoder of SMCA has multiple cross-attention heads, each of\n",
      "which estimates the object center and scale from a slightly different\n",
      "location, resulting in a series of different spatial weight maps. This\n",
      "weight map is used to spatially adjust the co-attention features, which\n",
      "improves the detection performance. Based on these improvements,\n",
      "SMCA can achieve 43.7 mAP in 50 epochs and 45.6 mAP in 108 epochs\n",
      "on the COCO dataset.\n",
      "3.2.8. ACT\n",
      "Due to the slow convergence of DETR, Zheng et al. (2021) proposed\n",
      "the Adaptive Clustering Transformer (ACT) to address the problem of\n",
      "high trial and error costs for improving DETR. ACT is a plug-and-\n",
      "play module that is fully compatible with Transformer and can be\n",
      "ported to DETR without any training. Its core design is first, to perform\n",
      "feature clustering adaptively for the attention redundancy (points with\n",
      "similar semantics and similar spatial locations produce similar atten-\n",
      "tion maps) of encoder, select representative prototypes, and broadcast\n",
      "feature updates to their nearest neighboring points based on Euclidean\n",
      "distance. Second, an adaptive clustering algorithm is designed for the\n",
      "encoder note feature diversity problem (for different inputs, the feature\n",
      "distribution of each encoder layer is quite different), and a multi-\n",
      "round exact Euclidean location-sensitive hash (E2LSH) is chosen for this\n",
      "algorithm to adaptively determine the number of prototypes. Thanks\n",
      "Engineering Applications of Artificial Intelligence 126 (2023) 107021\n",
      "9\n",
      "Y. Li et al.\n",
      "Fig. 8. The architecture of Deformable DETR. Its attention module focuses on only a small number of key sampling points around the reference point, and assigns a fixed and\n",
      "small number of keys to each object query, thus alleviating the problems of slow convergence and low feature resolution.\n",
      "Source: Image from Zhu et al. (2021).\n",
      "Fig. 9. A Decoder layer of Conditional DETR. The gray shaded box indicates that the\n",
      "Conditional spatial query is predicted from the learnable 2D coordinates 𝑠and the\n",
      "embedding output of the previous Decoder layer.\n",
      "Source: Image from Meng et al. (2021).\n",
      "to these improvements, ACT can reduce the FLOPS of DETR from\n",
      "73.4 Gflops to 58.2 Gflops (excluding Backbone Resnet FLOPs) without\n",
      "additional training, while the loss of AP is only 0.7%. The AP loss\n",
      "can be further reduced to 0.2% by multitasking knowledge distillation.\n",
      "Given its excellent performance, exploring ACT training from scratch\n",
      "and fusion with multi-scale features is a worthy research direction in\n",
      "the future.\n",
      "3.2.9. TSP\n",
      "Sun et al. (2021) concluded after a lot of analysis that the cross-\n",
      "attention part of decoder and Hungarian loss of DETR are the main\n",
      "reasons for the slow convergence of DETR. So they proposed two\n",
      "improved models of DETR with only encoder, TSP-FCOS and TSP-\n",
      "RCNN corresponding to the One-Stage and Two-Stage object detection\n",
      "methods, respectively. Both models can be viewed as feature pyra-\n",
      "mid (Lin et al., 2016) based. The model uses a feature of interest (FoI)\n",
      "selection mechanism that helps encoder process multi-scale features. In\n",
      "addition, the model applies matching distillation to solve the instability\n",
      "of bipartite graph matching. Experiments show that TSP achieves better\n",
      "results with reduced training cost, using only 36-epoch to achieve the\n",
      "500-epoch results of the original DETR training.\n",
      "3.2.10. DINO\n",
      "The Hungarian algorithm has been used in DETR (Carion et al.,\n",
      "2020) to match the output of the object by Decoder with Ground Truth.\n",
      "However, the discreteness of the Hungarian algorithm matching and\n",
      "the randomness of the model training cause the matching process to be\n",
      "dynamic and unstable, resulting the final slow convergence of DETR.\n",
      "By deeply studying the iteration mechanism and optimization prob-\n",
      "lems of the DETR model, Zhang et al. (2022a)proposed DINO (DETR\n",
      "with Improved deNoising anchor boxes) based on DN-DETR (Li et al.,\n",
      "2022), DAB-DETR (Liu et al., 2022a) and Deformable DETR (Zhu et al.,\n",
      "2021). The key design of DINO is that the training phase uses denoising\n",
      "training as a shortcut to learning the relative offset of anchor by first\n",
      "adding noise near the Ground Truth box, and then the Hungarian\n",
      "matching directly reconstructs the truth bounding box, thus improving\n",
      "the stability of matching. Secondly, the model also uses a query-based\n",
      "Engineering Applications of Artificial Intelligence 126 (2023) 107021\n",
      "10\n",
      "Y. Li et al.\n",
      "Fig. 10. The architecture of Pyramid Vision Transformer, the whole model is divided into 4 stages to generate feature maps at different scales. Each stage consists of a patch\n",
      "embedding layer, 𝐿𝑖-layer, and reshape operation.\n",
      "Source: Image form Wang et al. (2021a).\n",
      "dynamic anchor formulation to initialize the query and correct the\n",
      "parameters of adjacent earlier layers with the gradients of later layers.\n",
      "DINO breaks the dominance of classical architecture detector (SwinV2-\n",
      "G (Liu et al., 2021a), Florence (Yuan et al., 2021), DyHead (Dai\n",
      "et al., 2021b), etc.). DINO-Res50, which combines multi-scale features,\n",
      "achieves 48.3AP and 51.0AP on the COCO2017 dataset with 12-epoch\n",
      "and 36-epoch training schemes, respectively. Moreover, DINO-Swin-L\n",
      "even achieves the highest performance 63.3AP after training on a larger\n",
      "dataset.\n",
      "3.3. Transformer backbone\n",
      "Other efforts such as ViT (Dosovitskiy et al., 2021) have used Trans-\n",
      "former in the image classification and achieved comparable results.\n",
      "However, there are some limitations in other complex CV tasks. These\n",
      "challenges of transferring the high performance of Transformer in NLP\n",
      "to the CV can be explained by the differences between the two domains.\n",
      "1. The object entities in CV tasks often have dramatic scale varia-\n",
      "tion.\n",
      "2. Compared to text, the matrix nature of images makes it con-\n",
      "tain at least hundreds of pixels for an image that can express\n",
      "information. Especially the very long sequence unfolded by high-\n",
      "resolution images is difficult for Transformer to model.\n",
      "3. Many CV tasks such as semantic segmentation require pixel-level\n",
      "dense prediction, and the computational complexity of the self-\n",
      "attention mechanism in ViT increases quadratically with image\n",
      "size, which leads to unacceptable computational overhead.\n",
      "4. In the existing Transformer-based models, tokens are fixed in\n",
      "scale and not improved in design for CV tasks.\n",
      "To address the above challenges, many Transformer-based back-\n",
      "bones have been proposed for CV tasks and combined with methods\n",
      "such as multi-scale to compensate for the shortcomings that ViT can\n",
      "only detect at low resolution and so on. These methods can replace the\n",
      "backbone of mainstream object detection models, and in the benchmark\n",
      "Table 5, we list the performance of Mask R-CNN (He et al., 2017)\n",
      "and RetinaNet (Ross and Dollár, 2017) comparison after replacing the\n",
      "backbone and review the classical models in this subsection.\n",
      "3.3.1. PVT&PVTv2\n",
      "The feature maps output by ViT (Dosovitskiy et al., 2021) are\n",
      "difficult to apply to dense prediction due to their single scale and low\n",
      "resolution. Wang et al. (2021a) proposed the Pyramid Vision Trans-\n",
      "former (PVT) by incorporating the multi-scale feature into Transformer.\n",
      "PVT can be used as a Backbone for various dense detection tasks,\n",
      "especially it can replace the CNN backbone of DETR-like models or be\n",
      "combined into a pure Transformer model without manual components\n",
      "such as NMS.\n",
      "Benefiting from the progressive shrinking pyramid structure in the\n",
      "PVT, the Transformer sequence length decreases as the network gets\n",
      "deeper. Meanwhile, in order to further reduce the computation of\n",
      "fine-grained segmentation of images, they propose spatial-reduction\n",
      "attention (SRA) to reduce the computation of learning high-resolution\n",
      "feature maps (As shown in Fig. 10).\n",
      "Compared with the CNN method based on feature pyramid struc-\n",
      "ture, PVT not only generates multi-scale feature maps to detect ob-\n",
      "jects of different sizes but also fuses global information through self-\n",
      "attention mechanism. The PVTv2 (Wang et al., 2022) proposed by\n",
      "the same team subsequently improves the PVT by adding a linear\n",
      "complexity attention layer, overlapping patch embedding, and convo-\n",
      "lutional feed-forward network to improve the performance of the PVT\n",
      "as backbone. On the COCO dataset, both achieved competitive results\n",
      "at that time.\n",
      "3.3.2. Swin transformer\n",
      "Liu et al. (2021b) proposed Swin Transformer, which creatively uses\n",
      "a hierarchical design to make the Transformer available as a backbone\n",
      "for most CV tasks, rather than just a detection head. As shown in\n",
      "Fig. 11, It is easy to see that, unlike other Transformer models, Swin\n",
      "Transformer builds a feature map with hierarchical representation,\n",
      "similar to the feature pyramid structure in CNN. As the network level\n",
      "deepens, the receptive expands, enabling the extraction of multi-scale\n",
      "features of the image. Secondly, Swin Transformer divides the feature\n",
      "map with multiple windows, and each non-overlapping window per-\n",
      "forms local multi-head attention calculation without correspondence\n",
      "between windows, which makes the computation greatly reduced and\n",
      "linear with the image size, as shown in the Eq. (16). In contrast, ViT\n",
      "produces a single low-resolution image and calculates global attention,\n",
      "Engineering Applications of Artificial Intelligence 126 (2023) 107021\n",
      "11\n",
      "Y. Li et al.\n",
      "Fig. 11. Compare Swin Transformer (left) with ViT (right).\n",
      "Source: Image form Liu et al. (2021b).\n",
      "so the computational complexity and image size are quadratically\n",
      "related, as shown in Eq. (17)\n",
      "𝛺(W −MSA) = 4ℎ𝑤𝐶2 + 2𝑀2ℎ𝑤𝐶,\n",
      "(16)\n",
      "𝛺(MSA) = 4ℎ𝑤𝐶2 + 2(ℎ𝑤)2𝐶,\n",
      "(17)\n",
      "where M is a fixed window size (set to 7 by default), computing global\n",
      "attention for ViT is unacceptable for large image sizes 𝐻𝑊, while\n",
      "window-based multi-head self-attention (W-MSA) is scalable.\n",
      "The Pipeline of the Swin Transformer is shown in Fig. 12(a). The\n",
      "input image is spreading into a sequence after Patch Partition and\n",
      "Linear Embedding layers, and then input into 4 stages. The Swin\n",
      "Transformer block in each stage replaces the standard multi-head self-\n",
      "attention (MSA) module in the Transformer module with window-based\n",
      "self-attention (W-MSA) or a shift window-based module (SW-MSA),\n",
      "which introduces a relative position bias in the computation of at-\n",
      "tention to account for the geometric relationships in the self-attention\n",
      "computation, as shown in Eq. (18). This parameter accounts for the\n",
      "relative spatial configuration of the visual elements and is shown to be\n",
      "critical in various visual tasks, especially for intensive recognition tasks\n",
      "such as object detection and semantic segmentation.\n",
      "Attention(𝑄, 𝐾, 𝑉) = SoftMax\n",
      "(\n",
      "𝑄𝐾𝑇∕\n",
      "√\n",
      "𝑑+ 𝐵\n",
      ")\n",
      "𝑉,\n",
      "(18)\n",
      "Although W-MSA reduces the computation greatly, W-MSA loses the\n",
      "ability to model the relationship between different windows, and the\n",
      "lack of information exchange between non-overlapping windows affects\n",
      "the representation of the model. So they introduced SW-MSA, which\n",
      "shifts the windows in the Swin Transformer Block of the next layer to\n",
      "introduce correspondence of the previous layer. This operation greatly\n",
      "increases the actual receptive field, as shown in Fig. 13. In this way,\n",
      "the multi-head attention is computed inside the new window to include\n",
      "the boundary of the original window and achieve the modeling of the\n",
      "relationship between windows.\n",
      "But this approach causes the number of windows to change, and\n",
      "the window size is not uniform. An easy way to solve this problem\n",
      "is padding the small window, but it will increase the computation. So\n",
      "they proposed cyclic-shifting, a more efficient method of batch com-\n",
      "putation. This method cyclically shifts and merges small windows, so\n",
      "that a window may contain content from different windows, and there-\n",
      "fore the masked MSA mechanism is used to restrict the self-attention\n",
      "computation to each sub-window, as shown in Fig. 14.\n",
      "Swin Transformer has achieved SOTA performance on classification,\n",
      "detection, and segmentation tasks. Its biggest contribution is to propose\n",
      "a backbone that can be widely used in CV. And most of the hyper-\n",
      "parameters commonly found in CNNs can be manually tuned in Swin\n",
      "Transformer, such as the number of network blocks and the size of input\n",
      "images. This method combines the advantages of both Transformer and\n",
      "CNN, fully considers the size invariance of CNN and the relationship\n",
      "between receptive field and number of layers, and solves the problem\n",
      "of slow application of Transformer in CV.\n",
      "3.3.3. Swin TransformerV2\n",
      "After Swin Transformer, Liu et al. (2021a) proposed Swin Trans-\n",
      "formerV2 to address the problems of expansion of CV models and\n",
      "training with high-resolution images, as well as the excessive GPU\n",
      "memory consumption for large models. Swin Transformer is optimized\n",
      "to scale up to 3 billion parameters and can be trained with images up\n",
      "to 1536 × 1536 resolutions. The improved method is shown in Fig. 15.\n",
      "Post normalization technique: They found that when scaling up\n",
      "the model, the activation values in the deep layer increase dramatically.\n",
      "In fact, in the pre-normalized configuration (Layer Norm layer before\n",
      "the Attention layer), the output activation values of each residual block\n",
      "are directly merged back to the main branch, and the amplitude of the\n",
      "main branch becomes in the deeper layers. The huge amplitude differ-\n",
      "ences between different layers may cause training instability problems.\n",
      "Therefore, they propose a post normalization technique, in which the\n",
      "output of each residual block is normalized before it is merged back\n",
      "into the main branch, and the amplitude of the main branch does not\n",
      "accumulate as the number of layers deepens.\n",
      "Scaled cosine attention: In the original self-attention computation,\n",
      "the similarity terms of pixel pairs are computed as dot products of\n",
      "queries and keys vectors. However, when using this approach for\n",
      "large visual models, the learned attention graph for some blocks and\n",
      "attention heads is often dominated by several pixel pairs, especially\n",
      "in post-normalization configurations. To alleviate this problem, the\n",
      "authors propose a scaled cosine attention (Scaled cosine attention)\n",
      "method, which computes the number of attention pairs for a pixel pair\n",
      "𝑖and 𝑗by a scaled cosine function:\n",
      "Sim (𝐪𝑖, 𝐤𝑗\n",
      ") = cos (𝐪𝑖, 𝐤𝑗\n",
      ") ∕𝜏+ 𝐵𝑖𝑗,\n",
      "(19)\n",
      "where 𝐵𝑖𝑗is the relative position bias between pixels 𝑖and 𝑗; 𝜏is a\n",
      "learnable scalar that cannot be shared across heads and layers. The 𝜏is\n",
      "set to be greater than 0.01. The cosine function is naturally normalized\n",
      "so that it can have milder attention values, which improves the stability\n",
      "of large visual models and makes the model capacity easier to be scaled\n",
      "up.\n",
      "Log-spaced continuous position bias:They found that the original\n",
      "relative position encoding method was weak for scale generalization of\n",
      "the model, and proposed log-spaced continuous position bias so that\n",
      "the relative position bias can be transferred smoothly across windows at\n",
      "different resolutions, effectively transferring models pre-trained in low-\n",
      "resolution images and windows to their higher resolution counterparts:\n",
      "̂\n",
      "𝛥𝑥= sign(𝑥) ⋅log(1 + |𝛥𝑥|),\n",
      "̂\n",
      "𝛥𝑦= sign(𝑦) ⋅log(1 + |𝛥𝑦|),\n",
      "(20)\n",
      "where 𝛿𝑥, 𝛿𝑦and ̂\n",
      "𝛥𝑥, ̂\n",
      "𝛥𝑦are the coordinates of linear scale and\n",
      "logarithmic space, respectively. The optimized resulting architecture\n",
      "was named Swin TransformerV2, and the model achieved a box/mask\n",
      "mAP of 63.1/54 in the COCO2017 dataset.\n",
      "3.3.4. Other representative methods\n",
      "In addition to the conventional approaches, our benchmark extends\n",
      "to include comparisons with several cutting-edge techniques. ViL, in-\n",
      "troduced by Zhang et al. (2021), realizes a multi-scale configuration\n",
      "through the sequential stacking of numerous ViT stages. Furthermore,\n",
      "it enhances the attention mechanism, thus elevating both efficiency and\n",
      "classification performance. The Focal Transformer introduces the novel\n",
      "Focal Self-Attention mechanism. This integrates both granular local and\n",
      "coarse global interactions, thereby ensuring the effective capturing of\n",
      "both proximal and distal visual dependencies. Twins (Chu et al., 2021)\n",
      "propose two highly efficient vision transformer architectures, Twins-\n",
      "PCPVT and Twins-SVT, both leveraging a restructured spatial attention\n",
      "mechanism. This methodology incorporates both locally-grouped self-\n",
      "attention and global sub-sampled attention, capturing both fine-grained\n",
      "proximal and coarse-grained distal visual information. Dong et al.\n",
      "(2022) introduced the CSWin Transformer, a robust Transformer-based\n",
      "Engineering Applications of Artificial Intelligence 126 (2023) 107021\n",
      "12\n",
      "Y. Li et al.\n",
      "Fig. 12. (a) Swin Transformer (Swin-T) (b) Swin Transformer Block.\n",
      "Source: Image from Liu et al. (2021b).\n",
      "Fig. 13. The shift window approach can calculates the self-attention across the window\n",
      "boundary of the previous layer.\n",
      "Source: Image from Liu et al. (2021b).\n",
      "Fig. 14. An illustration of circular shift.\n",
      "Source: Image from Liu et al. (2021b).\n",
      "Fig. 15. Comparison of the attention modules of Swin TransformerV1 and V2.\n",
      "Source: Image from Liu et al. (2021a).\n",
      "backbone for vision tasks. It integrates the Cross-Shaped Window self-\n",
      "attention mechanism, varies stripe widths based on network depth,\n",
      "and introduces a novel Locally-enhanced Positional Encoding (LePE)\n",
      "scheme to handle local positional information optimally, resulting in\n",
      "competitive performance across standard vision tasks.\n",
      "3.4. Analysis and discussion for detectors\n",
      "This section provides a succinct review of conventional Transformer-\n",
      "based object detectors, offering a detailed performance comparison in\n",
      "Tables 4 and 5. Each method was evaluated using the NVIDIA A100\n",
      "GPU and adhered to the DETR training protocol. The AdamW opti-\n",
      "mizer (Loshchilov and Hutter, 2017) was uniformly employed across all\n",
      "methods, with the initial learning rate for the transformer set to 10−4,\n",
      "the backbone’s to 10−5, and weight decay at 10−4. The transformer\n",
      "weights were initialized with Xavier init (Glorot and Bengio, 2010),\n",
      "while the backbone leveraged the ImageNet-pretrained ResNet model\n",
      "from torchvision, with frozen batch normalization layers.\n",
      "For Transformer Neck-based models, they treat object detection as\n",
      "a straightforward set prediction, removing manual components (such\n",
      "as anchor set and NMS) that cannot be optimized, thus enabling end-\n",
      "to-end detection. Starting from the original DETR with slow conver-\n",
      "gence and poor detection of small objects, subsequent researchers have\n",
      "proposed optimization strategies from different perspectives.\n",
      "1. To address the problem of slow convergence, researchers often\n",
      "start by improving the attention mechanism. Deformable DETR (Zhu\n",
      "et al., 2021) accelerates convergence 12× faster with the Deformable\n",
      "Attention Module. Conditional DETR improves the cross-attention of\n",
      "DETR and gets 8× faster convergence. Meanwhile, the box mAP on\n",
      "the COCO dataset is improved by 1.8%. Unlike the above methods,\n",
      "ACT (Zheng et al., 2021) proposes a plug-and-play module for adaptive\n",
      "clustering, which reduces the GFLOPS of DETR by 15.2 without addi-\n",
      "tional training, while the AP loss is only 0.7%. Sparse DETR achieves\n",
      "higher performance and the same detection speed (FPS) as Faster\n",
      "R-CNN by improving and reducing the GFLOPs by 75.\n",
      "2. For the problem of poor detection of small objects, multi-scale\n",
      "feature is currently the main focus. Methods such as SMCA (Gao et al.,\n",
      "2021) (as shown in Table 4) introduce multi-scale feature with different\n",
      "operations and significantly improve the accuracy of the detector.\n",
      "Moreover, DINO (Zhang et al., 2022a) reaches 63.3 AP over all classical\n",
      "object detection methods.\n",
      "Presently, most Transformer Backbones are primarily active in im-\n",
      "age classification, with only a few researchers transitioning them to tra-\n",
      "ditional object detectors for dense prediction. These have then achieved\n",
      "state-of-the-art (SOTA) performance. Compared to CNN-based Back-\n",
      "bones, Transformer-based Backbones can integrate global contextual\n",
      "information while outputting multi-scale feature maps, thereby enhanc-\n",
      "ing feature extraction. Although Transformers have challenged CNN’s\n",
      "dominance in object detection, recent advancements such as FAIR’s\n",
      "redesign of ConvNet (Liu et al., 2022b), which draws from the strengths\n",
      "of the Transformer structure, underscore the continued potential of\n",
      "CNNs. In the future, CNN and visual Transformer are expected to\n",
      "continue improving by leveraging each other’s strengths.\n",
      "4. Discussion\n",
      "Although the Transformer model has made great progress (as shown\n",
      "in Table 6) and has shown excellent performance (Table 4, Table 5),\n",
      "they still face some challenges, as well as limitations in practical\n",
      "Engineering Applications of Artificial Intelligence 126 (2023) 107021\n",
      "13\n",
      "Y. Li et al.\n",
      "Table 5\n",
      "The prediction results of RetinaNet and Mask R-CNN with Transformer as Backbone on COCO 2017 Val Set. Where 3 × schedule denotes 36-epoch, MS denotes multi-scale input\n",
      "(MS), and the numbers before and after ‘‘/’’ denote the parameters of RetinaNet and Mask R-CNN, respectively.\n",
      "Backbone\n",
      "#Params\n",
      "FlOPs\n",
      "RetinaNet 3×schedule + MS\n",
      "Mask R-CNN 3×schedule + MS\n",
      "(M)\n",
      "(G)\n",
      "𝐀𝐏𝑏\n",
      "𝐀𝐏𝑏\n",
      "50\n",
      "𝐀𝐏𝑏\n",
      "75\n",
      "𝐀𝐏𝑆\n",
      "𝐀𝐏𝑀\n",
      "𝐀𝐏𝐿\n",
      "𝐀𝐏𝑏\n",
      "𝐀𝐏𝑏\n",
      "50\n",
      "𝐀𝐏𝑏\n",
      "75\n",
      "𝐀𝐏𝑚\n",
      "𝐀𝐏𝑚\n",
      "50\n",
      "𝐀𝐏𝑚\n",
      "75\n",
      "ResNet50 (He et al., 2015)\n",
      "38/44\n",
      "239/260\n",
      "39\n",
      "58.4\n",
      "41.8\n",
      "22.4\n",
      "42.8\n",
      "51.6\n",
      "41\n",
      "61.7\n",
      "44.9\n",
      "37.1\n",
      "58.4\n",
      "40.1\n",
      "PVTv1-S (Wang et al., 2021a)\n",
      "34/44\n",
      "226/245\n",
      "42.2\n",
      "62.7\n",
      "45.0\n",
      "26.2\n",
      "45.2\n",
      "57.2\n",
      "43.0\n",
      "65.3\n",
      "46.9\n",
      "39.9\n",
      "62.5\n",
      "42.8\n",
      "ViL-S (Zhang et al., 2021)\n",
      "36/45\n",
      "252/174\n",
      "42.9\n",
      "63.8\n",
      "45.6\n",
      "27.8\n",
      "46.4\n",
      "56.3\n",
      "43.4\n",
      "64.9\n",
      "47.0\n",
      "39.6\n",
      "62.1\n",
      "42.4\n",
      "Swin-T (Liu et al., 2021b)\n",
      "39/48\n",
      "245/264\n",
      "45.0\n",
      "65.9\n",
      "48.4\n",
      "29.7\n",
      "48.9\n",
      "58.1\n",
      "46.0\n",
      "68.1\n",
      "50.3\n",
      "41.6\n",
      "65.1\n",
      "44.9\n",
      "PVTv2-B2-Li (Liu et al., 2021b)\n",
      "32/42\n",
      "-/-\n",
      "–\n",
      "–\n",
      "–\n",
      "–\n",
      "–\n",
      "–\n",
      "46.8\n",
      "68.7\n",
      "51..4\n",
      "42.3\n",
      "65.7\n",
      "45.4\n",
      "Focal-T (Yang et al., 2021)\n",
      "39/49\n",
      "265/291\n",
      "45.5\n",
      "66.3\n",
      "48.8\n",
      "31.2\n",
      "49.2\n",
      "58.7\n",
      "47.2\n",
      "69.4\n",
      "51.9\n",
      "42.7\n",
      "66.5\n",
      "45.9\n",
      "TwinsP-S (Chu et al., 2021)\n",
      "34/44\n",
      "-/245\n",
      "45.2\n",
      "66.5\n",
      "48.6\n",
      "30.0\n",
      "48.8\n",
      "58.9\n",
      "46.8\n",
      "69.3\n",
      "51.8\n",
      "42.6\n",
      "66.3\n",
      "46.0\n",
      "Twins-S (Chu et al., 2021)\n",
      "34/55\n",
      "-/228\n",
      "45.6\n",
      "67.1\n",
      "48.6\n",
      "29.8\n",
      "49.3\n",
      "60.0\n",
      "46.8\n",
      "69.2\n",
      "51.2\n",
      "42.6\n",
      "66.3\n",
      "45.8\n",
      "CSwin-T (Dong et al., 2022)\n",
      "-/42\n",
      "-/279\n",
      "–\n",
      "–\n",
      "–\n",
      "–\n",
      "–\n",
      "–\n",
      "49.0\n",
      "70.7\n",
      "53.7\n",
      "43.6\n",
      "67.9\n",
      "46.6\n",
      "PVTv2-B2 (Wang et al., 2022)\n",
      "35/45\n",
      "-/-\n",
      "–\n",
      "–\n",
      "–\n",
      "–\n",
      "–\n",
      "–\n",
      "47.8\n",
      "69.7\n",
      "52.6\n",
      "43.1\n",
      "66.8\n",
      "46.7\n",
      "ResNet101 (He et al., 2015)\n",
      "57/63\n",
      "315/336\n",
      "40.9\n",
      "60.1\n",
      "44.0\n",
      "23.7\n",
      "45.0\n",
      "53.8\n",
      "42.8\n",
      "63.2\n",
      "47.1\n",
      "38.5\n",
      "60.1\n",
      "41.3\n",
      "ResNeXt101-32 × 4d (He et al., 2015)\n",
      "56/63\n",
      "319/340\n",
      "41.4\n",
      "61 .0\n",
      "44.3\n",
      "23.9\n",
      "45.5\n",
      "53.7\n",
      "44.0\n",
      "64.4\n",
      "48.0\n",
      "39.2\n",
      "61.4\n",
      "41.9\n",
      "PVTv1-M (Wang et al., 2021a)\n",
      "54/64\n",
      "283/302\n",
      "43.2\n",
      "63.8\n",
      "46.1\n",
      "27.3\n",
      "46.3\n",
      "58.9\n",
      "44.2\n",
      "66.0\n",
      "48.2\n",
      "40.5\n",
      "63.1\n",
      "43.5\n",
      "ViL-M (Zhang et al., 2021)\n",
      "51/60\n",
      "339/261\n",
      "43.7\n",
      "64.6\n",
      "46.4\n",
      "27.9\n",
      "47.1\n",
      "56.9\n",
      "44.6\n",
      "66.3\n",
      "48.5\n",
      "40.7\n",
      "63.8\n",
      "43.7\n",
      "TwinsP-B (Chu et al., 2021)\n",
      "54/64\n",
      "-/302\n",
      "46.4\n",
      "67.7\n",
      "49.8\n",
      "31.3\n",
      "50.2\n",
      "61.4\n",
      "47.9\n",
      "70.1\n",
      "52.5\n",
      "43.2\n",
      "67.2\n",
      "46.3\n",
      "Twins-B (Chu et al., 2021)\n",
      "67/76\n",
      "-/340\n",
      "46.9\n",
      "68.0\n",
      "50.2\n",
      "31.7\n",
      "50.3\n",
      "61.8\n",
      "48.0\n",
      "69.5\n",
      "52.7\n",
      "43.0\n",
      "66.8\n",
      "46.6\n",
      "Swin-Scite (Liu et al., 2021b)\n",
      "60/69\n",
      "335/354\n",
      "46.4\n",
      "67.0\n",
      "50.1\n",
      "31.0\n",
      "50.1\n",
      "60.3\n",
      "48.5\n",
      "70.2\n",
      "53.5\n",
      "43.3\n",
      "67.3\n",
      "46.6\n",
      "Focal-S (Yang et al., 2021)\n",
      "62/71\n",
      "367/401\n",
      "47.3\n",
      "67.8\n",
      "51.0\n",
      "31.6\n",
      "50.9\n",
      "61.1\n",
      "48.8\n",
      "70.5\n",
      "53.6\n",
      "43.8\n",
      "67.7\n",
      "47.2\n",
      "CSwin-S (Dong et al., 2022)\n",
      "-/54\n",
      "-/342\n",
      "–\n",
      "–\n",
      "–\n",
      "–\n",
      "–\n",
      "–\n",
      "50.0\n",
      "71.3\n",
      "54.7\n",
      "44.5\n",
      "68.4\n",
      "47.7\n",
      "ResNeXt101-64 × 4d (He et al., 2015)\n",
      "96/102\n",
      "473/493\n",
      "41.8\n",
      "61.5\n",
      "44.4\n",
      "25.2\n",
      "45.4\n",
      "54.6\n",
      "44.4\n",
      "64.9\n",
      "48.8\n",
      "39.7\n",
      "61.9\n",
      "42.6\n",
      "PVTv1-Large (Wang et al., 2021a)\n",
      "71/81\n",
      "345/364\n",
      "43.4\n",
      "63.6\n",
      "46.1\n",
      "26.1\n",
      "46.0\n",
      "59.5\n",
      "44.5\n",
      "66.0\n",
      "48.3\n",
      "40.7\n",
      "63.4\n",
      "43.7\n",
      "ViL-Base (Zhang et al., 2021)\n",
      "67/76\n",
      "443/365\n",
      "44.7\n",
      "65.5\n",
      "47.6\n",
      "29.9\n",
      "48.0\n",
      "58.1\n",
      "45.7\n",
      "67.2\n",
      "49.9\n",
      "41.3\n",
      "64.4\n",
      "44.5\n",
      "Swin-Base (Liu et al., 2021b)\n",
      "98/107\n",
      "477/496\n",
      "45.8\n",
      "66.4\n",
      "49.1\n",
      "29.9\n",
      "49.4\n",
      "60.3\n",
      "48.5\n",
      "69.8\n",
      "53.2\n",
      "43.4\n",
      "66.8\n",
      "46.9\n",
      "Focal-Base (Yang et al., 2021)\n",
      "101/110\n",
      "514/533\n",
      "46.9\n",
      "67.8\n",
      "50.3\n",
      "31.9\n",
      "50.3\n",
      "61.5\n",
      "49.0\n",
      "70.1\n",
      "53.6\n",
      "43.7\n",
      "67.6\n",
      "47.0\n",
      "CSWin-B (Dong et al., 2022)\n",
      "-/97\n",
      "-/526\n",
      "–\n",
      "–\n",
      "–\n",
      "–\n",
      "–\n",
      "–\n",
      "50.8\n",
      "72.1\n",
      "55.8\n",
      "44.9\n",
      "69.1\n",
      "48.3\n",
      "applications. This section will summarize the innovative improvements\n",
      "of the current method, analyze the problems encountered by the\n",
      "Transformer detector, and give an outlook on the future development\n",
      "prospects.\n",
      "4.1. Challenges\n",
      "High computational overhead. Typical properties of CNNs include\n",
      "inductive bias, which is expressed as translation invariance, weight\n",
      "sharing, and sparse connectivity (Dosovitskiy et al., 2021). These prop-\n",
      "erties grant CNNs a robust local feature extraction capability and enable\n",
      "them to achieve high performance through the simple sliding match-\n",
      "ing of convolutional kernels. As a result, compared to Transformers,\n",
      "CNNs often exhibit competitive performance with lower computational\n",
      "overhead. However, current CNN architectures possess less potential\n",
      "than Transformers due to their weaker extraction of global features and\n",
      "contextual information. The self-attention mechanism in Transformers\n",
      "can also emulate convolutional layers, requiring only a sufficient num-\n",
      "ber of heads to focus on each pixel within the convolutional receptive\n",
      "field and employing relative positional encoding to ensure translation\n",
      "invariance (Cordonnier et al., 2020). This full-attention operation can\n",
      "effectively integrate local and global attention while dynamically gen-\n",
      "erating attention weights based on feature relationships. Nevertheless,\n",
      "Transformers face certain limitations in practical applications. One of\n",
      "the main challenges stems from their high computational complexity.\n",
      "The expensive computational overhead restricts the application\n",
      "of Transformer-based detectors on mobile computing platforms. At\n",
      "present, most mobile detection platforms primarily rely on one-stage\n",
      "detectors (Zhao et al., 2019), while the trend for Transformer de-\n",
      "tectors leans towards offline high-precision detection. Additionally,\n",
      "Transformers require large amounts of data, and common solutions\n",
      "include data augmentation, self-supervised, or semi-supervised learning\n",
      "approaches (He et al., 2021). Compared to state-of-the-art CNN-based\n",
      "approaches, their deployment on mobile platforms is constrained by\n",
      "higher computational complexity.\n",
      "The impact of computational overhead on deploying Transformer-\n",
      "based object detection models in practical scenarios is influenced by\n",
      "factors such as the number of parameters, running time (FPS), and\n",
      "floating-point operations (FLOPs). However, these metrics’ influence\n",
      "varies depending on the application context and hardware environ-\n",
      "ment. For example, in situations like autonomous driving or robotic\n",
      "navigation, FPS is a critical factor, as algorithms must process video\n",
      "streams at high frame rates to respond quickly to external changes.\n",
      "In the case of mobile devices and embedded systems, the number of\n",
      "parameters and FLOPs are more influential due to energy and memory\n",
      "constraints. Consequently, deploying algorithms on mobile platforms\n",
      "necessitates balancing performance, energy consumption, and memory\n",
      "usage. In cloud computing and high-performance hardware settings,\n",
      "computational overhead is not the most critical factor since compu-\n",
      "tational resources are relatively abundant. In these scenarios, model\n",
      "performance and accuracy are paramount.\n",
      "According to the data in Table 4, Table 5, modern Transformer-\n",
      "based models have outperformed classical two-stage object detection\n",
      "algorithms (e.g., Faster R-CNN) in terms of FPS and achieved improved\n",
      "accuracy, rendering them viable for practical applications. To ensure\n",
      "efficient deployment and application in real-world engineering sce-\n",
      "narios, researchers typically optimize object detection algorithms for\n",
      "specific contexts, minimizing computational overhead and improving\n",
      "real-time performance and energy efficiency. This optimization may\n",
      "involve techniques such as model compression, knowledge distillation,\n",
      "and network architecture design.\n",
      "Insufficient understanding of visual Transformer. Compared to\n",
      "the well-established research and applications of CNNs, our current un-\n",
      "derstanding of the underlying mechanisms behind visual Transformers\n",
      "is still limited. The Transformer architecture was originally designed for\n",
      "sequence processing tasks (Vaswani et al., 2017). Although Transform-\n",
      "ers have demonstrated strong performance when applied to computer\n",
      "vision tasks, there is relatively little explanation regarding their specific\n",
      "roles and functions in this context. Consequently, gaining a deeper\n",
      "understanding of the principles behind visual Transformers is crucial\n",
      "to facilitate more fundamental optimization improvements and en-\n",
      "hance the model’s interpretability. This deeper understanding could\n",
      "potentially involve investigating the attention mechanisms, hierarchical\n",
      "feature representation, and the interaction between different layers\n",
      "Engineering Applications of Artificial Intelligence 126 (2023) 107021\n",
      "14\n",
      "Y. Li et al.\n",
      "Table 6\n",
      "Summary of the advantages and limitations of Transformer-based object detection models.\n",
      "Type\n",
      "Method\n",
      "Highlights\n",
      "Limitations\n",
      "Transformer Neck\n",
      "DETR (Carion et al., 2020)\n",
      "(1) Proposed Transformer-based end-to-end object\n",
      "detection framework, (2) Removed hand-designed\n",
      "anchor set and non-maximal suppression (NMS)\n",
      "(1) Requires massive dataset\n",
      "training, (2) Convergence is\n",
      "very slow, (3) Poor\n",
      "performance for small objects.\n",
      "SMCA (Gao et al., 2021)\n",
      "(1) Combining a learnable co-attention map and a\n",
      "manual space prior speeds up the convergence of\n",
      "DETR, (2) Incorporating a scale selection network\n",
      "in decoder.\n",
      "(1) Good performance for\n",
      "large objects and poor\n",
      "performance for small objects,\n",
      "(2) High computational\n",
      "overhead.\n",
      "Deformable DETR (Zhu et al., 2021)\n",
      "(1) Proposed deformable attention mechanism,\n",
      "which pays more attention to local information\n",
      "and improves convergence speed; (2) Combined\n",
      "with multi-scale feature, (3) Proposed reference\n",
      "point visualization object query, (4) Two-stage\n",
      "Deformable DETR is also proposed.\n",
      "(1) Low accuracy for large\n",
      "objects, (2) Deformable\n",
      "attention brings unordered\n",
      "memory access, (3) High\n",
      "computational overhead.\n",
      "Efficient DETR (Yao et al., 2021)\n",
      "(1) They found that different object container\n",
      "initialization methods have a great impact on\n",
      "decoder; (2) They also proposed an efficient way\n",
      "of initializing object containers using the\n",
      "characteristics of dense detection and sparse\n",
      "detection.\n",
      "(1) Poor performance for\n",
      "small objects, (2) High\n",
      "computational overhead.\n",
      "DINO (Zhang et al., 2022a)\n",
      "(1) Propose a contrast denoising training method,\n",
      "(2) Combine class DETR and two-stage model and\n",
      "propose a mixed query selection method to better\n",
      "initialize object query, (3) Look Forward Twice:\n",
      "Introducing proximity layer information to update\n",
      "parameters and improve the detection of small\n",
      "objects.\n",
      "(1) High computational\n",
      "overhead at high scales, (2)\n",
      "Diminishing marginal benefit\n",
      "from stacking too many scales.\n",
      "YOLOS (Fang et al., 2021)\n",
      "(1) Replace [cls] token with [det] token and\n",
      "image classification loss with bipartite matching\n",
      "loss, (2)Propose a pre-trained Transformer object\n",
      "detection paradigm.\n",
      "(1) Low detection accuracy,\n",
      "(2) High computational\n",
      "overhead.\n",
      "UP-DETR (Dai et al., 2021a)\n",
      "(1) Propose a new unsupervised pre-text task to\n",
      "perform unsupervised pre-training on Transformer,\n",
      "(2) Propose a patch detection reconstruction\n",
      "branch that is jointly optimized with patch\n",
      "detection.\n",
      "(1) Slow convergence, (2)\n",
      "Poor performance for small\n",
      "objects.\n",
      "Transformer Backbone\n",
      "FPT (Zhang et al., 2020)\n",
      "(1) Propose a feature interaction method across\n",
      "space and scale, (2) High compatibility.\n",
      "(1) Low detection accuracy,\n",
      "(2) High computational\n",
      "overhead.\n",
      "PVT (Wang et al., 2021a)\n",
      "(1) It can output multi-scale high-resolution\n",
      "feature maps; (2) The proposed spatial reduction\n",
      "attention module makes PVT successfully applied\n",
      "to dense prediction.\n",
      "(1) High computational\n",
      "overhead for high-resolution\n",
      "images; (2) Simple image\n",
      "division loses the connection\n",
      "information between different\n",
      "patches.\n",
      "Swin Transformer (Liu et al., 2021b)\n",
      "(1) Hierarchical representation, (2) Introduced\n",
      "communication between windows by computing\n",
      "attention within shifted windows and reduced the\n",
      "computational complexity to be linear with the\n",
      "image size.\n",
      "(1) Excessive GPU memory\n",
      "consumption at higher image\n",
      "resolutions, (2) Difficult to\n",
      "retrain on small datasets, (3)\n",
      "Difficult to transform\n",
      "pre-trained models at low\n",
      "resolutions to higher\n",
      "resolutions.\n",
      "within the visual Transformer models. By exploring these aspects, we\n",
      "can potentially uncover novel optimization strategies and improve the\n",
      "model’s overall performance in various computer vision tasks.\n",
      "The inefficient image-sequence information transformation.\n",
      "Unlike images, human-created languages have a high semantic density.\n",
      "Each word in a sentence can be treated as high-dimensional semantic\n",
      "information embedded in a low-dimensional vector representation.\n",
      "However images, as a natural signal with high spatial redundancy,\n",
      "have a low information density per pixel. For example,\n",
      "He et al.\n",
      "(2021) performed random high-scale masking of images, and then\n",
      "reconstructed the images well with Decoder, demonstrating that much\n",
      "higher semantic features than pixel information density can be captured\n",
      "in the images. But the current way of representing image information in\n",
      "sequences using Transformer is not efficient enough, which can bring\n",
      "about accuracy degradation as well as high computational overhead.\n",
      "Establishing efficient transformations of image sequences can help\n",
      "unlock the potential of the Transformer for CV tasks.\n",
      "4.2. Future development outlook\n",
      "Visual Transformer has made great progress in recent years, espe-\n",
      "cially in object detection, and the performance has surpassed SOTA\n",
      "CNN-based model on the COCO dataset. However, Transformer is not\n",
      "mature enough in practical application deployment. For example, the\n",
      "computational overhead is too large to be deployed on platforms with\n",
      "limited computer resources, and the real-time performance is not as\n",
      "good as the CNN-based one-stage approach.\n",
      "Self-supervised learning. While self-supervised learning has made\n",
      "a great success in natural language processing, current object detection\n",
      "models, which are mainly supervised learning, require large amounts\n",
      "Engineering Applications of Artificial Intelligence 126 (2023) 107021\n",
      "15\n",
      "Y. Li et al.\n",
      "of high-quality manually labeled data, which is usually too expensive.\n",
      "Therefore, It is natural to think of using self-supervised learning for\n",
      "visual tasks in order to pre-training models using a large amount of\n",
      "cheap data available on the Internet. For example, the MAE proposed\n",
      "by He et al. (2021) uses masked self-encoders for self-supervised learn-\n",
      "ing, which are adequately pre-trained and then migrated to specific\n",
      "tasks for fine-tuning.\n",
      "Lightweight Transformer. Since the performance of the current\n",
      "Transformer-based detector is powerful enough, the development of a\n",
      "lightweight Transformer architecture should be considered to broaden\n",
      "its applicability. Key considerations could include a reduction in com-\n",
      "putational demands, optimization for object detection, and intelligent\n",
      "query design to ensure high performance while minimizing computa-\n",
      "tional overhead. This would enable deployment on mobile platforms\n",
      "with limited computational resources.\n",
      "Multitasking. Within CNN-based methods, Mask R-CNN (He et al.,\n",
      "2017) successfully performs instance segmentation alongside object\n",
      "detection, yielding superior results. Could a Transformer detector also\n",
      "undertake multiple tasks simultaneously and derive benefits from this\n",
      "approach? For instance, the performance of the object detector could be\n",
      "enhanced by incorporating semantic segmentation. Semantic segmenta-\n",
      "tion captures object boundaries, aiding object localization in detection,\n",
      "and segments the background to delineate the contextual information\n",
      "of the object, improving detection probability. Such an approach is\n",
      "especially useful as objects typically exist within specific contexts, such\n",
      "as cars appearing on roads.\n",
      "5. Conclusion\n",
      "For the past decade, CNN-based models have reigned supreme in\n",
      "the field of object detection. However, the Transformer has recently\n",
      "demonstrated superior performance and substantial potential in com-\n",
      "puter vision (CV), rendering Transformer-based models a burgeoning\n",
      "research topic within object detection. In this paper, we have conducted\n",
      "an extensive review of mainstream Transformer-based object detectors\n",
      "developed over the past three years. Our focus has mainly been on\n",
      "their concepts, innovative aspects, and detection accuracy. We have\n",
      "categorized these methods according to their model structure and es-\n",
      "tablished a benchmark based on the COCO2017 dataset. Furthermore,\n",
      "we have conducted a multi-perspective analysis and comparison of\n",
      "these methods, summarizing their innovations and enhancements. We\n",
      "have also provided a comprehensive analysis of their limitations and\n",
      "summarized the existing challenges that persist within the application\n",
      "of Transformer in object detection. This study aims to aid readers in\n",
      "deepening their understanding of Transformer object detectors, spark-\n",
      "ing research interest to unleash the potential of the Transformer model,\n",
      "and enhancing its practical applications.\n",
      "CRediT authorship contribution statement\n",
      "Yong Li: Writing – review & editing, Supervision, Project admin-\n",
      "istration. Naipeng Miao: Conceptualization, Methodology, Validation,\n",
      "Formal analysis, Investigation, Writing- – original draft, Writing –\n",
      "review & editing, Visualization. Liangdi Ma: Writing – review & edit-\n",
      "ing. Feng Shuang: Funding acquisition, Resources. Xingwen Huang:\n",
      "Writing – review & editing.\n",
      "Declaration of competing interest\n",
      "The authors declare that they have no known competing finan-\n",
      "cial interests or personal relationships that could have appeared to\n",
      "influence the work reported in this paper.\n",
      "Data availability\n",
      "The data is public.\n",
      "References\n",
      "Arkin, Ershat, Yadikar, Nurbiya, Muhtar, Yusnur, Ubul, Kurban, 2021. A survey of\n",
      "object detection based on CNN and transformer. In: 2021 IEEE 2nd International\n",
      "Conference on Pattern Recognition and Machine Learning (PRML). pp. 99–108.\n",
      "http://dx.doi.org/10.1109/PRML52754.2021.9520732.\n",
      "Arkin, E., Yadikar, N., Xu, X., Aysa, A., Ubul, K., 2022. A survey: object detection\n",
      "methods from cnn to transformer. Multimedia Tools Appl. http://dx.doi.org/10.\n",
      "1007/s11042-022-13801-3.\n",
      "Bai, Y., Mei, J., Yuille, A., Xie, C., 2021. Are transformers more robust than CNNs? http:\n",
      "//dx.doi.org/10.48550/arXiv.2111.05464.\n",
      "Bochkovskiy, A., Wang, C.-Y., Liao, H.-Y.M., 2020. YOLOv4: Optimal speed and\n",
      "accuracy of object detection.\n",
      "Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Neelakan-\n",
      "tan, A., Shyam, P., Sastry, G., Askell, A., 2020. Language models are few-shot\n",
      "learners. Adv. Neural Inf. Process. Syst. 33, 1877–1901.\n",
      "Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., Zagoruyko, S., 2020.\n",
      "End-to-end object detection with transformers.\n",
      "Chen, X., Ma, H., Wan, J., Li, B., Xia, T., 2017. Multi-view 3d object detection network\n",
      "for autonomous driving. In: Proceedings of the IEEE Conference on Computer Vision\n",
      "and Pattern Recognition. pp. 1907–1915.\n",
      "Chen, T., Saxena, S., Li, L., Fleet, D.J., Hinton, G., 2021. Pix2seq: A language modeling\n",
      "framework for object detection. arXiv:2109.10852[cs].\n",
      "Chen, C., Seff, A., Kornhauser, A., Xiao, J., 2015. Deepdriving: Learning affordance for\n",
      "direct perception in autonomous driving. In: Proceedings of the IEEE International\n",
      "Conference on Computer Vision. pp. 2722–2730.\n",
      "Chu, X., Tian, Z., Wang, Y., Zhang, B., Ren, H., Wei, X., Xia, H., Shen, C., 2021.\n",
      "Twins: Revisiting the design of spatial attention in vision transformers. arXiv:\n",
      "2104.13840[cs].\n",
      "Cordonnier,\n",
      "J.-B.,\n",
      "Loukas,\n",
      "A.,\n",
      "Jaggi,\n",
      "M.,\n",
      "2020.\n",
      "On\n",
      "the\n",
      "relationship\n",
      "between\n",
      "self-attention and convolutional layers. arXiv:1911.03584[cs, stat].\n",
      "Dai, Z., Cai, B., Lin, Y., Chen, J., 2021a. UP-DETR: Unsupervised pre-training for object\n",
      "detection with transformers. arXiv:2011.09094[cs].\n",
      "Dai, X., Chen, Y., Xiao, B., Chen, D., Liu, M., Yuan, L., Zhang, L., 2021b. Dynamic head:\n",
      "Unifying object detection heads with attentions. In: Proceedings of the IEEE/CVF\n",
      "Conference on Computer Vision and Pattern Recognition. pp. 7373–7382.\n",
      "Dai, X., Chen, Y., Yang, J., Zhang, P., Yuan, L., Zhang, L., 2021c. Dynamic DETR: End-\n",
      "to-end object detection with dynamic attention. In: 2021 IEEE/CVF International\n",
      "Conference on Computer Vision. ICCV, IEEE, Montreal, QC, Canada, pp. 2968–2977.\n",
      "http://dx.doi.org/10.1109/ICCV48922.2021.00298.\n",
      "Dai, J., Qi, H., Xiong, Y., Li, Y., Zhang, G., Hu, H., Wei, Y., 2017. Deformable\n",
      "convolutional networks. In: Proceedings of the IEEE International Conference on\n",
      "Computer Vision. pp. 764–773.\n",
      "Devlin, J., Chang, M.-W., Lee, K., Toutanova, K., 2018. Bert: Pre-training of deep\n",
      "bidirectional transformers for language understanding. arXiv preprint arXiv:1810.\n",
      "04805.\n",
      "Dollar, P., Wojek, C., Schiele, B., Perona, P., 2012. Pedestrian detection: An evaluation\n",
      "of the state of the art. IEEE Trans. Pattern Anal. Mach. Intell. 34 (4), 743–761,\n",
      "doi:10/bjsn5q.\n",
      "Dong, X., Bao, J., Chen, D., Zhang, W., Yu, N., Yuan, L., Chen, D., Guo, B., 2022. CSWin\n",
      "transformer: A general vision transformer backbone with cross-shaped windows.\n",
      "arXiv:2107.00652[cs].\n",
      "Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T.,\n",
      "Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., Houlsby, N., 2021.\n",
      "An image is worth 16 × 16 words: Transformers for image recognition at scale.\n",
      "arXiv:2010.11929[cs].\n",
      "Everingham, M., Van Gool, L., Williams, C.K.I., Winn, J., Zisserman, A., 2007. The\n",
      "PASCAL visual object classes challenge 2007 (VOC2007) results. http://www.\n",
      "pascal-network.org/challenges/VOC/voc2007/workshop/index.html.\n",
      "Everingham, M., Van Gool, L., Williams, C.K.I., Winn, J., Zisserman, A., 2012. The\n",
      "PASCAL visual object classes challenge 2012 (VOC2012) results. http://www.\n",
      "pascal-network.org/challenges/VOC/voc2012/workshop/index.html.\n",
      "Fang, Y., Liao, B., Wang, X., Fang, J., Qi, J., Wu, R., Niu, J., Liu, W., 2021. You only\n",
      "look at one sequence: Rethinking transformer in vision through object detection.\n",
      "arXiv:2106.00666[cs].\n",
      "Felzenszwalb, P.F., Girshick, R.B., McAllester, D., Ramanan, D., 2010. Object detection\n",
      "with discriminatively trained part-based models. IEEE Trans. Pattern Anal. Mach.\n",
      "Intell. 32 (9), 1627–1645, doi: 10/fgv7fd.\n",
      "Gao, P., Zheng, M., Wang, X., Dai, J., Li, H., 2021. Fast convergence of DETR with\n",
      "spatially modulated co-attention. arXiv:2101.07448[cs].\n",
      "Ge, Z., Liu, S., Wang, F., Li, Z., Sun, J., 2021. YOLOX: Exceeding YOLO series in 2021.\n",
      "Geirhos, R., Rubisch, P., Michaelis, C., Bethge, M., Wichmann, F.A., Brendel, W., 2019.\n",
      "ImageNet-trained CNNs are biased towards texture; increasing shape bias improves\n",
      "accuracy and robustness. arXiv:1811.12231[cs, q-bio, stat].\n",
      "Glorot, X., Bengio, Y., 2010. Understanding the difficulty of training deep feedforward\n",
      "neural networks. In: Proceedings of the Thirteenth International Conference on Ar-\n",
      "tificial Intelligence and Statistics. In: JMLR Workshop and Conference Proceedings,\n",
      "pp. 249–256.\n",
      "Engineering Applications of Artificial Intelligence 126 (2023) 107021\n",
      "16\n",
      "Y. Li et al.\n",
      "Han, K., Wang, Y., Chen, H., Chen, X., Guo, J., Liu, Z., Tang, Y., Xiao, A., Xu, C.,\n",
      "Xu, Y., Yang, Z., Zhang, Y., Tao, D., 2022. A survey on vision transformer.\n",
      "In: IEEE Transactions on Pattern Analysis and Machine Intelligence. p. 1. http:\n",
      "//dx.doi.org/10.1109/TPAMI.2022.3152247.\n",
      "He, K., Chen, X., Xie, S., Li, Y., Dollár, P., Girshick, R., 2021. Masked autoencoders\n",
      "are scalable vision learners.\n",
      "He, K., Gkioxari, G., Dollár, P., Girshick, R., 2017. Mask r-cnn. In: Proceedings of the\n",
      "IEEE International Conference on Computer Vision. pp. 2961–2969.\n",
      "He, K., Zhang, X., Ren, S., Sun, J., 2015. Deep residual learning for image recognition.\n",
      "arXiv:1512.03385[cs].\n",
      "Khan, S., Naseer, M., Hayat, M., Zamir, S.W., Khan, F.S., Shah, M., 2021. Transformers\n",
      "in vision: A survey. arXiv:2101.01169.\n",
      "Kobatake, H., Yoshinaga, Y., 1996. Detection of spicules on mammogram based on\n",
      "skeleton analysis. IEEE Trans. Med. Imaging 15 (3), 235–245. http://dx.doi.org/\n",
      "10.1109/42.500062.\n",
      "Lan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., Soricut, R., 2019. Albert:\n",
      "A lite bert for self-supervised learning of language representations. arXiv preprint\n",
      "arXiv:1909.11942.\n",
      "Li, F., Zhang, H., Liu, S., Guo, J., Ni, L.M., Zhang, L., 2022. DN-DETR: Accelerate DETR\n",
      "training by introducing query denoising. arXiv:2203.01305[cs].\n",
      "Lin, T.-Y., Dollár, P., Girshick, R., He, K., Hariharan, B., Belongie, S., 2016. Feature\n",
      "pyramid networks for object detection. arXiv:1612.03144.\n",
      "Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P.,\n",
      "Zitnick, C.L., 2014. Microsoft coco: Common objects in context. In: European\n",
      "Conference on Computer Vision. Springer, pp. 740–755.\n",
      "Lin, J., Mao, X., Chen, Y., Xu, L., He, Y., Xue, H., 2022. D2ETR: Decoder-only DETR\n",
      "with computationally efficient cross-scale attention. arXiv:2203.00860[cs].\n",
      "Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C.-Y., Berg, A.C., 2016.\n",
      "SSD: Single shot multibox detector. In: Computer Vision – ECCV 2016. pp. 21–37.\n",
      "http://dx.doi.org/10.1007/978-3-319-46448-0_2.\n",
      "Liu, Z., Hu, H., Lin, Y., Yao, Z., Xie, Z., Wei, Y., Ning, J., Cao, Y., Zhang, Z., Dong, L.,\n",
      "Wei, F., Guo, B., 2021a. Swin transformer V2: Scaling up capacity and resolution.\n",
      "arXiv:2111.09883[cs].\n",
      "Liu, S., Li, F., Zhang, H., Yang, X., Qi, X., Su, H., Zhu, J., Zhang, L., 2022a. DAB-DETR:\n",
      "Dynamic anchor boxes are better queries for DETR. arXiv:2201.12329[cs].\n",
      "Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B., 2021b. Swin\n",
      "transformer: Hierarchical vision transformer using shifted windows. arXiv:2103.\n",
      "14030[cs].\n",
      "Liu, Z., Mao, H., Wu, C.-Y., Feichtenhofer, C., Darrell, T., Xie, S., 2022b. A ConvNet\n",
      "for the 2020s. arXiv:2201.03545[cs].\n",
      "Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettle-\n",
      "moyer, L., Stoyanov, V., 2019. RoBERTa: A robustly optimized BERT pretraining\n",
      "approach. arXiv:1907.11692[cs].\n",
      "Liu, Y., Zhang, Y., Wang, Y., Hou, F., Yuan, J., Tian, J., Zhang, Y., Shi, Z., Fan, J.,\n",
      "He, Z., 2021c. A survey of visual transformers. arXiv:2111.06091[cs].\n",
      "Loshchilov, I., Hutter, F., 2017. Decoupled weight decay regularization. arXiv preprint\n",
      "arXiv:1711.05101.\n",
      "Meng, D., Chen, X., Fan, Z., Zeng, G., Li, H., Yuan, Y., Sun, L., Wang, J., 2021.\n",
      "Conditional DETR for fast training convergence. In: Proceedings of the IEEE/CVF\n",
      "International Conference on Computer Vision. pp. 3651–3660.\n",
      "Radford, A., Narasimhan, K., Salimans, T., Sutskever, I., 2018. Improving language\n",
      "understanding by generative pre-training.\n",
      "Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., 2019. Language\n",
      "models are unsupervised multitask learners. Openai Blog 1 (8), 9.\n",
      "Rajpurkar, P., Zhang, J., Lopyrev, K., Liang, P., 2016. SQuAD: 100, 000+ questions for\n",
      "machine comprehension of text. arXiv:1606.05250[cs].\n",
      "Redmon, J., Divvala, S., Girshick, R., Farhadi, A., 2016. You only look once: Unified,\n",
      "real-time object detection. In: 2016 IEEE Conference on Computer Vision and\n",
      "Pattern Recognition. CVPR, IEEE, Las Vegas, NV, USA, pp. 779–788, doi:10/gc7rk9.\n",
      "Redmon, J., Farhadi, A. and, 2017. YOLO9000: Better, faster, stronger. In: 2017 IEEE\n",
      "Conference on Computer Vision and Pattern Recognition. CVPR, pp. 6517–6525,\n",
      "DOI: 10/gffdbj.\n",
      "Redmon, J., Farhadi, A., 2018. YOLOv3: An incremental improvement.\n",
      "Ren, S., He, K., Girshick, R., Sun, J., 2016. Faster R-CNN: Towards Real-Time Object\n",
      "Detection with region proposal networks. arXiv:1506.01497 [cs].\n",
      "Roh, B., Shin, J., Shin, W., Kim, S., 2022. Sparse DETR: Efficient end-to-end object\n",
      "detection with learnable sparsity. arXiv:2111.14330[cs].\n",
      "Ross, T.-Y., Dollár, G., 2017. Focal loss for dense object detection. In: Proceedings of\n",
      "the IEEE Conference on Computer Visionv and Pattern Recognition. pp. 2980–2988.\n",
      "Sang, E.F.T.K., De Meulder, F., 2003. Introduction to the CoNLL-2003 shared task:\n",
      "Language-independent named entity recognition. arXiv:cs/0306050.\n",
      "Sun, Z., Cao, S., Yang, Y., Kitani, K.M., 2021. Rethinking transformer-based set\n",
      "prediction for object detection. In: Proceedings of the IEEE/CVF International\n",
      "Conference on Computer Vision. pp. 3611–3620.\n",
      "Sung, K.-K., Poggio, T., 1998. Example-based learning for view-based human face\n",
      "detection. IEEE Trans. Pattern Anal. Mach. Intell. 20 (1), 39–51. http://dx.doi.\n",
      "org/10.1109/34.655648/bnkgmt.\n",
      "Sutskever, I., Vinyals, O., Le, Q.V., 2014. Sequence to sequence learning with neural\n",
      "networks. 9.\n",
      "Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, Ł.,\n",
      "Polosukhin, I., 2017. Attention is all you need. In: Advances in Neural Information\n",
      "Processing Systems, Vol. 30. Curran Associates, Inc.\n",
      "Wang, W., Xie, E., Li, X., Fan, D.-P., Song, K., Liang, D., Lu, T., Luo, P., Shao, L., 2021a.\n",
      "Pyramid vision transforme: A versatile backbone for dense prediction without\n",
      "convolutions. arXiv:2102.12122[cs].\n",
      "Wang, W., Xie, E., Li, X., Fan, D.-P., Song, K., Liang, D., Lu, T., Luo, P., Shao, L., 2022.\n",
      "PVTv2: Improved baselines with pyramid vision transformer. arXiv:2106.13797[cs].\n",
      "Wang, T., Yuan, L., Chen, Y., Feng, J., Yan, S., 2021b. PnP-DETR: Towards efficient\n",
      "visual analysis with transformers. In: Proceedings of the IEEE/CVF International\n",
      "Conference on Computer Vision. pp. 4661–4670.\n",
      "Wang, Y., Zhang, X., Yang, T., Sun, J., 2021c. Anchor DETR: Query design for\n",
      "transformer-based object detection.\n",
      "Yang, J., Li, C., Zhang, P., Dai, X., Xiao, B., Yuan, L., Gao, J., 2021. Focal self-attention\n",
      "for local-global interactions in vision transformers. arXiv:2107.00641[cs].\n",
      "Yao, Z., Ai, J., Li, B., Zhang, C., 2021. Efficient DETR: Improving end-to-end object\n",
      "detector with dense prior. arXiv:2104.01318[cs].\n",
      "Yuan, L., Chen, D., Chen, Y.-L., Codella, N., Dai, X., Gao, J., Hu, H., Huang, X., Li, B.,\n",
      "Li, C., Liu, C., Liu, M., Liu, Z., Lu, Y., Shi, Y., Wang, L., Wang, J., Xiao, B., Xiao, Z.,\n",
      "Yang, J., Zeng, M., Zhou, L., Zhang, P., 2021. Florence: A new foundation model\n",
      "for computer vision. arXiv:2111.11432[cs].\n",
      "Zhang, P., Dai, X., Yang, J., Xiao, B., Yuan, L., Zhang, L., Gao, J., 2021. Multi-scale\n",
      "vision longformer: A new vision transformer for high-resolution image encoding.\n",
      "arXiv preprint arXiv:2103.15358.\n",
      "Zhang, H., Li, F., Liu, S., Zhang, L., Su, H., Zhu, J., Ni, L.M., Shum, H.-Y., 2022a.\n",
      "DINO: DETR with improved denoising anchor boxes for end-to-end object detection.\n",
      "arXiv:2203.03605[cs].\n",
      "Zhang, G., Luo, Z., Yu, Y., Cui, K., Lu, S., 2022b. Accelerating DETR convergence via\n",
      "semantic-aligned matching. arXiv:2203.06883[cs].\n",
      "Zhang, D., Zhang, H., Tang, J., Wang, M., Hua, X., Sun, Q., 2020. Feature pyramid\n",
      "transformer. In: European Conference on Computer Vision. Springer, pp. 323–339.\n",
      "Zhao, Z.-Q., Zheng, P., Xu, S.-T., Wu, X., 2019. Object detection with deep learning: A\n",
      "review. IEEE Trans. Neural Netw. Learn. Syst. 30 (11), 3212–3232. http://dx.doi.\n",
      "org/10.1109/TNNLS.2018.2876865.\n",
      "Zheng, M., Gao, P., Zhang, R., Li, K., Wang, X., Li, H., Dong, H., 2021. End-to-end\n",
      "object detection with adaptive clustering transformer. arXiv:2011.09315[cs].\n",
      "Zhu, X., Su, W., Lu, L., Li, B., Wang, X., Dai, J., 2021. Deformable DETR: Deformable\n",
      "transformers for end-to-end object detection. arXiv:2010.04159[cs].\n",
      "\n",
      "Source: main.pdf\n"
     ]
    }
   ],
   "source": [
    "for document in documents:\n",
    "    print(f\"Extracted Content: {document.content}\")\n",
    "    print(f\"Source: {document.metadata['source']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Explanation:**\n",
    "\n",
    "Parser Initialization: We import and instantiate the FitzPdfParser.\n",
    "\n",
    "Parsing PDF: The .parse() method is used to extract text from the specified PDF file.\n",
    "\n",
    "Result: The extracted text content and source (PDF file path) are printed.\n",
    "\n",
    "## **Conclusion:**\n",
    "Document parsers like the FitzPdfParser allow easy extraction of text from PDF documents. They are crucial when working with large volumes of documents that require automated text extraction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **NOTEBOOK METADATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Dominion John \n",
      "GitHub Username: DOMINION-JOHN1\n",
      "Last Modified: 2024-10-21 15:12:19.640951\n",
      "Platform: Windows 11\n",
      "Python Version: 3.12.7 (tags/v3.12.7:0b05ead, Oct  1 2024, 03:06:41) [MSC v.1941 64 bit (AMD64)]\n",
      "Swarmauri Version: 0.5.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import platform\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# Display author information\n",
    "author_name = \"Dominion John \" \n",
    "github_username = \"DOMINION-JOHN1\"  \n",
    "\n",
    "print(f\"Author: {author_name}\")\n",
    "print(f\"GitHub Username: {github_username}\")\n",
    "\n",
    "# Last modified datetime (file's metadata)\n",
    "notebook_file = \"Notebook _3 _Document_parsers.ipynb\" \n",
    "try:\n",
    "    last_modified_time = os.path.getmtime(notebook_file)\n",
    "    last_modified_datetime = datetime.fromtimestamp(last_modified_time)\n",
    "    print(f\"Last Modified: {last_modified_datetime}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not retrieve last modified datetime: {e}\")\n",
    "\n",
    "# Display platform, Python version, and Swarmauri version\n",
    "print(f\"Platform: {platform.system()} {platform.release()}\")\n",
    "print(f\"Python Version: {sys.version}\")\n",
    "\n",
    "# Checking Swarmauri version\n",
    "try:\n",
    "    import swarmauri\n",
    "    print(f\"Swarmauri Version: {swarmauri.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"Swarmauri is not installed.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swarmauri(0.5.0)",
   "language": "python",
   "name": "swarmauri-0.5.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
