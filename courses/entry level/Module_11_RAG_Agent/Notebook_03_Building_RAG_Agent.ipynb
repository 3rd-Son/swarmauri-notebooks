{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0aeefb54-392c-4851-8604-b643181ca1d5",
   "metadata": {},
   "source": [
    "# Notebook 03: Building the RAG Agent\n",
    "## Introduction\n",
    "\n",
    "In this notebook, we will focus on constructing a fully functional Retrieval-Augmented Generation (RAG) Agent. The RAG agent combines various componentsâ€”like the language model (LLM), vector store, and conversation contextâ€”into a unified pipeline to retrieve relevant information from external documents and generate contextually enriched responses.\n",
    "\n",
    "We will walk through the final steps of assembling the RAG agent and demonstrate how to execute queries using the agent.\n",
    "\n",
    "## Review of Components\n",
    "\n",
    "Before diving into the full implementation of the RAG agent, letâ€™s briefly recap the key components:\n",
    "\n",
    "- **Language Model (LLM)**: Generates responses based on the input query.\n",
    "- **Vector Store**: Holds external documents and allows retrieval of relevant documents based on the query.\n",
    "- **Conversation Context**: Maintains the dialogue context, ensuring that responses are coherent and relevant to previous interactions.\n",
    "\n",
    "In this notebook, we will integrate these components into the RagAgent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a42e960-8b6f-4a15-97a8-616e2505c6ae",
   "metadata": {},
   "source": [
    "## Setting Up the Vector Store and Adding Documents\n",
    "\n",
    "The first step is to set up the vector store, which will hold our external knowledge in the form of documents. The RAG agent will use this store to retrieve relevant information during query processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bb924ff-6a9b-49e9-b471-979226690a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 documents added to the vector store.\n"
     ]
    }
   ],
   "source": [
    "from swarmauri.documents.concrete.Document import Document\n",
    "from swarmauri.vector_stores.concrete.TfidfVectorStore import TfidfVectorStore\n",
    "\n",
    "# Initialize the vector store\n",
    "vector_store = TfidfVectorStore()\n",
    "\n",
    "# Sample documents containing knowledge\n",
    "documents = [\n",
    "    Document(content=\"Their sister's name is Jane.\"),\n",
    "    Document(content=\"Their mother's name is Jean.\"),\n",
    "    Document(content=\"Their father's name is Joseph.\"),\n",
    "    Document(content=\"Their grandfather's name is Alex.\"),\n",
    "]\n",
    "\n",
    "# Add documents to the vector store\n",
    "vector_store.add_documents(documents)\n",
    "\n",
    "# Verify the documents have been added\n",
    "print(f\"{len(vector_store.documents)} documents added to the vector store.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52675a5-cc0e-4391-a8ee-a1f974080cb8",
   "metadata": {},
   "source": [
    "## Configuring the Conversation Context\n",
    "\n",
    "Next, we set up the conversation context to ensure the agent can handle multi-turn dialogues. This context helps the agent remember previous user queries and system responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "880404fb-d0b0-499f-a7fc-56d630ba1869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current conversation history:\n",
      "Your name is Jeff.\n",
      "What is my name?\n"
     ]
    }
   ],
   "source": [
    "from swarmauri.conversations.concrete.MaxSystemContextConversation import MaxSystemContextConversation\n",
    "from swarmauri.messages.concrete.SystemMessage import SystemMessage\n",
    "from swarmauri.messages.concrete.HumanMessage import HumanMessage \n",
    "\n",
    "# Create a system message\n",
    "system_context = SystemMessage(content=\"Your name is Jeff.\")\n",
    "\n",
    "# Initialize the conversation\n",
    "conversation = MaxSystemContextConversation(system_context=system_context, max_size=4)\n",
    "\n",
    "# Add a user message\n",
    "user_message = HumanMessage(content=\"What is my name?\")\n",
    "conversation.add_message(user_message)\n",
    "\n",
    "# Print the current conversation context\n",
    "print(\"Current conversation history:\")\n",
    "for message in conversation.history:\n",
    "    print(message.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc6461d-5b47-47b3-88e9-6bc5ccb24204",
   "metadata": {},
   "source": [
    "## Integrating the Language Model (LLM)\n",
    "\n",
    "The GroqModel (our chosen LLM) will generate responses based on both the retrieved documents and the conversation context. Let's initialize the LLM and integrate it into our RAG agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c2e906e-ef08-4344-9040-f0d36e75ad84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resource: LLM\n",
      "Type: GroqModel\n",
      "Default Name: gemma-7b-it\n",
      "Allowed Models: ['gemma-7b-it', 'gemma2-9b-it', 'llama-3.1-70b-versatile', 'llama-3.1-8b-instant', 'llama-3.2-11b-text-preview', 'llama-3.2-1b-preview', 'llama-3.2-3b-preview', 'llama3-8b-8192', 'llama3-groq-70b-8192-tool-use-preview', 'llama3-groq-8b-8192-tool-use-preview']\n",
      "Prediction with no system context for llama3-groq-8b-8192-tool-use-preview: Hello! ðŸ‘‹ I'm happy to hear from you. What can I do for you today? ðŸ˜Š\n",
      "Prediction with system context for llama3-groq-8b-8192-tool-use-preview: **Human 1**\n",
      "\n",
      "**Physical Characteristics:**\n",
      "\n",
      "* Unique genetic code\n",
      "* bipedal gait\n",
      "* Large brain size and complex language abilities\n",
      "* Distinctive facial features and prominent brow ridges\n",
      "\n",
      "**Origins and Evolution:**\n",
      "\n",
      "* Descended from hominin ancestors who evolved in Africa approximately 2 million years ago\n",
      "* Gradual evolution from upright-walking bipeds to modern humans\n",
      "* Shared ancestors with chimpanzees and bonobos\n",
      "\n",
      "**Culture and Behavior:**\n",
      "\n",
      "* Complex language and symbolic thought\n",
      "* Toolmaking and technological advancements\n",
      "* Social and cooperative behavior\n",
      "* Highly adaptable and resourceful\n",
      "\n",
      "**Contributions to Society:**\n",
      "\n",
      "* Scientific and technological innovations\n",
      "* Artistic and cultural achievements\n",
      "* Political and philosophical advancements\n",
      "* Social and economic development\n",
      "\n",
      "**Contemporary Issues:**\n",
      "\n",
      "* Genetic diversity and health disparities\n",
      "* Environmental concerns and climate change\n",
      "* Social inequality and poverty\n",
      "* Existential threats and the future of humanity\n",
      "\n",
      "**Future Prospects:**\n",
      "\n",
      "* Advancements in genetic engineering and biotechnology\n",
      "* Potential for space exploration and colonization\n",
      "* Solutions to address societal challenges and promote sustainability\n",
      "* Evolution of human potential and capabilities\n",
      "\n",
      "**Significance:**\n",
      "\n",
      "* Humans are unique and irreplaceable members of the Earth's biodiversity.\n",
      "* Their contributions to science, culture, and society are\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from swarmauri.llms.concrete.GroqModel import GroqModel as LLM\n",
    "from swarmauri.conversations.concrete.Conversation import Conversation\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "# Function to get allowed models, filtering out failing ones\n",
    "def get_allowed_models(llm):\n",
    "    failing_llms = [\n",
    "        \"llama3-70b-8192\",\n",
    "        \"llama-3.2-90b-text-preview\",\n",
    "        \"mixtral-8x7b-32768\",\n",
    "        \"llava-v1.5-7b-4096-preview\",\n",
    "        \"llama-guard-3-8b\",\n",
    "    ]\n",
    "    return [model for model in llm.allowed_models if model not in failing_llms]\n",
    "\n",
    "# Initialize the GroqModel\n",
    "if API_KEY:\n",
    "    llm = LLM(api_key=API_KEY)\n",
    "\n",
    "    # Print model information\n",
    "    print(f\"Resource: {llm.resource}\")\n",
    "    print(f\"Type: {llm.type}\")\n",
    "    print(f\"Default Name: {llm.name}\")\n",
    "\n",
    "    # Get allowed models\n",
    "    allowed_models = get_allowed_models(llm)\n",
    "    print(\"Allowed Models:\", allowed_models)\n",
    "\n",
    "    # Example usage with no system context\n",
    "\n",
    "    llm.name = allowed_models[0]\n",
    "    \n",
    "    # Create a conversation\n",
    "    conversation = Conversation()\n",
    "    \n",
    "    # Add a human message\n",
    "    input_data = \"Hello\"\n",
    "    human_message = HumanMessage(content=input_data)\n",
    "    conversation.add_message(human_message)\n",
    "\n",
    "    # Predict response\n",
    "    llm.predict(conversation=conversation)\n",
    "    prediction = conversation.get_last().content\n",
    "    print(f\"Prediction with no system context for {model_name}: {prediction}\")\n",
    "\n",
    "    # Example usage with a system context\n",
    "    system_context = ''\n",
    "    conversation = MaxSystemContextConversation(system_context=SystemMessage(content=system_context), max_size=2)\n",
    "    system_message = SystemMessage(content=system_context)\n",
    "    conversation.add_message(HumanMessage(content=\"human1\"))\n",
    "\n",
    "    human_message = HumanMessage(content=\"Hi\")\n",
    "    conversation.add_message(human_message)\n",
    "\n",
    "    # Predict response\n",
    "    llm.predict(conversation=conversation)\n",
    "    prediction = conversation.get_last().content\n",
    "    print(f\"Prediction with system context for {model_name}: {prediction}\")\n",
    "\n",
    "else:\n",
    "    print(\"API key is not set. Please set the GROQ_API_KEY environment variable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d756f9-b156-4100-b454-e93a0b3b82e8",
   "metadata": {},
   "source": [
    "## Building the RAG Agent\n",
    "Finally, we will assemble all the components into the RagAgent. The agent will retrieve relevant documents from the vector store and use the language model to generate responses based on both the retrieved content and the conversation context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9355ccd-0187-4f3d-9188-3c6596a084c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG Agent Response: The name of their grandfather is Alex, as stated in the given sentence.\n"
     ]
    }
   ],
   "source": [
    "from swarmauri.agents.concrete.RagAgent import RagAgent\n",
    "\n",
    "# Initialize the RAG Agent by combining LLM, conversation, and vector store\n",
    "rag_agent = RagAgent(\n",
    "    llm=llm,\n",
    "    conversation=conversation,\n",
    "    system_context=system_context,\n",
    "    vector_store=vector_store,\n",
    ")\n",
    "\n",
    "# Example query to the RAG agent\n",
    "query = \"What is the name of their grandfather?\"\n",
    "response = rag_agent.exec(query)\n",
    "\n",
    "# Print the agent's response\n",
    "print(f\"RAG Agent Response: {response}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50f83af-4aa8-45a7-a11e-933a30d2b816",
   "metadata": {},
   "source": [
    "## Handling Queries with the RAG Agent\n",
    "Now that the RAG agent is fully configured, we can test it with various queries. The RAG agent will retrieve documents from the vector store, interpret the conversation context, and generate informed responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db2657fc-8673-4f5e-8721-45614c093b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is the name of their mother?\n",
      "RAG Agent Response: The name of their mother is Jean, as stated in the given information.\n",
      "\n",
      "Query: What is the name of their sister?\n",
      "RAG Agent Response: The name of their sister is Jane, as mentioned in the given sentence.\n",
      "\n",
      "Query: Tell me more about their family.\n",
      "RAG Agent Response: The provided text does not contain any additional information regarding the family members or their characteristics, so I am unable to provide further details about their family.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the agent with different queries\n",
    "queries = [\n",
    "    \"What is the name of their mother?\",\n",
    "    \"What is the name of their sister?\",\n",
    "    \"Tell me more about their family.\"\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    response = rag_agent.exec(query)\n",
    "    print(f\"Query: {query}\\nRAG Agent Response: {response}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf92cacc-6f62-49be-a69f-ad7ea6297b31",
   "metadata": {},
   "source": [
    "## Notebook Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1839c758-4b94-45e5-accf-b5aea7c7a4f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Huzaifa Irshad \n",
      "GitHub Username: irshadhuzaifa\n",
      "Last Modified: 2024-10-22 18:44:18.063169\n",
      "Platform: Windows 11\n",
      "Python Version: 3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:17:27) [MSC v.1929 64 bit (AMD64)]\n",
      "Swarmauri Version: 0.5.0\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "author_name = \"Huzaifa Irshad \" \n",
    "github_username = \"irshadhuzaifa\"\n",
    "\n",
    "print(f\"Author: {author_name}\")\n",
    "print(f\"GitHub Username: {github_username}\")\n",
    "\n",
    "notebook_file = \"Notebook_03_Building_RAG_Agent.ipynb\"\n",
    "try:\n",
    "    last_modified_time = os.path.getmtime(notebook_file)\n",
    "    last_modified_datetime = datetime.fromtimestamp(last_modified_time)\n",
    "    print(f\"Last Modified: {last_modified_datetime}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not retrieve last modified datetime: {e}\")\n",
    "\n",
    "print(f\"Platform: {platform.system()} {platform.release()}\")\n",
    "print(f\"Python Version: {sys.version}\")\n",
    "\n",
    "try:\n",
    "    import swarmauri\n",
    "    print(f\"Swarmauri Version: {swarmauri.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"Swarmauri is not installed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0785d52-d6d4-4d89-89ed-5f4fd4f6e4b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swarmauri(0.5)",
   "language": "python",
   "name": "swarmauri-0.5.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
