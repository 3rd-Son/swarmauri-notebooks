{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60bdba78-b328-42f7-8817-5efaa3d71061",
   "metadata": {},
   "source": [
    "# Preprocessing Data for Embeddings\n",
    "\n",
    "## Introduction to Preprocessing\n",
    "Preprocessing is an essential step in preparing textual data for machine learning models. Before generating embeddings, documents need to be processed to ensure they are structured appropriately. Chunking documents by sentences or sections helps capture smaller, more manageable segments, making it easier for the model to learn from the data.\n",
    "\n",
    "## Using a Chunker for Preprocessing\n",
    "In this section, we will showcase how a `Chunker` can split data based on sentence boundaries or specific chunk sizes.\n",
    "\n",
    "### Chunking by Sentence\n",
    "We will start with an example of preprocessing data by splitting it into sentences before embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de883c47-71a5-432e-9a24-d3c830f899d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from swarmauri.chunkers.concrete.SentenceChunker import SentenceChunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5364ecc7-8564-4547-879a-a270d993c6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks (Sentence-based): ['This is the first sentence.', 'This is the second sentence.', 'This is the third sentence.']\n"
     ]
    }
   ],
   "source": [
    "# Initialize the chunker to split by sentence\n",
    "chunker = SentenceChunker()\n",
    "\n",
    "# Example document\n",
    "document = \"This is the first sentence. This is the second sentence. This is the third sentence.\"\n",
    "\n",
    "# Preprocess by sentence\n",
    "chunks = chunker.chunk_text(document)\n",
    "print(\"Chunks (Sentence-based):\", chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd33cc6a-476a-414e-9588-9611eb538942",
   "metadata": {},
   "source": [
    "### Chunking by Size\n",
    "\n",
    "Next, we will demonstrate how to chunk the text based on a specific size, for instance, by splitting the text into chunks of 10 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be6f30f7-fedf-42fe-b399-2841e13a541a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resource: Chunker\n",
      "Type: FixedLengthChunker\n",
      "Chunks (Size-based): ['This is the first sentence. This is the second sentence. This is the third sentence.']\n"
     ]
    }
   ],
   "source": [
    "from swarmauri.chunkers.concrete.FixedLengthChunker import FixedLengthChunker\n",
    "\n",
    "# Basic usage of FixedLengthChunker\n",
    "fixed_chunker = FixedLengthChunker()\n",
    "\n",
    "# Checking the resource and type attributes\n",
    "print(f\"Resource: {fixed_chunker.resource}\")\n",
    "print(f\"Type: {fixed_chunker.type}\")\n",
    "\n",
    "# Preprocess by chunk size\n",
    "chunks_by_size = fixed_chunker.chunk_text(document)\n",
    "print(\"Chunks (Size-based):\", chunks_by_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a47888-e3f8-4b4d-8fcc-c595ed7fcfa5",
   "metadata": {},
   "source": [
    "## Generating Embeddings after Chunking\n",
    "\n",
    "Once the document has been chunked, we can apply the `TfidfEmbedding` to generate embeddings for each chunk. Letâ€™s see how this works with the previously defined chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b0f0c17-8952-4ed9-a9f8-c2b388986fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for Chunk 1: name=None id='ea8653ef-f29a-40f2-bb85-36a224cecf44' members=[] owner=None host=None resource='Vector' version='0.1.0' type='Vector' value=[0.6461289150464732, 0.3816141458138271, 0.0, 0.3816141458138271, 0.3816141458138271, 0.0, 0.3816141458138271]\n",
      "Embedding for Chunk 2: name=None id='70055066-8fe3-45ab-be8c-2b4e478b954b' members=[] owner=None host=None resource='Vector' version='0.1.0' type='Vector' value=[0.0, 0.3816141458138271, 0.6461289150464732, 0.3816141458138271, 0.3816141458138271, 0.0, 0.3816141458138271]\n",
      "Embedding for Chunk 3: name=None id='357d9fbd-da17-413c-a303-16c9592c3d52' members=[] owner=None host=None resource='Vector' version='0.1.0' type='Vector' value=[0.0, 0.3816141458138271, 0.0, 0.3816141458138271, 0.3816141458138271, 0.6461289150464732, 0.3816141458138271]\n"
     ]
    }
   ],
   "source": [
    "from swarmauri.embeddings.concrete.TfidfEmbedding import TfidfEmbedding\n",
    "\n",
    "# Initialize the TF-IDF embedder\n",
    "embedder = TfidfEmbedding()\n",
    "\n",
    "# Generate embeddings for chunks (sentence-based)\n",
    "sentence_embeddings = embedder.fit_transform(chunks)\n",
    "for i, embedding in enumerate(sentence_embeddings):\n",
    "    print(f\"Embedding for Chunk {i+1}: {embedding}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa4c851-635f-4d2b-8071-c3be64b40c6d",
   "metadata": {},
   "source": [
    "## Explanation\n",
    "\n",
    "Chunking affects the quality and interpretability of embeddings, especially in cases where context matters. For instance, larger chunks (like whole documents) may capture more contextual information, but they can also introduce noise if the document contains varied topics. On the other hand, smaller chunks (like sentences) tend to produce embeddings that are more focused and specific, which can be beneficial for tasks that require understanding individual components of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf941b4-9a8b-477b-b2c0-7986660c005b",
   "metadata": {},
   "source": [
    "## Notebook Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ab230cb-031c-4d05-a2f9-4fd07a395825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Huzaifa Irshad \n",
      "GitHub Username: irshadhuzaifa\n",
      "Last Modified: 2024-10-18 11:15:10.217636\n",
      "Platform: Windows 11\n",
      "Python Version: 3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:17:27) [MSC v.1929 64 bit (AMD64)]\n",
      "Swarmauri Version: 0.5.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import platform\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "author_name = \"Huzaifa Irshad \" \n",
    "github_username = \"irshadhuzaifa\"\n",
    "\n",
    "print(f\"Author: {author_name}\")\n",
    "print(f\"GitHub Username: {github_username}\")\n",
    "\n",
    "notebook_file = \"Notebook_02_Preprocessing_Data_For_Embeddings.ipynb\"\n",
    "try:\n",
    "    last_modified_time = os.path.getmtime(notebook_file)\n",
    "    last_modified_datetime = datetime.fromtimestamp(last_modified_time)\n",
    "    print(f\"Last Modified: {last_modified_datetime}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not retrieve last modified datetime: {e}\")\n",
    "\n",
    "print(f\"Platform: {platform.system()} {platform.release()}\")\n",
    "print(f\"Python Version: {sys.version}\")\n",
    "\n",
    "try:\n",
    "    import swarmauri\n",
    "    print(f\"Swarmauri Version: {swarmauri.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"Swarmauri is not installed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1794c8-d859-47c8-95b4-98a9ef896dd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swarmauri(0.5)",
   "language": "python",
   "name": "swarmauri-0.5.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
